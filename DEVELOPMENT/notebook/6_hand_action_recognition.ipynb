{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad688dd",
   "metadata": {},
   "source": [
    "# Tendon Gliding Hand Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54cd334",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "- The goal is to perform Tendon Gliding Hand Action Recognition by classifying hand postures into five distinct classes using real-time video. Additionally, the task involves calculating the accuracy of hand skeleton occurrences compared to the ground truth.\n",
    "- The classes are as follows: **Hand Open**, **Intrinsic Plan**, **Straight Fist**, **Hand Close**, and **Hook Hand**.  \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src='../../IMAGES/Classes.jpg' width=\"500px\" />\n",
    "</p>\n",
    "\n",
    "- The base workflow is as follows: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Import RAW Data]\n",
    "    A --> B[Separate into 5 different classes]\n",
    "    B --> C[Generate 2D keypoints of X and Y coordinates]\n",
    "    C --> D[Train LSTM model]\n",
    "    D --> E[Evaluate Performance]\n",
    "    E --> F[Metrics: Accuracy, Specificity, Sensitivity, F1-Score, Confusion Matrix]\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc7005",
   "metadata": {},
   "source": [
    "## 0. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac3d193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\albumentations\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import rerun as rr\n",
    "import rerun.blueprint as rrb\n",
    "import seedir as sd\n",
    "import os\n",
    "import albumentations as A\n",
    "from pyorbbecsdk import *\n",
    "import cv2\n",
    "from utils import frame_to_bgr_image\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"NO_ALBUMENTATIONS_UPDATE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4de32",
   "metadata": {},
   "source": [
    "## 1. Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3b579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bag_files(root_dir='data'):\n",
    "    \"\"\"\n",
    "    List all .bag files in the directory structure, excluding those in 'open-hand' folders.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory to start the search from.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of paths to .bag files.\n",
    "    \"\"\"\n",
    "    bag_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        # Skip 'open-hand' directories\n",
    "        if 'open-fist' in os.path.basename(root):\n",
    "            continue\n",
    "            \n",
    "        # Add .bag files to the list\n",
    "        for file in files:\n",
    "            if file.endswith('.bag'):\n",
    "                bag_files.append(os.path.join(root, file))\n",
    "                \n",
    "    return bag_files\n",
    "\n",
    "def playback_state_callback(state):\n",
    "    \"\"\"Callback function to handle playback state transitions.\"\"\"\n",
    "    global playback_finished\n",
    "    if state == OBMediaState.OB_MEDIA_BEGIN:\n",
    "        print(\"Bag player begin\")\n",
    "    elif state == OBMediaState.OB_MEDIA_END:\n",
    "        print(\"Bag player end\")\n",
    "        playback_finished = True  # Signal that playback has finished\n",
    "    elif state == OBMediaState.OB_MEDIA_PAUSED:\n",
    "        print(\"Bag player paused\")\n",
    "\n",
    "def process_frames(bag_file):\n",
    "    \"\"\"\n",
    "    Process the .bag file and return lists of processed images.\n",
    "    \n",
    "    Returns:\n",
    "        depth_image_list: List of raw depth data (converted to float and scaled).\n",
    "        color_image_list: List of processed color images.\n",
    "        overlaid_image_list: List of images with overlay (color blended with depth colormap).\n",
    "    \"\"\"\n",
    "    global playback_finished\n",
    "    playback_finished = False  # Reset flag\n",
    "\n",
    "    pipeline = Pipeline(bag_file)\n",
    "    playback = pipeline.get_playback()\n",
    "    playback.set_playback_state_callback(playback_state_callback)\n",
    "\n",
    "    # Start the pipeline\n",
    "    pipeline.start()\n",
    "\n",
    "    depth_image_list = []\n",
    "    color_image_list = []\n",
    "\n",
    "    while not playback_finished:\n",
    "        frames = pipeline.wait_for_frames(100)\n",
    "        if frames is None:\n",
    "            if playback_finished:\n",
    "                print(\"All frames have been processed and converted successfully.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Retrieve frames once per iteration\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        if depth_frame is not None:\n",
    "            width = depth_frame.get_width()\n",
    "            height = depth_frame.get_height()\n",
    "            scale = depth_frame.get_depth_scale()\n",
    "\n",
    "            # Process raw depth data\n",
    "            depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n",
    "            depth_data = depth_data.reshape((height, width))\n",
    "            depth_data = depth_data.astype(np.float32) * scale\n",
    "            depth_image_list.append(depth_data)\n",
    "\n",
    "            # Normalize and invert to obtain desired mapping (farthest = red, closest = blue)\n",
    "            depth_norm = cv2.normalize(depth_data, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "            inverted_depth = 255 - depth_norm\n",
    "            depth_image = cv2.applyColorMap(inverted_depth, cv2.COLORMAP_JET)\n",
    "        else:\n",
    "            depth_image = None\n",
    "\n",
    "        if color_frame is not None:\n",
    "            width = color_frame.get_width()\n",
    "            height = color_frame.get_height()\n",
    "\n",
    "            color_data = frame_to_bgr_image(color_frame)\n",
    "            color_image = cv2.resize(color_data, (width, height))\n",
    "            # Convert to BGR if necessary; adjust if frame_to_bgr_image already outputs BGR\n",
    "            color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "            color_image_list.append(color_image)\n",
    "        else:\n",
    "            color_image = None\n",
    "\n",
    "    return depth_image_list, color_image_list\n",
    "\n",
    "def rerun_visualization(*image_lists):\n",
    "    \"\"\"\n",
    "    Create a rerun visualization with multiple image lists displayed in a grid.\n",
    "    \n",
    "    Args:\n",
    "        *image_lists: Variable number of image lists to display\n",
    "    \"\"\"\n",
    "    stream = rr.new_recording(\"spawn\", spawn=True)\n",
    "    \n",
    "    # Dynamically create spatial views for each image list\n",
    "    spatial_views = []\n",
    "    for i in range(len(image_lists)):\n",
    "        spatial_views.append(rrb.Spatial2DView(origin=f'/color_image_{i}'))\n",
    "    \n",
    "    # Calculate a reasonable number of columns for the grid\n",
    "    # You can adjust this logic based on your preference\n",
    "    num_columns = min(3, len(image_lists))  # Max 3 columns\n",
    "    \n",
    "    # Setup the blueprint with dynamic grid configuration\n",
    "    blueprint = rrb.Blueprint(\n",
    "        rrb.Grid(*spatial_views, grid_columns=num_columns),\n",
    "        collapse_panels=True\n",
    "    )\n",
    "    \n",
    "    # Calculate the maximum length across all image lists\n",
    "    max_length = max(len(image_list) for image_list in image_lists)\n",
    "    \n",
    "    # Log all images with proper time sequencing\n",
    "    for idx in range(max_length):\n",
    "        stream.set_time_sequence(\"frame\", idx)\n",
    "        \n",
    "        # Log each image list at the current index if available\n",
    "        for list_idx, image_list in enumerate(image_lists):\n",
    "            if idx < len(image_list):\n",
    "                stream.log(f\"color_image_{list_idx}\", rr.Image(image_list[idx]))\n",
    "    \n",
    "    stream.send_blueprint(blueprint)\n",
    "\n",
    "def sliding_window_sample(images, window_size=16, stride=8):\n",
    "    total_frames = len(images)\n",
    "    windows = []\n",
    "    \n",
    "    for start_idx in range(0, total_frames - window_size + 1, stride):\n",
    "        end_idx = start_idx + window_size\n",
    "        window = np.stack(images[start_idx:end_idx])\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "def visualize_windows(windows):\n",
    "    \"\"\"\n",
    "    Visualize multiple windows of image sequences in Rerun.\n",
    "    \n",
    "    Args:\n",
    "        windows: List of image windows, where each window is a sequence of frames\n",
    "        max_windows_to_display: Maximum number of windows to display in the grid\n",
    "    \"\"\"\n",
    "    stream = rr.new_recording(\"spawn\", spawn=True)\n",
    "    \n",
    "    # Limit the number of windows to display to avoid overcrowding\n",
    "    num_windows = len(windows)\n",
    "    \n",
    "    # Create spatial views for each window\n",
    "    spatial_views = []\n",
    "    for i in range(num_windows):\n",
    "        spatial_views.append(rrb.Spatial2DView(origin=f'/window_{i}'))\n",
    "    \n",
    "    # Calculate grid layout (max 3 columns)\n",
    "    num_columns = min(3, num_windows)\n",
    "    \n",
    "    # Setup the blueprint with dynamic grid configuration\n",
    "    blueprint = rrb.Blueprint(\n",
    "        rrb.Grid(*spatial_views, grid_columns=num_columns),\n",
    "        collapse_panels=True\n",
    "    )\n",
    "    \n",
    "    # Log the frames of each window\n",
    "    window_size = windows[0].shape[0]  # Get the size of each window\n",
    "    \n",
    "    # Log each frame in each window\n",
    "    for frame_idx in range(window_size):\n",
    "        stream.set_time_sequence(\"frame\", frame_idx)\n",
    "        \n",
    "        # Log the current frame from each window\n",
    "        for window_idx in range(num_windows):\n",
    "            if window_idx < len(windows):\n",
    "                stream.log(f\"window_{window_idx}\", \n",
    "                          rr.Image(windows[window_idx][frame_idx]))\n",
    "    \n",
    "    # Display info about the windows\n",
    "    print(f\"Visualizing {num_windows} windows out of {len(windows)} total\")\n",
    "    print(f\"Each window contains {window_size} frames\")\n",
    "    print(f\"Window shape: {windows[0].shape}\")\n",
    "    \n",
    "    stream.send_blueprint(blueprint)\n",
    "\n",
    "def save_windowed_data(recordings, bag_files, window_size=16, stride=8, windowed_data_dir='windowed_data'):\n",
    "    \"\"\"\n",
    "    Save windowed data for RGB and depth images for each recording to its respective folder.\n",
    "    \n",
    "    Args:\n",
    "        recordings: Dictionary containing recordings with color_images and depth_images\n",
    "        bag_files: List of bag file paths sorted to match recording indices\n",
    "        window_size: Size of each window\n",
    "        stride: Stride between consecutive windows\n",
    "    \"\"\"\n",
    "    print(f\"Saving windowed data for {len(recordings)} recordings...\")\n",
    "    \n",
    "    for recording_idx, bag_file_path in enumerate(bag_files):\n",
    "        recording_key = f\"recording_{recording_idx}\"\n",
    "        \n",
    "        # Check if the recording exists\n",
    "        if recording_key in recordings:\n",
    "            recording_data = recordings[recording_key]\n",
    "            # Create base output directory for this recording\n",
    "            bag_path = Path(bag_file_path)\n",
    "            base_output_dir = bag_path.parent / windowed_data_dir\n",
    "            \n",
    "            # Process each image type (RGB and depth)\n",
    "            image_types = {\n",
    "                \"color_images\": \"rgb\",\n",
    "                \"depth_images\": \"depth\"\n",
    "            }\n",
    "            \n",
    "            for image_key, folder_name in image_types.items():\n",
    "                if image_key in recording_data and recording_data[image_key] is not None:\n",
    "                    images = recording_data[image_key]\n",
    "                    \n",
    "                    # Create windows for this image type\n",
    "                    windows = sliding_window_sample(images, window_size=window_size, stride=stride)\n",
    "                    \n",
    "                    # Create output directory for this image type\n",
    "                    output_dir = base_output_dir / folder_name\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Create a base filename from the bag file\n",
    "                    base_filename = bag_path.stem\n",
    "                    \n",
    "                    # Save each window as a separate .npy file\n",
    "                    for window_idx, window in enumerate(windows):\n",
    "                        # Create filename: RecordXXX_window_NNN.npy\n",
    "                        window_filename = f\"{base_filename}_window_{window_idx:03d}.npy\"\n",
    "                        output_path = output_dir / window_filename\n",
    "                        \n",
    "                        # Save the window data\n",
    "                        np.save(output_path, window)\n",
    "                    \n",
    "                    print(f\"Saved {len(windows)} {folder_name} windows for {recording_key} to {output_dir}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {image_key} for {recording_key}: Data not available\")\n",
    "        else:\n",
    "            print(f\"Skipping recording {recording_idx}: Recording not found\")\n",
    "    \n",
    "    print(\"Windowed data saving complete!\")\n",
    "\n",
    "def prepare_recordings_dict(processed_data):\n",
    "    recordings = {}\n",
    "    for i in range(6):  # Assuming there are 6 recordings (0-5)\n",
    "        recording_key = f\"recording_{i}\"\n",
    "        if recording_key in processed_data:\n",
    "            recordings[recording_key] = processed_data[recording_key]\n",
    "    return recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66277b49",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "- In total there are six different recordings of tendon gliding task as follows: \n",
    "    - From ðŸ“ `20250402`\n",
    "        - `Record_20250402151124.bag`\n",
    "        - `Record_20250402151609.bag`\n",
    "        - `Record_20250402152331.bag`\n",
    "    - From ðŸ“ `20250506`\n",
    "        - `Record_20250506145704.bag`\n",
    "        - `Record_20250506152951.bag`\n",
    "        - `Record_20250506162630.bag`\n",
    "- Each data consist of RGB frames and Depth frames\n",
    "- Workflow: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data Extraction] --> B[Data Visualization]\n",
    "    B --> C[Windowing Process]\n",
    "    C --> D[Save Windowed Data]\n",
    "    D --> E[Data Class Generation]\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af7b46",
   "metadata": {},
   "source": [
    "### 2.1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c173b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_files = list_bag_files()\n",
    "data_20250402 = bag_files[:3]      # First three items\n",
    "data_20250506 = bag_files[-3:]     # Last three items\n",
    "processed_data = {}\n",
    "\n",
    "# Process the 20250402 files\n",
    "for idx, bag_file in enumerate(data_20250402):\n",
    "    print(f\"Processing bag file: {bag_file}\")\n",
    "    # Process the frames in the bag file\n",
    "    depth_images, color_images = process_frames(bag_file)\n",
    "    # Store the lists in the dictionary with dataset identifier\n",
    "    processed_data[f\"recording_{idx}\"] = {\n",
    "        \"dataset\": \"20250402\",\n",
    "        \"depth_images\": depth_images,\n",
    "        \"color_images\": color_images\n",
    "    }\n",
    "\n",
    "# Process the 20250506 files\n",
    "for idx, bag_file in enumerate(data_20250506):\n",
    "    print(f\"Processing bag file: {bag_file}\")\n",
    "    # Process the frames in the bag file\n",
    "    depth_images, color_images = process_frames(bag_file)\n",
    "    # Store the lists in the dictionary with dataset identifier\n",
    "    processed_data[f\"recording_{idx+3}\"] = {  # Add offset to avoid key collision\n",
    "        \"dataset\": \"20250506\",\n",
    "        \"depth_images\": depth_images,\n",
    "        \"color_images\": color_images\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5712de1",
   "metadata": {},
   "source": [
    "### 2.2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_0 = processed_data.get(\"recording_0\")\n",
    "recording_1 = processed_data.get(\"recording_1\")\n",
    "recording_2 = processed_data.get(\"recording_2\")\n",
    "recording_3 = processed_data.get(\"recording_3\")\n",
    "recording_4 = processed_data.get(\"recording_4\")\n",
    "recording_5 = processed_data.get(\"recording_5\")\n",
    "\n",
    "image_lists = []\n",
    "for recording_idx in range(6):  # Assuming you have 6 recordings\n",
    "    recording_key = f\"recording_{recording_idx}\"\n",
    "    if recording_key in processed_data and \"color_images\" in processed_data[recording_key]:\n",
    "        image_lists.append(processed_data[recording_key][\"color_images\"])\n",
    "\n",
    "# Visualize all valid image lists together\n",
    "if image_lists:\n",
    "    rerun_visualization(*image_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e748db",
   "metadata": {},
   "source": [
    "### 2.3. Detect Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91072361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\mmengine\\optim\\optimizer\\zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mmcv\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from mmpose.utils import adapt_mmdet_pipeline\n",
    "from mmpose.evaluation.functional import nms\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Tuple, List, Optional, Union, Dict, Any\n",
    "\n",
    "# Global model cache to avoid reloading models\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "def detect_hands(img: Union[str, np.ndarray, List[Union[str, np.ndarray]]], \n",
    "                det_config: str = 'configs/rtmdet_nano_320-8xb32_hand.py',\n",
    "                det_checkpoint: str = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth',\n",
    "                device: str = 'cuda:0',\n",
    "                det_cat_id: int = 0,\n",
    "                bbox_thr: float = 0.3,\n",
    "                nms_thr: float = 0.3,\n",
    "                suppress_warnings: bool = True,\n",
    "                input_size: Tuple[int, int] = (300, 300),\n",
    "                crop_size: Tuple[int, int] = (600, 600),\n",
    "                return_coordinates: str = 'original') -> Union[\n",
    "                    Tuple[np.ndarray, np.ndarray, Dict[str, Any]],  # For 'original' mode\n",
    "                    Tuple[np.ndarray, np.ndarray, Dict[str, Any], np.ndarray],  # For 'crop' or 'model' modes\n",
    "                    List[Union[\n",
    "                        Tuple[np.ndarray, np.ndarray, Dict[str, Any]],  # List of results in 'original' mode\n",
    "                        Tuple[np.ndarray, np.ndarray, Dict[str, Any], np.ndarray]  # List of results in 'crop' or 'model' modes\n",
    "                    ]]\n",
    "                ]:\n",
    "    \"\"\"\n",
    "    Detect hand bounding boxes in a single image or a list of images.\n",
    "    \n",
    "    Args:\n",
    "        img: Image path, numpy array (RGB), or list of image paths/arrays.\n",
    "        det_config: Path to detection config file.\n",
    "        det_checkpoint: Path or URL to detection checkpoint.\n",
    "        device: Device to run inference on.\n",
    "        det_cat_id: Category ID for hands.\n",
    "        bbox_thr: Threshold for bounding box confidence.\n",
    "        nms_thr: Threshold for non-maximum suppression.\n",
    "        suppress_warnings: Whether to suppress warning messages.\n",
    "        input_size: Model input size (width, height), defaults to (300, 300)\n",
    "        crop_size: Size for center crop (width, height), defaults to (600, 600)\n",
    "        return_coordinates: Coordinate system for returned bounding boxes:\n",
    "                          - 'original': In original input image coordinates (default)\n",
    "                          - 'crop': In center-cropped image coordinates\n",
    "                          - 'model': In model input size coordinates (300x300 by default)\n",
    "        \n",
    "    Returns:\n",
    "        For a single image:\n",
    "            bboxes: Numpy array of hand bounding boxes [x1, y1, x2, y2] in specified coordinate system\n",
    "            scores: Numpy array of confidence scores\n",
    "            crop_info: Dictionary with information about the crop (for transforming coordinates)\n",
    "            processed_img: The cropped or model-sized image based on return_coordinates value\n",
    "            \n",
    "        For multiple images:\n",
    "            List of tuples, where each tuple contains the above results for one image\n",
    "    \"\"\"\n",
    "    global MODEL_CACHE\n",
    "    \n",
    "    # Validate return_coordinates parameter\n",
    "    valid_return_options = ['original', 'crop', 'model']\n",
    "    if return_coordinates not in valid_return_options:\n",
    "        raise ValueError(f\"return_coordinates must be one of {valid_return_options}, got {return_coordinates}\")\n",
    "    \n",
    "    # Set up warning suppression if requested\n",
    "    if suppress_warnings:\n",
    "        # Filter warnings\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "        # Filter mmengine and other library logs\n",
    "        logging.getLogger('mmengine').setLevel(logging.ERROR)\n",
    "        logging.getLogger('mmdet').setLevel(logging.ERROR)\n",
    "        logging.getLogger('mmpose').setLevel(logging.ERROR)\n",
    "        \n",
    "        # Disable PyTorch CUDA warnings\n",
    "        os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "    \n",
    "    # Check if model is already in cache\n",
    "    model_key = f\"{det_config}_{det_checkpoint}_{device}\"\n",
    "    if model_key not in MODEL_CACHE:\n",
    "        # Build detector\n",
    "        detector = init_detector(det_config, det_checkpoint, device=device)\n",
    "        detector.cfg = adapt_mmdet_pipeline(detector.cfg)\n",
    "        MODEL_CACHE[model_key] = detector\n",
    "    else:\n",
    "        detector = MODEL_CACHE[model_key]\n",
    "    \n",
    "    # Define the processing function inside to maintain access to all variables\n",
    "    def _process_single_image(current_img):\n",
    "        # Convert image to numpy array if it's a path\n",
    "        if isinstance(current_img, str):\n",
    "            current_img = mmcv.imread(current_img, channel_order='rgb')\n",
    "        elif isinstance(current_img, np.ndarray) and current_img.shape[-1] == 3:\n",
    "            # Convert BGR to RGB if necessary\n",
    "            if not isinstance(current_img, np.ndarray):\n",
    "                current_img = np.array(current_img)\n",
    "            current_img = mmcv.bgr2rgb(current_img)\n",
    "        \n",
    "        # Store original image dimensions\n",
    "        original_height, original_width = current_img.shape[:2]\n",
    "        \n",
    "        # Apply center crop first to get the cropped image\n",
    "        center_crop = A.CenterCrop(height=crop_size[1], width=crop_size[0])\n",
    "        crop_result = center_crop(image=current_img)\n",
    "        cropped_img = crop_result['image']\n",
    "        \n",
    "        # Calculate actual crop offsets based on original image dimensions\n",
    "        crop_x_offset = max(0, (original_width - crop_size[0]) // 2)\n",
    "        crop_y_offset = max(0, (original_height - crop_size[1]) // 2)\n",
    "        \n",
    "        # Now resize the cropped image to input size for the model\n",
    "        resize_transform = A.Resize(height=input_size[1], width=input_size[0])\n",
    "        resize_result = resize_transform(image=cropped_img)\n",
    "        processed_img = resize_result['image']\n",
    "        \n",
    "        # Store crop information for coordinate mapping\n",
    "        crop_info = {\n",
    "            'original_size': (original_width, original_height),\n",
    "            'crop_size': crop_size,\n",
    "            'input_size': input_size,\n",
    "            'crop_offset': (crop_x_offset, crop_y_offset)\n",
    "        }\n",
    "        \n",
    "        # Detect hands (bounding boxes)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            # Suppress torch warnings\n",
    "            torch._C._jit_set_profiling_executor(False)\n",
    "            torch._C._jit_set_profiling_mode(False)\n",
    "            \n",
    "            det_result = inference_detector(detector, processed_img)\n",
    "        \n",
    "        pred_instance = det_result.pred_instances.cpu().numpy()\n",
    "        \n",
    "        # Extract bounding boxes with scores\n",
    "        if len(pred_instance.bboxes) > 0:\n",
    "            bboxes = np.concatenate(\n",
    "                (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "            \n",
    "            # Filter bboxes by category and score\n",
    "            mask = np.logical_and(pred_instance.labels == det_cat_id,\n",
    "                                pred_instance.scores > bbox_thr)\n",
    "            bboxes = bboxes[mask]\n",
    "            \n",
    "            # Apply NMS\n",
    "            if len(bboxes) > 0:\n",
    "                keep_indices = nms(bboxes, nms_thr)\n",
    "                filtered_bboxes = bboxes[keep_indices, :4]  # Just the coordinates\n",
    "                filtered_scores = bboxes[keep_indices, 4]   # Just the scores\n",
    "                \n",
    "                # Map bounding boxes to appropriate coordinates based on user choice\n",
    "                if len(filtered_bboxes) > 0:\n",
    "                    # For 'model' coordinates, we keep the bounding boxes as is\n",
    "                    if return_coordinates == 'model':\n",
    "                        # Clip to model input boundaries (usually redundant but good practice)\n",
    "                        filtered_bboxes[:, 0] = np.clip(filtered_bboxes[:, 0], 0, input_size[0])\n",
    "                        filtered_bboxes[:, 1] = np.clip(filtered_bboxes[:, 1], 0, input_size[1])\n",
    "                        filtered_bboxes[:, 2] = np.clip(filtered_bboxes[:, 2], 0, input_size[0])\n",
    "                        filtered_bboxes[:, 3] = np.clip(filtered_bboxes[:, 3], 0, input_size[1])\n",
    "                        \n",
    "                    else:  # 'crop' or 'original'\n",
    "                        # First scale from input_size to crop_size\n",
    "                        scale_x = crop_size[0] / input_size[0]\n",
    "                        scale_y = crop_size[1] / input_size[1]\n",
    "                        \n",
    "                        filtered_bboxes[:, 0] *= scale_x  # x1\n",
    "                        filtered_bboxes[:, 1] *= scale_y  # y1\n",
    "                        filtered_bboxes[:, 2] *= scale_x  # x2\n",
    "                        filtered_bboxes[:, 3] *= scale_y  # y2\n",
    "                        \n",
    "                        # For 'crop' coordinates, clip to crop boundaries\n",
    "                        if return_coordinates == 'crop':\n",
    "                            filtered_bboxes[:, 0] = np.clip(filtered_bboxes[:, 0], 0, crop_size[0])\n",
    "                            filtered_bboxes[:, 1] = np.clip(filtered_bboxes[:, 1], 0, crop_size[1])\n",
    "                            filtered_bboxes[:, 2] = np.clip(filtered_bboxes[:, 2], 0, crop_size[0])\n",
    "                            filtered_bboxes[:, 3] = np.clip(filtered_bboxes[:, 3], 0, crop_size[1])\n",
    "                        \n",
    "                        # For 'original' coordinates, add crop offsets and clip to original boundaries\n",
    "                        elif return_coordinates == 'original':\n",
    "                            filtered_bboxes[:, 0] += crop_x_offset  # x1\n",
    "                            filtered_bboxes[:, 1] += crop_y_offset  # y1\n",
    "                            filtered_bboxes[:, 2] += crop_x_offset  # x2\n",
    "                            filtered_bboxes[:, 3] += crop_y_offset  # y3\n",
    "                            \n",
    "                            # Clip to original image boundaries\n",
    "                            filtered_bboxes[:, 0] = np.clip(filtered_bboxes[:, 0], 0, original_width)\n",
    "                            filtered_bboxes[:, 1] = np.clip(filtered_bboxes[:, 1], 0, original_height)\n",
    "                            filtered_bboxes[:, 2] = np.clip(filtered_bboxes[:, 2], 0, original_width)\n",
    "                            filtered_bboxes[:, 3] = np.clip(filtered_bboxes[:, 3], 0, original_height)\n",
    "                \n",
    "                # Return appropriate image along with results based on return_coordinates\n",
    "                if return_coordinates == 'model':\n",
    "                    return filtered_bboxes, filtered_scores, crop_info, processed_img\n",
    "                elif return_coordinates == 'crop':\n",
    "                    return filtered_bboxes, filtered_scores, crop_info, cropped_img\n",
    "                else:  # 'original'\n",
    "                    return filtered_bboxes, filtered_scores, crop_info\n",
    "        \n",
    "        # Return empty arrays if no detections, with appropriate image\n",
    "        if return_coordinates == 'model':\n",
    "            return np.empty((0, 4), dtype=np.float32), np.empty((0,), dtype=np.float32), crop_info, processed_img\n",
    "        elif return_coordinates == 'crop':\n",
    "            return np.empty((0, 4), dtype=np.float32), np.empty((0,), dtype=np.float32), crop_info, cropped_img\n",
    "        else:  # 'original'\n",
    "            return np.empty((0, 4), dtype=np.float32), np.empty((0,), dtype=np.float32), crop_info\n",
    "    \n",
    "    # Check if input is a list of images\n",
    "    if isinstance(img, list):\n",
    "        return [_process_single_image(single_img) for single_img in img]\n",
    "    else:\n",
    "        return _process_single_image(img)\n",
    "\n",
    "def visualize_hand_detections(image, bboxes, scores=None, thickness=2, color=(0, 255, 0), \n",
    "                             display_scores=True, score_threshold=0.0, \n",
    "                             figsize=(12, 12), save_path=None, display=True):\n",
    "    \"\"\"\n",
    "    Visualize hand bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "        image: Original image as numpy array (RGB)\n",
    "        bboxes: Numpy array of bounding boxes in [x1, y1, x2, y2] format\n",
    "        scores: Optional numpy array of confidence scores for each box\n",
    "        thickness: Thickness of bounding box lines\n",
    "        color: Color of bounding box lines in BGR format (default: green)\n",
    "        display_scores: Whether to display confidence scores\n",
    "        score_threshold: Only display boxes with scores above this threshold\n",
    "        figsize: Size of the displayed figure\n",
    "        save_path: Optional path to save the visualization\n",
    "        display: Whether to display the plot (set to False to avoid showing plots)\n",
    "        \n",
    "    Returns:\n",
    "        vis_image: Numpy array of the visualization image with bounding boxes\n",
    "    \"\"\"\n",
    "    # Make a copy of the image to avoid modifying the original\n",
    "    vis_image = image.copy()\n",
    "    \n",
    "    # Convert to BGR for OpenCV if it's RGB\n",
    "    if vis_image.shape[2] == 3:\n",
    "        vis_image_bgr = cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR)\n",
    "    else:\n",
    "        vis_image_bgr = vis_image.copy()\n",
    "    \n",
    "    # Draw each bounding box\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        # Convert to integers\n",
    "        x1, y1, x2, y2 = bbox.astype(np.int32)\n",
    "        \n",
    "        # Get score if available\n",
    "        score = scores[i] if scores is not None else None\n",
    "        \n",
    "        # Skip boxes with scores below threshold\n",
    "        if score is not None and score < score_threshold:\n",
    "            continue\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(vis_image_bgr, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        # Display score if requested\n",
    "        if display_scores and score is not None:\n",
    "            score_text = f\"{score:.2f}\"\n",
    "            text_position = (x1, y1 - 10)\n",
    "            cv2.putText(vis_image_bgr, score_text, text_position,\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
    "    \n",
    "    # Convert back to RGB for matplotlib display\n",
    "    vis_image = cv2.cvtColor(vis_image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the image if requested\n",
    "    if display:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(vis_image)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save the image if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "            print(f\"Saved visualization to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    elif save_path:\n",
    "        # If not displaying but still need to save\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(vis_image)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()  # Close the figure to avoid memory leaks\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "    \n",
    "    return vis_image\n",
    "\n",
    "def detect_and_visualize_hands(images, \n",
    "                              det_config='configs/rtmdet_nano_320-8xb32_hand.py',\n",
    "                              det_checkpoint='https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth',\n",
    "                              device='cuda:0',\n",
    "                              bbox_thr=0.3,\n",
    "                              nms_thr=0.3,\n",
    "                              visualize=True,  # Now controls whether to create visualizations\n",
    "                              display_plot=True,  # New parameter to control showing the plot\n",
    "                              save_path=None,\n",
    "                              grid_display=False,\n",
    "                              max_images_per_row=3,\n",
    "                              return_coordinates='original',\n",
    "                              crop_size=(600, 600)):\n",
    "    \"\"\"\n",
    "    Detect and visualize hands in a single image or a list of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Path to image, single numpy array, or list of numpy arrays\n",
    "        det_config: Path to detection config file\n",
    "        det_checkpoint: Path or URL to detection checkpoint\n",
    "        device: Device to run inference on\n",
    "        bbox_thr: Threshold for bounding box confidence\n",
    "        nms_thr: Threshold for non-maximum suppression\n",
    "        visualize: Whether to create visualizations with bounding boxes (always returns vis_images)\n",
    "        display_plot: Whether to display the visualization plots\n",
    "        save_path: Optional path to save the visualization\n",
    "        grid_display: When processing multiple images, display them in a grid\n",
    "        max_images_per_row: Maximum number of images per row in the grid\n",
    "        return_coordinates: Coordinate system for returned bounding boxes:\n",
    "                          - 'original': In original input image coordinates (default)\n",
    "                          - 'crop': In center-cropped image coordinates\n",
    "                          - 'model': In model input size coordinates\n",
    "        crop_size: Size for center crop (width, height), defaults to (600, 600)\n",
    "        \n",
    "    Returns:\n",
    "        bboxes_list: List of numpy arrays of hand bounding boxes\n",
    "        scores_list: List of numpy arrays of confidence scores\n",
    "        crop_info_list: List of crop information dictionaries\n",
    "        vis_images: Visualization images with bounding boxes (if visualize=True)\n",
    "        processed_images: List of processed images without bounding boxes (if return_coordinates is 'crop' or 'model')\n",
    "    \"\"\"\n",
    "    # Check if the input is a list or a single image\n",
    "    if isinstance(images, list) or (isinstance(images, np.ndarray) and len(images.shape) == 4):\n",
    "        # We have a list of images\n",
    "        is_list = True\n",
    "        if isinstance(images, np.ndarray):\n",
    "            # Convert 4D array to list of 3D arrays\n",
    "            images = [images[i] for i in range(images.shape[0])]\n",
    "    else:\n",
    "        # We have a single image\n",
    "        is_list = False\n",
    "        images = [images]  # Convert to list for uniform processing\n",
    "    \n",
    "    bboxes_list = []\n",
    "    scores_list = []\n",
    "    vis_images = []\n",
    "    crop_info_list = []\n",
    "    processed_images = []\n",
    "    \n",
    "    # Process each image\n",
    "    for i, image in enumerate(images):\n",
    "        # Load image if path is provided\n",
    "        if isinstance(image, str):\n",
    "            img = cv2.imread(image)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            img = image.copy()\n",
    "        \n",
    "        # Detect hands - handle different return values based on return_coordinates\n",
    "        detect_result = detect_hands(\n",
    "            img, \n",
    "            det_config=det_config,\n",
    "            det_checkpoint=det_checkpoint,\n",
    "            device=device,\n",
    "            bbox_thr=bbox_thr,\n",
    "            nms_thr=nms_thr,\n",
    "            crop_size=crop_size,\n",
    "            return_coordinates=return_coordinates\n",
    "        )\n",
    "        \n",
    "        # Unpack the result based on return_coordinates\n",
    "        if return_coordinates == 'original':\n",
    "            bboxes, scores, crop_info = detect_result\n",
    "            vis_base_img = img  # Use original image for visualization\n",
    "        else:  # 'crop' or 'model'\n",
    "            bboxes, scores, crop_info, processed_img = detect_result\n",
    "            vis_base_img = processed_img  # Use processed image for visualization\n",
    "            processed_images.append(processed_img)\n",
    "        \n",
    "        bboxes_list.append(bboxes)\n",
    "        scores_list.append(scores)\n",
    "        crop_info_list.append(crop_info)\n",
    "        \n",
    "        # Create visualization (always, regardless of display_plot parameter)\n",
    "        if visualize:\n",
    "            # Create individual save path if provided\n",
    "            individual_save_path = None\n",
    "            if save_path and len(images) > 1:\n",
    "                base, ext = os.path.splitext(save_path) if '.' in os.path.basename(save_path) else (save_path, '.jpg')\n",
    "                individual_save_path = f\"{base}_{i}{ext}\"\n",
    "            elif save_path:\n",
    "                individual_save_path = save_path\n",
    "            \n",
    "            # Only display during loop if requested and not using grid display\n",
    "            should_display = display_plot and not (grid_display and len(images) > 1)\n",
    "            \n",
    "            if len(bboxes) > 0:\n",
    "                # Create visualization with bounding boxes\n",
    "                vis_result = visualize_hand_detections(\n",
    "                    vis_base_img,  # Use the appropriate image based on return_coordinates\n",
    "                    bboxes, \n",
    "                    scores, \n",
    "                    save_path=individual_save_path if not grid_display else None,\n",
    "                    display=should_display  # Control whether to show the plot\n",
    "                )\n",
    "                vis_images.append(vis_result)\n",
    "                \n",
    "                if should_display:\n",
    "                    print(f\"Image {i+1}: Detected {len(bboxes)} hands\")\n",
    "            else:\n",
    "                # No hands detected, just use the base image\n",
    "                vis_images.append(vis_base_img)\n",
    "                if should_display:\n",
    "                    print(f\"Image {i+1}: No hands detected\")\n",
    "        else:\n",
    "            # If not visualizing, just add the base image without bounding boxes\n",
    "            vis_images.append(vis_base_img)\n",
    "    \n",
    "    # Create a grid visualization for multiple images\n",
    "    if visualize and grid_display and len(images) > 1:\n",
    "        # Calculate grid dimensions\n",
    "        num_images = len(vis_images)\n",
    "        num_cols = min(max_images_per_row, num_images)\n",
    "        num_rows = (num_images + num_cols - 1) // num_cols  # Ceiling division\n",
    "        \n",
    "        # Create figure and plot images\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 5*num_rows))\n",
    "        \n",
    "        # Make axes accessible in a uniform way\n",
    "        if num_rows == 1 and num_cols == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif num_rows == 1 or num_cols == 1:\n",
    "            axes = axes.reshape(num_rows, num_cols)\n",
    "        \n",
    "        # Plot each image\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                idx = i * num_cols + j\n",
    "                if idx < num_images:\n",
    "                    axes[i, j].imshow(vis_images[idx])\n",
    "                    axes[i, j].set_title(f\"Image {idx+1}: {len(bboxes_list[idx])} hands\")\n",
    "                    axes[i, j].axis('off')\n",
    "                else:\n",
    "                    axes[i, j].axis('off')  # Hide unused subplots\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save grid if requested\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"Saved grid visualization to {save_path}\")\n",
    "        \n",
    "        # Only display the grid if requested\n",
    "        if display_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()  # Close the figure to avoid memory leaks\n",
    "    \n",
    "    # Return appropriate values based on input type and return_coordinates\n",
    "    if not is_list:\n",
    "        # Single image was provided\n",
    "        if return_coordinates == 'original':\n",
    "            return bboxes_list[0], scores_list[0], crop_info_list[0], vis_images[0]\n",
    "        else:  # 'crop' or 'model'\n",
    "            return bboxes_list[0], scores_list[0], crop_info_list[0], vis_images[0], processed_images[0]\n",
    "    else:\n",
    "        # List of images was provided\n",
    "        if return_coordinates == 'original':\n",
    "            return bboxes_list, scores_list, crop_info_list, vis_images\n",
    "        else:  # 'crop' or 'model'\n",
    "            return bboxes_list, scores_list, crop_info_list, vis_images, processed_images\n",
    "\n",
    "# recording = processed_data.get(\"recording_0\")\n",
    "# rgb_image = recording[\"color_images\"]\n",
    "\n",
    "# boxes_list, scores_list, crop_info_list, vis_images, processed_imgs = detect_and_visualize_hands(\n",
    "#     rgb_image,\n",
    "#     return_coordinates='crop',\n",
    "#     crop_size=(800, 800),\n",
    "#     visualize=True,\n",
    "#     display_plot=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44779813",
   "metadata": {},
   "source": [
    "### 2.4. Cropping to Target Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e4c8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Optional, Union\n",
    "\n",
    "def crop_hands_sequence(images: Union[List[np.ndarray], np.ndarray],\n",
    "                      output_size: Tuple[int, int] = (300, 300),\n",
    "                      margin_percent: float = 0.2,\n",
    "                      det_config: str = 'configs/rtmdet_nano_320-8xb32_hand.py',\n",
    "                      det_checkpoint: str = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth',\n",
    "                      device: str = 'cuda:0',\n",
    "                      bbox_thr: float = 0.3,\n",
    "                      nms_thr: float = 0.3,\n",
    "                      suppress_warnings: bool = True,\n",
    "                      save: bool = False,\n",
    "                      save_path: Optional[str] = None) -> Tuple[List[np.ndarray], Tuple[int, int, int, int]]:\n",
    "    \"\"\"\n",
    "    Crop hands from a sequence of images using the same crop coordinates for all frames.\n",
    "    \n",
    "    Args:\n",
    "        images: List of RGB images or a 4D array (frames, height, width, channels)\n",
    "        output_size: Size of the output images (width, height), defaults to (300, 300)\n",
    "        margin_percent: Extra margin to add around detected hand (as percentage of bbox dims)\n",
    "        det_config: Path to detection config file\n",
    "        det_checkpoint: Path or URL to detection checkpoint\n",
    "        device: Device to run inference on\n",
    "        bbox_thr: Threshold for bounding box confidence\n",
    "        nms_thr: Threshold for non-maximum suppression\n",
    "        suppress_warnings: Whether to suppress warning messages\n",
    "        save: Whether to save the cropped sequence as a .npy file\n",
    "        save_path: Path where to save the .npy file (required if save=True)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - List of cropped hand images of size output_size\n",
    "            - Crop coordinates (x_min, y_min, x_max, y_max) used for all images\n",
    "    \"\"\"\n",
    "    # Convert input to list of images if needed\n",
    "    if isinstance(images, np.ndarray) and len(images.shape) == 4:\n",
    "        images_list = [images[i] for i in range(images.shape[0])]\n",
    "    elif not isinstance(images, list):\n",
    "        images_list = [images]  # Single image case\n",
    "    else:\n",
    "        images_list = images\n",
    "    \n",
    "    # Get the first image for hand detection\n",
    "    first_image = images_list[0]\n",
    "    \n",
    "    # 1. Detect hand in the first frame\n",
    "    bboxes, scores, _ = detect_hands(\n",
    "        first_image,\n",
    "        det_config=det_config,\n",
    "        det_checkpoint=det_checkpoint,\n",
    "        device=device,\n",
    "        bbox_thr=bbox_thr,\n",
    "        nms_thr=nms_thr,\n",
    "        suppress_warnings=suppress_warnings,\n",
    "        crop_size=(800, 800),\n",
    "        return_coordinates='original'\n",
    "    )\n",
    "    \n",
    "    # Check if any hands were detected\n",
    "    if len(bboxes) == 0:\n",
    "        # No hands detected, return the original images resized\n",
    "        crop_transform = A.Resize(height=output_size[1], width=output_size[0])\n",
    "        resized_imgs = [crop_transform(image=img)['image'] for img in images_list]\n",
    "        \n",
    "        # Save the resized images as .npy if requested\n",
    "        if save:\n",
    "            if save_path is None:\n",
    "                raise ValueError(\"save_path must be provided when save=True\")\n",
    "            \n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)\n",
    "            \n",
    "            # Convert list to numpy array and save\n",
    "            np_sequence = np.array(resized_imgs)\n",
    "            np.save(save_path, np_sequence)\n",
    "            print(f\"Saved resized sequence to {save_path}, shape: {np_sequence.shape}\")\n",
    "            \n",
    "        return resized_imgs, (0, 0, first_image.shape[1], first_image.shape[0])\n",
    "    \n",
    "    # Use the first detected hand (highest confidence)\n",
    "    x1, y1, x2, y2 = bboxes[0]\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    # 2. Add margin to the bounding box\n",
    "    margin_w = width * margin_percent\n",
    "    margin_h = height * margin_percent\n",
    "    \n",
    "    crop_x1 = max(0, int(x1 - margin_w))\n",
    "    crop_y1 = max(0, int(y1 - margin_h))\n",
    "    crop_x2 = min(first_image.shape[1], int(x2 + margin_w))\n",
    "    crop_y2 = min(first_image.shape[0], int(y2 + margin_h))\n",
    "    \n",
    "    # 3. Create crop transform\n",
    "    crop_transform = A.Compose([\n",
    "        A.Crop(x_min=crop_x1, y_min=crop_y1, x_max=crop_x2, y_max=crop_y2),\n",
    "        A.Resize(height=output_size[1], width=output_size[0])\n",
    "    ])\n",
    "    \n",
    "    # Apply the same crop transform to all images\n",
    "    cropped_imgs = []\n",
    "    for img in images_list:\n",
    "        # Handle images of different sizes\n",
    "        if img.shape[:2] != first_image.shape[:2]:\n",
    "            # Resize to match first image\n",
    "            img = cv2.resize(img, (first_image.shape[1], first_image.shape[0]))\n",
    "        \n",
    "        # Apply crop transform\n",
    "        cropped_img = crop_transform(image=img)['image']\n",
    "        cropped_imgs.append(cropped_img)\n",
    "    \n",
    "    # Save the cropped sequence as .npy if requested\n",
    "    if save:\n",
    "        if save_path is None:\n",
    "            raise ValueError(\"save_path must be provided when save=True\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(save_path)), exist_ok=True)\n",
    "        \n",
    "        # Convert list to numpy array and save\n",
    "        np_sequence = np.array(cropped_imgs)\n",
    "        np.save(save_path, np_sequence)\n",
    "        print(f\"Saved cropped sequence to {save_path}, shape: {np_sequence.shape}\")\n",
    "    \n",
    "    # 4. Return the cropped images and crop coordinates\n",
    "    return cropped_imgs, (crop_x1, crop_y1, crop_x2, crop_y2)\n",
    "\n",
    "# recording = processed_data.get(\"recording_0\")\n",
    "# rgb_images = recording[\"color_images\"]\n",
    "\n",
    "# cropped_frames, _ = crop_hands_sequence(\n",
    "#     rgb_images,\n",
    "#     output_size=(300, 300),\n",
    "#     margin_percent=0.5,\n",
    "#     save=True,\n",
    "#     save_path=\"cropped_hands_sequence.npy\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a96ace",
   "metadata": {},
   "source": [
    "### 2.5. Windowing Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the color images from recording_0\n",
    "rgb_images = recording_0[\"color_images\"]\n",
    "\n",
    "cropped_frames, _ = crop_hands_sequence(\n",
    "    rgb_images,\n",
    "    output_size=(300, 300),\n",
    "    margin_percent=0.5\n",
    ")\n",
    "\n",
    "# Get multiple windows of 16 frames each\n",
    "rgb_windows = sliding_window_sample(cropped_frames, window_size=16, stride=8)\n",
    "\n",
    "# Visualize the windows\n",
    "visualize_windows(rgb_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c54758",
   "metadata": {},
   "source": [
    "### 2.6. Save Windowed Data into ðŸ“ `rgb` and ðŸ“ `depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1749f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the recordings dictionary\n",
    "recordings = prepare_recordings_dict(processed_data)\n",
    "\n",
    "# Create a new dictionary to store the hand-cropped recordings\n",
    "cropped_recordings = {}\n",
    "output_size = (300, 300)  # Output size for cropped images\n",
    "margin_percent = 0.5      # Extra margin around detected hands\n",
    "\n",
    "# Insert loop to process each recording with `crop_hands_sequence`\n",
    "for recording_key, recording_data in recordings.items():\n",
    "    print(f\"Processing {recording_key} for hand cropping...\")\n",
    "    \n",
    "    # Create a copy of the recording data to modify\n",
    "    cropped_recording_data = {\n",
    "        \"dataset\": recording_data[\"dataset\"]  # Preserve dataset identifier\n",
    "    }\n",
    "    \n",
    "    # Process color images (RGB) first\n",
    "    if \"color_images\" in recording_data and recording_data[\"color_images\"] is not None:\n",
    "        color_images = recording_data[\"color_images\"]\n",
    "        print(f\"  Found {len(color_images)} color frames\")\n",
    "        \n",
    "        # Apply hand detection and cropping to color images\n",
    "        cropped_color_images, crop_coords = crop_hands_sequence(\n",
    "            color_images,\n",
    "            output_size=output_size,\n",
    "            margin_percent=margin_percent,\n",
    "            device='cuda:0',  # Adjust based on your setup\n",
    "            bbox_thr=0.3,\n",
    "            nms_thr=0.3\n",
    "        )\n",
    "        \n",
    "        # Store the cropped color images\n",
    "        cropped_recording_data[\"color_images\"] = np.array(cropped_color_images)\n",
    "        cropped_recording_data[\"crop_info\"] = {\n",
    "            \"coords\": crop_coords,\n",
    "            \"output_size\": output_size\n",
    "        }\n",
    "        \n",
    "        print(f\"  Cropped {len(cropped_color_images)} color frames to size {output_size}\")\n",
    "    else:\n",
    "        print(f\"  No color images found for {recording_key}\")\n",
    "        cropped_recording_data[\"color_images\"] = None\n",
    "    \n",
    "    # Process depth images using the same crop coordinates\n",
    "    if \"depth_images\" in recording_data and recording_data[\"depth_images\"] is not None:\n",
    "        depth_images = recording_data[\"depth_images\"]\n",
    "        print(f\"  Found {len(depth_images)} depth frames\")\n",
    "        \n",
    "        # Only proceed if we have crop coordinates from color images\n",
    "        if \"crop_info\" in cropped_recording_data:\n",
    "            crop_coords = cropped_recording_data[\"crop_info\"][\"coords\"]\n",
    "            \n",
    "            # Create crop transform\n",
    "            x_min, y_min, x_max, y_max = crop_coords\n",
    "            crop_transform = A.Compose([\n",
    "                A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max),\n",
    "                A.Resize(height=output_size[1], width=output_size[0])\n",
    "            ])\n",
    "            \n",
    "            # Apply crop transform to depth images\n",
    "            cropped_depth_images = []\n",
    "            for depth_frame in depth_images:\n",
    "                # Check if depth is 2D (single channel) or 3D\n",
    "                if len(depth_frame.shape) == 2:\n",
    "                    # Convert to 3-channel for albumentations\n",
    "                    depth_3ch = np.stack([depth_frame] * 3, axis=-1)\n",
    "                    transformed = crop_transform(image=depth_3ch)['image']\n",
    "                    # Extract first channel to return to single-channel\n",
    "                    cropped_depth_images.append(transformed[:, :, 0])\n",
    "                else:\n",
    "                    transformed = crop_transform(image=depth_frame)['image']\n",
    "                    cropped_depth_images.append(transformed)\n",
    "            \n",
    "            # Store the cropped depth images\n",
    "            cropped_recording_data[\"depth_images\"] = np.array(cropped_depth_images)\n",
    "            print(f\"  Cropped {len(cropped_depth_images)} depth frames to size {output_size}\")\n",
    "        else:\n",
    "            # No crop coordinates available (likely no color images), just resize\n",
    "            print(f\"  No crop coordinates available for {recording_key}, resizing depth images\")\n",
    "            resize_transform = A.Resize(height=output_size[1], width=output_size[0])\n",
    "            resized_depth_images = [\n",
    "                resize_transform(image=frame)['image'] for frame in depth_images\n",
    "            ]\n",
    "            cropped_recording_data[\"depth_images\"] = np.array(resized_depth_images)\n",
    "    else:\n",
    "        print(f\"  No depth images found for {recording_key}\")\n",
    "        cropped_recording_data[\"depth_images\"] = None\n",
    "    \n",
    "    # Store the cropped recording data\n",
    "    cropped_recordings[recording_key] = cropped_recording_data\n",
    "    print(f\"Completed processing {recording_key}\")\n",
    "\n",
    "save_windowed_data(cropped_recordings, bag_files, windowed_data_dir='windowed_hand_focused_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f3535",
   "metadata": {},
   "source": [
    "### 2.7. Data Class Generation\n",
    "\n",
    "There are 5 class to be pick up from the windowed data: `Hand Open (HO)`, `Intrinsic Hand (IH)`, `Straight Fist (SF)`, `Hand Close (HC)`, and `Hook Hand (HH)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(recordings, bag_files, class_info_dict, output_dir=\"video\", max_samples_per_recording=2, windowed_dir='windowed_data', modality_option=\"both\"):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset with RGB and depth samples for each class from each recording.\n",
    "    \n",
    "    Args:\n",
    "        recordings: Dictionary containing recordings with processed data\n",
    "        bag_files: List of bag file paths\n",
    "        class_info_dict: Dictionary mapping recording names to class window indices\n",
    "        output_dir: Output directory name\n",
    "        max_samples_per_recording: Maximum number of samples to include per class per recording\n",
    "    \"\"\"\n",
    "    # Define class acronyms and their full names\n",
    "    class_acronyms = [\"OH\", \"IH\", \"SF\", \"HC\", \"HH\"]\n",
    "    class_full_names = {\n",
    "        \"OH\": \"Open Hand\",\n",
    "        \"IH\": \"Intrinsic Hand\",\n",
    "        \"SF\": \"Straight Fist\",\n",
    "        \"HC\": \"Hand Close\",\n",
    "        \"HH\": \"Hook Hand\"\n",
    "    }\n",
    "    \n",
    "    # Define modalities based on the modality_option parameter\n",
    "    if modality_option.lower() == \"rgb\":\n",
    "        modalities = [\"rgb\"]\n",
    "    elif modality_option.lower() == \"depth\":\n",
    "        modalities = [\"depth\"]\n",
    "    elif modality_option.lower() == \"both\":\n",
    "        modalities = [\"rgb\", \"depth\"]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid modality_option. Choose 'rgb', 'depth', or 'both'.\")\n",
    "    \n",
    "    # Create main output directory\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create class and modality folders\n",
    "    for acronym in class_acronyms:\n",
    "        for modality in modalities:\n",
    "            class_modality_folder = output_path / modality / acronym\n",
    "            os.makedirs(class_modality_folder, exist_ok=True)\n",
    "    \n",
    "    # Track which samples we've collected for each class\n",
    "    collected_samples = defaultdict(list)\n",
    "    \n",
    "    # Process each recording\n",
    "    for recording_idx, bag_file_path in enumerate(bag_files):\n",
    "        bag_path = Path(bag_file_path)\n",
    "        recording_name = bag_path.stem\n",
    "        recording_key = f\"recording_{recording_idx}\"\n",
    "        \n",
    "        # Skip if recording doesn't exist in our data\n",
    "        if recording_key not in recordings:\n",
    "            print(f\"Warning: {recording_key} not found in recordings, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Skip if recording doesn't have class information\n",
    "        if recording_name not in class_info_dict:\n",
    "            print(f\"Warning: No class information for {recording_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get class information for this recording\n",
    "        recording_class_info = class_info_dict[recording_name]\n",
    "        \n",
    "        # Process each class in this recording\n",
    "        for acronym in class_acronyms:\n",
    "            # Get available window indices for this class\n",
    "            window_indices = recording_class_info.get(acronym, [])\n",
    "            \n",
    "            # Limit to the maximum number of samples per recording\n",
    "            selected_indices = window_indices[:max_samples_per_recording]\n",
    "            \n",
    "            # For each selected window index\n",
    "            for window_idx in selected_indices:\n",
    "                # Process each modality (rgb and depth)\n",
    "                for modality in modalities:\n",
    "                    # Get windowed data directory for this recording and modality\n",
    "                    data_dir = bag_path.parent / windowed_dir / modality\n",
    "                    \n",
    "                    # Build the source filename\n",
    "                    window_filename = f\"{recording_name}_window_{window_idx:03d}.npy\"\n",
    "                    source_path = data_dir / window_filename\n",
    "                    \n",
    "                    # Skip if source file doesn't exist\n",
    "                    if not source_path.exists():\n",
    "                        print(f\"Warning: {modality} file {source_path} not found, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Create destination filename\n",
    "                    dest_filename = f\"{recording_name}_w{window_idx:03d}.npy\"\n",
    "                    dest_path = output_path / modality / acronym / dest_filename\n",
    "                    \n",
    "                    # Copy the file\n",
    "                    shutil.copy2(source_path, dest_path)\n",
    "                \n",
    "                # Track which samples we've collected (only track once per sample, not per modality)\n",
    "                collected_samples[acronym].append((recording_name, window_idx))\n",
    "                \n",
    "                print(f\"Copied {class_full_names[acronym]} ({acronym}) sample from {recording_name}, window {window_idx} (both rgb and depth)\")\n",
    "    \n",
    "    # Check if we have enough samples for each class (12 samples per class)\n",
    "    target_count = len(bag_files) * max_samples_per_recording\n",
    "    for acronym in class_acronyms:\n",
    "        samples_count = len(collected_samples[acronym])\n",
    "        \n",
    "        print(f\"Class {class_full_names[acronym]} ({acronym}): {samples_count}/{target_count} samples collected\")\n",
    "        \n",
    "        # If we don't have enough samples, try to get more from recordings with extra samples\n",
    "        if samples_count < target_count:\n",
    "            additional_needed = target_count - samples_count\n",
    "            print(f\"Need {additional_needed} more samples for {acronym}\")\n",
    "            \n",
    "            # Find all available samples for this class across all recordings\n",
    "            all_available = []\n",
    "            for recording_idx, bag_file_path in enumerate(bag_files):\n",
    "                bag_path = Path(bag_file_path)\n",
    "                recording_name = bag_path.stem\n",
    "                \n",
    "                if recording_name in class_info_dict and acronym in class_info_dict[recording_name]:\n",
    "                    # Get indices that haven't been used yet\n",
    "                    used_indices = [idx for name, idx in collected_samples[acronym] if name == recording_name]\n",
    "                    available_indices = [idx for idx in class_info_dict[recording_name][acronym] if idx not in used_indices]\n",
    "                    \n",
    "                    for idx in available_indices:\n",
    "                        all_available.append((recording_name, idx))\n",
    "            \n",
    "            # Use extra samples to fill in\n",
    "            for i, (recording_name, window_idx) in enumerate(all_available):\n",
    "                if i >= additional_needed:\n",
    "                    break\n",
    "                    \n",
    "                # Find the bag file path for this recording\n",
    "                recording_bag_path = None\n",
    "                for bag_file_path in bag_files:\n",
    "                    if Path(bag_file_path).stem == recording_name:\n",
    "                        recording_bag_path = Path(bag_file_path)\n",
    "                        break\n",
    "                \n",
    "                if recording_bag_path is None:\n",
    "                    continue\n",
    "                \n",
    "                # Process each modality for this additional sample\n",
    "                for modality in modalities:\n",
    "                    # Get the source path\n",
    "                    data_dir = recording_bag_path.parent / windowed_dir / modality\n",
    "                    window_filename = f\"{recording_name}_window_{window_idx:03d}.npy\"\n",
    "                    source_path = data_dir / window_filename\n",
    "                    \n",
    "                    # Skip if source file doesn't exist\n",
    "                    if not source_path.exists():\n",
    "                        print(f\"Warning: Additional {modality} file {source_path} not found, skipping...\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Create destination filename\n",
    "                    dest_filename = f\"{recording_name}_w{window_idx:03d}.npy\"\n",
    "                    dest_path = output_path / modality / acronym / dest_filename\n",
    "                    \n",
    "                    # Copy the file\n",
    "                    shutil.copy2(source_path, dest_path)\n",
    "                \n",
    "                # Track which samples we've collected (only once per sample)\n",
    "                collected_samples[acronym].append((recording_name, window_idx))\n",
    "                \n",
    "                print(f\"Added extra {class_full_names[acronym]} ({acronym}) sample from {recording_name}, window {window_idx} (both rgb and depth)\")\n",
    "    \n",
    "    # Final count and validation\n",
    "    for acronym in class_acronyms:\n",
    "        rgb_count = len(list((output_path / \"rgb\" / acronym).glob(\"*.npy\")))\n",
    "        depth_count = len(list((output_path / \"depth\" / acronym).glob(\"*.npy\")))\n",
    "        \n",
    "        print(f\"Final count for {class_full_names[acronym]} ({acronym}):\")\n",
    "        print(f\"  - RGB: {rgb_count} samples\")\n",
    "        print(f\"  - Depth: {depth_count} samples\")\n",
    "        \n",
    "        # Check if counts match\n",
    "        if rgb_count != depth_count:\n",
    "            print(f\"  - WARNING: RGB and depth counts don't match for {acronym}!\")\n",
    "\n",
    "# Use the provided dictionary directly\n",
    "info_dict = {\n",
    "    \"Record_20250402151124\": {\n",
    "        \"OH\": [0, 10, 11],\n",
    "        \"IH\": [1, 2],\n",
    "        \"SF\": [3, 4],\n",
    "        \"HC\": [5, 6],\n",
    "        \"HH\": [7, 8]\n",
    "    },\n",
    "    \"Record_20250402151609\": {\n",
    "        \"OH\": [0, 9],\n",
    "        \"IH\": [1, 2],\n",
    "        \"SF\": [3, 4],\n",
    "        \"HC\": [5],\n",
    "        \"HH\": [6, 7]\n",
    "    },\n",
    "    \"Record_20250402152331\": {\n",
    "        \"OH\": [0, 1, 2, 12],\n",
    "        \"IH\": [3, 4],\n",
    "        \"SF\": [5, 6],\n",
    "        \"HC\": [7],\n",
    "        \"HH\": [8, 9]\n",
    "    },\n",
    "    \"Record_20250506145704\": {\n",
    "        \"OH\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        \"IH\": [11, 12, 13, 14, 15],\n",
    "        \"SF\": [16, 17, 18, 19, 20, 21],\n",
    "        \"HC\": [22, 23, 24, 25, 26],\n",
    "        \"HH\": [27, 28, 29, 30, 31]\n",
    "    },\n",
    "    \"Record_20250506152951\": {\n",
    "        \"OH\": [0, 1, 2, 3, 4],\n",
    "        \"IH\": [5, 6, 7, 8],\n",
    "        \"SF\": [10, 11, 12, 13],\n",
    "        \"HC\": [14, 15, 16, 17],\n",
    "        \"HH\": [18, 19, 20, 21]\n",
    "    },\n",
    "    \"Record_20250506162630\": {\n",
    "        \"OH\": [0, 1, 2, 3, 4, 5],\n",
    "        \"IH\": [6, 7],\n",
    "        \"SF\": [8],\n",
    "        \"HC\": [9, 10, 11, 12],\n",
    "        \"HH\": [13, 14]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the dataset\n",
    "create_dataset(processed_data, bag_files, info_dict, output_dir=r\"./data/video_hand_focused_data\", windowed_dir='windowed_hand_focused_data', modality_option=\"rgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1af4b",
   "metadata": {},
   "source": [
    "### 2.8. Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd79d8",
   "metadata": {},
   "source": [
    "#### Splitting into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "video_dir = r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\DATA\\video_hand_focused_data\\rgb'\n",
    "save_dir = os.path.join(os.path.dirname(video_dir), 'split_rgb')\n",
    "\n",
    "# Define subfolders\n",
    "train_dir = os.path.join(save_dir, 'train')\n",
    "val_dir = os.path.join(save_dir, 'val')\n",
    "\n",
    "# Create directory structure\n",
    "for target_dir in [train_dir, val_dir]:\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Loop over each class folder\n",
    "for class_name in os.listdir(video_dir):\n",
    "    class_path = os.path.join(video_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    # List all .npy files in the class\n",
    "    npy_files = glob.glob(os.path.join(class_path, '*.npy'))\n",
    "\n",
    "    # Split using sklearn\n",
    "    train_files, val_files = train_test_split(npy_files, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Destination subfolders\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    val_class_dir = os.path.join(val_dir, class_name)\n",
    "    os.makedirs(train_class_dir, exist_ok=True)\n",
    "    os.makedirs(val_class_dir, exist_ok=True)\n",
    "\n",
    "    # Copy files to train\n",
    "    for f in train_files:\n",
    "        shutil.copy2(f, os.path.join(train_class_dir, os.path.basename(f)))\n",
    "\n",
    "    # Copy files to val\n",
    "    for f in val_files:\n",
    "        shutil.copy2(f, os.path.join(val_class_dir, os.path.basename(f)))\n",
    "\n",
    "print(f\"Dataset split complete. Saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832584a",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d116c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "video_dir       = r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\DATA\\video_hand_focused_data'\n",
    "train_dir       = os.path.join(video_dir, \"split_rgb\", \"train\")\n",
    "aug_dir         = os.path.join(os.path.dirname(train_dir), \"train_aug\")\n",
    "n_applications  = 6   # how many aug variants per original\n",
    "\n",
    "transform = A.ReplayCompose([\n",
    "    A.ElasticTransform(alpha=0.5, p=0.5),\n",
    "    A.ShiftScaleRotate(scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=50, g_shift_limit=50, b_shift_limit=50, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.CLAHE(p=0.5),\n",
    "    A.PixelDropout(drop_value=0, dropout_prob=0.01, p=0.5),\n",
    "    A.PixelDropout(drop_value=255, dropout_prob=0.01, p=0.5),\n",
    "    A.Blur(blur_limit=(2, 4), p=0.5)\n",
    "])\n",
    "\n",
    "def augment_numpy_video(arr: np.ndarray):\n",
    "    # Original data is in T, H, W, C format\n",
    "    T, H, W, C = arr.shape\n",
    "    out_frames, replay = [], None\n",
    "\n",
    "    for t in range(T):\n",
    "        # No need to transpose the frame since it's already in H, W, C format\n",
    "        frame = arr[t].astype(\"uint8\")\n",
    "        if t == 0:\n",
    "            data   = transform(image=frame)\n",
    "            new    = data[\"image\"]\n",
    "            replay = data[\"replay\"]\n",
    "        else:\n",
    "            new = A.ReplayCompose.replay(replay, image=frame)[\"image\"]\n",
    "        out_frames.append(new)\n",
    "\n",
    "    # Stack without transposing to keep H, W, C format\n",
    "    return np.stack(out_frames, axis=0)\n",
    "\n",
    "# Walk each class folder\n",
    "for cls in tqdm(os.listdir(train_dir), desc=\"Processing classes\"):\n",
    "    src_cls_dir = os.path.join(train_dir, cls)\n",
    "    dst_cls_dir = os.path.join(aug_dir, cls)\n",
    "\n",
    "    # 1) Clear out any previous augmented files\n",
    "    if os.path.isdir(dst_cls_dir):\n",
    "        for old in os.listdir(dst_cls_dir):\n",
    "            if \"_aug\" in old or old.endswith(\"_orig.npy\"):\n",
    "                os.remove(os.path.join(dst_cls_dir, old))\n",
    "    else:\n",
    "        os.makedirs(dst_cls_dir, exist_ok=True)\n",
    "\n",
    "    # 2) Generate fresh augmentations\n",
    "    file_list = [f for f in os.listdir(src_cls_dir) if f.endswith(\".npy\")]\n",
    "    for fname in tqdm(file_list, desc=f\"Augmenting {cls}\", leave=False):\n",
    "        src_path = os.path.join(src_cls_dir, fname)\n",
    "        base, _  = os.path.splitext(fname)\n",
    "\n",
    "        arr = np.load(src_path)\n",
    "\n",
    "        # Save a base copy\n",
    "        orig_path = os.path.join(dst_cls_dir, f\"{base}_orig.npy\")\n",
    "        np.save(orig_path, arr)\n",
    "\n",
    "        # Generate N augmentations\n",
    "        for i in range(1, n_applications + 1):\n",
    "            aug_arr = augment_numpy_video(arr)\n",
    "            out_path = os.path.join(dst_cls_dir, f\"{base}_aug{i}.npy\")\n",
    "            np.save(out_path, aug_arr)\n",
    "\n",
    "    print(f\"[{cls}] now has {len(os.listdir(dst_cls_dir))} files in {dst_cls_dir}\")\n",
    "\n",
    "print(\"âœ… Done creating fresh synthetic training videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045eaec2",
   "metadata": {},
   "source": [
    "#### Visualize Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70856e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun.blueprint as rrb\n",
    "import rerun as rr\n",
    "\n",
    "def rerun_visualization_from_npy(npy_path: str):\n",
    "    # Load the .npy video: shape (T, C, H, W)\n",
    "    video_array = np.load(npy_path)\n",
    "    print(f\"Loaded {npy_path}, shape: {video_array.shape}\")\n",
    "\n",
    "    stream = rr.new_recording(\"rerun_augmented_video\", spawn=True)\n",
    "\n",
    "    # Configure layout\n",
    "    blueprint = rrb.Blueprint(\n",
    "        rrb.Grid(\n",
    "            rrb.Vertical(\n",
    "                rrb.Spatial2DView(origin=\"/color_image\"),\n",
    "            ),\n",
    "        ),\n",
    "        collapse_panels=True,\n",
    "    )\n",
    "\n",
    "    # Log each frame\n",
    "    for idx, frame in enumerate(video_array):\n",
    "        stream.set_time_sequence(\"frame\", idx)\n",
    "        stream.log(\"color_image\", rr.Image(frame))\n",
    "\n",
    "    stream.send_blueprint(blueprint)\n",
    "\n",
    "classes = ['HC', 'HH', 'IH', 'OH', 'SF']\n",
    "target_dir = os.path.join(aug_dir, classes[0])\n",
    "\n",
    "# Files with _aug{i} suffix\n",
    "augmented_files = glob.glob(os.path.join(target_dir, '*_aug*.npy'))\n",
    "\n",
    "# Visualize first one\n",
    "rerun_visualization_from_npy(augmented_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ef796",
   "metadata": {},
   "source": [
    "### 2.9. Extract Hand Landmarks `X` and `Y` Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5aaed",
   "metadata": {},
   "source": [
    "#### Hand Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6ca574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmpose.apis import inference_topdown\n",
    "from mmpose.structures import merge_data_samples\n",
    "\n",
    "def pose(\n",
    "    images: Union[np.ndarray, List[np.ndarray]],\n",
    "    boxes: Union[np.ndarray, List[np.ndarray]],\n",
    "    scores: Optional[Union[np.ndarray, List[np.ndarray]]] = None,\n",
    "    pose_config: str = 'configs/rtmpose-m_8xb256-210e_hand5-256x256.py',\n",
    "    pose_checkpoint: str = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-hand5_pt-aic-coco_210e-256x256-74fb594_20230320.pth',\n",
    "    device: str = 'cuda:0',\n",
    "    normalize_keypoints: bool = False,\n",
    "    return_full_samples: bool = False\n",
    ") -> Union[Dict[str, Any], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Estimate pose keypoints for hand(s) in a single image or a list of images.\n",
    "    \n",
    "    Args:\n",
    "        images: A single image or list of images (numpy arrays in RGB format)\n",
    "        boxes: Bounding box(es) for hand detection either for a single image or matching the image list\n",
    "        scores: Optional confidence scores for the bounding boxes\n",
    "        pose_config: Path to pose estimation config file\n",
    "        pose_checkpoint: Path or URL to pose estimation checkpoint\n",
    "        device: Device to run inference on ('cuda:0', 'cpu', etc.)\n",
    "        normalize_keypoints: Whether to normalize keypoint coordinates to 0-1 range\n",
    "        return_full_samples: Whether to return the full data_samples or just the keypoints and scores\n",
    "        \n",
    "    Returns:\n",
    "        For a single image:\n",
    "            A dictionary containing:\n",
    "                - 'keypoints': numpy array of shape (num_hands, num_keypoints, 2) with x,y coordinates\n",
    "                - 'scores': numpy array of shape (num_hands, num_keypoints) with confidence scores\n",
    "                - 'boxes': numpy array of shape (num_hands, 4) with bounding boxes\n",
    "                - 'box_scores': numpy array of shape (num_hands) with bounding box confidence scores\n",
    "                - (Optional) 'data_samples': full pose estimation results\n",
    "                \n",
    "        For multiple images:\n",
    "            A list of dictionaries as described above, one for each image\n",
    "    \"\"\"\n",
    "    global MODEL_CACHE\n",
    "    \n",
    "    # Initialize or get cached pose estimator\n",
    "    model_key = f\"{pose_config}_{pose_checkpoint}_{device}\"\n",
    "    if not hasattr(pose, 'MODEL_CACHE'):\n",
    "        pose.MODEL_CACHE = {}\n",
    "        \n",
    "    if model_key not in pose.MODEL_CACHE:\n",
    "        pose_estimator = init_pose_estimator(\n",
    "            pose_config,\n",
    "            pose_checkpoint,\n",
    "            device=device\n",
    "        )\n",
    "        pose.MODEL_CACHE[model_key] = pose_estimator\n",
    "    else:\n",
    "        pose_estimator = pose.MODEL_CACHE[model_key]\n",
    "    \n",
    "    # Handle single image vs list of images\n",
    "    is_single_image = not isinstance(images, list)\n",
    "    \n",
    "    if is_single_image:\n",
    "        images = [images]\n",
    "        boxes = [boxes]\n",
    "        if scores is not None:\n",
    "            scores = [scores]\n",
    "    \n",
    "    # Make sure each image has corresponding boxes\n",
    "    assert len(images) == len(boxes), f\"Number of images ({len(images)}) must match number of box sets ({len(boxes)})\"\n",
    "    \n",
    "    # Process each image\n",
    "    all_results = []\n",
    "    \n",
    "    for i, (image, image_boxes) in enumerate(zip(images, boxes)):\n",
    "        # Skip processing if no hands detected\n",
    "        if len(image_boxes) == 0:\n",
    "            result = {\n",
    "                'keypoints': np.empty((0, 21, 2), dtype=np.float32),\n",
    "                'scores': np.empty((0, 21), dtype=np.float32),\n",
    "                'boxes': np.empty((0, 4), dtype=np.float32),\n",
    "                'box_scores': np.empty((0,), dtype=np.float32),\n",
    "            }\n",
    "            if return_full_samples:\n",
    "                result['data_samples'] = None\n",
    "            all_results.append(result)\n",
    "            continue\n",
    "            \n",
    "        # Run pose estimation\n",
    "        pose_results = inference_topdown(pose_estimator, image, image_boxes)\n",
    "        data_samples = merge_data_samples(pose_results)\n",
    "        \n",
    "        # Extract keypoints and scores\n",
    "        result = {}\n",
    "        \n",
    "        if hasattr(data_samples, 'pred_instances'):\n",
    "            # Extract keypoints from pred_instances\n",
    "            if hasattr(data_samples.pred_instances, 'keypoints'):\n",
    "                keypoints = data_samples.pred_instances.keypoints\n",
    "                keypoint_scores = data_samples.pred_instances.keypoint_scores\n",
    "                \n",
    "                # Normalize keypoint coordinates if requested\n",
    "                if normalize_keypoints:\n",
    "                    img_height, img_width = image.shape[:2]\n",
    "                    keypoints[:, :, 0] /= img_width\n",
    "                    keypoints[:, :, 1] /= img_height\n",
    "                \n",
    "                result['keypoints'] = keypoints\n",
    "                result['scores'] = keypoint_scores\n",
    "                result['boxes'] = image_boxes\n",
    "                \n",
    "                # Add box scores if available\n",
    "                if scores is not None:\n",
    "                    result['box_scores'] = scores[i]\n",
    "                else:\n",
    "                    # Default box scores to 1.0 if not provided\n",
    "                    result['box_scores'] = np.ones(len(image_boxes), dtype=np.float32)\n",
    "                \n",
    "                if return_full_samples:\n",
    "                    result['data_samples'] = data_samples\n",
    "            else:\n",
    "                # No keypoints detected\n",
    "                result = {\n",
    "                    'keypoints': np.empty((0, 21, 2), dtype=np.float32),\n",
    "                    'scores': np.empty((0, 21), dtype=np.float32),\n",
    "                    'boxes': image_boxes,\n",
    "                    'box_scores': scores[i] if scores is not None else np.ones(len(image_boxes), dtype=np.float32),\n",
    "                }\n",
    "                if return_full_samples:\n",
    "                    result['data_samples'] = data_samples\n",
    "        else:\n",
    "            # No instances detected\n",
    "            result = {\n",
    "                'keypoints': np.empty((0, 21, 2), dtype=np.float32),\n",
    "                'scores': np.empty((0, 21), dtype=np.float32),\n",
    "                'boxes': np.empty((0, 4), dtype=np.float32),\n",
    "                'box_scores': np.empty((0,), dtype=np.float32),\n",
    "            }\n",
    "            if return_full_samples:\n",
    "                result['data_samples'] = None\n",
    "        \n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Return single result for single image input\n",
    "    if is_single_image:\n",
    "        return all_results[0]\n",
    "    else:\n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc87ce",
   "metadata": {},
   "source": [
    "#### Visualize Hand Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def visualize_hand_landmarks(images: Union[np.ndarray, List[np.ndarray]], \n",
    "                           pose_results: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "                           keypoint_threshold: float = 0.3,\n",
    "                           bbox_thickness: int = 2,\n",
    "                           keypoint_radius: int = 4,\n",
    "                           skeleton_thickness: int = 2,\n",
    "                           show_bbox: bool = True,\n",
    "                           show_keypoints: bool = True,\n",
    "                           show_skeleton: bool = True,\n",
    "                           show_keypoint_labels: bool = False,\n",
    "                           show_confidence: bool = True,\n",
    "                           show_handedness: bool = True,\n",
    "                           backend: str = 'opencv') -> Union[np.ndarray, List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Visualize detected hand landmarks and bounding boxes on a single image or multiple images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input image(s) (numpy array or list of arrays in RGB format)\n",
    "        pose_results: Results from the pose() function\n",
    "                    For single image: Dictionary with 'keypoints', 'scores', 'boxes', 'box_scores'\n",
    "                    For multiple images: List of dictionaries, one for each image\n",
    "                    If None, no landmarks or boxes will be drawn\n",
    "        keypoint_threshold: Minimum confidence threshold for displaying keypoints\n",
    "        bbox_thickness: Thickness of bounding box lines\n",
    "        keypoint_radius: Radius of keypoint circles\n",
    "        skeleton_thickness: Thickness of skeleton lines\n",
    "        show_bbox: Whether to show bounding boxes\n",
    "        show_keypoints: Whether to show keypoints\n",
    "        show_skeleton: Whether to show skeleton connections\n",
    "        show_keypoint_labels: Whether to show keypoint labels\n",
    "        show_confidence: Whether to show confidence scores\n",
    "        show_handedness: Whether to show hand type (left/right prediction)\n",
    "        backend: Backend to use for visualization ('opencv' or 'matplotlib')\n",
    "        \n",
    "    Returns:\n",
    "        For single image: Visualized image with landmarks and bounding boxes\n",
    "        For multiple images: List of visualized images\n",
    "    \"\"\"\n",
    "    # Define hand keypoint connections for skeleton based on onehand10k dataset\n",
    "    skeleton_connections = [\n",
    "        # Thumb connections\n",
    "        (0, 1), (1, 2), (2, 3), (3, 4),  \n",
    "        # Index finger (forefinger) connections\n",
    "        (0, 5), (5, 6), (6, 7), (7, 8),  \n",
    "        # Middle finger connections\n",
    "        (0, 9), (9, 10), (10, 11), (11, 12),  \n",
    "        # Ring finger connections\n",
    "        (0, 13), (13, 14), (14, 15), (15, 16),  \n",
    "        # Pinky connections\n",
    "        (0, 17), (17, 18), (18, 19), (19, 20)  \n",
    "    ]\n",
    "    \n",
    "    # Keypoint names from onehand10k dataset\n",
    "    keypoint_names = [\n",
    "        \"wrist\", \n",
    "        \"thumb1\", \"thumb2\", \"thumb3\", \"thumb4\",\n",
    "        \"forefinger1\", \"forefinger2\", \"forefinger3\", \"forefinger4\",\n",
    "        \"middle_finger1\", \"middle_finger2\", \"middle_finger3\", \"middle_finger4\",\n",
    "        \"ring_finger1\", \"ring_finger2\", \"ring_finger3\", \"ring_finger4\",\n",
    "        \"pinky_finger1\", \"pinky_finger2\", \"pinky_finger3\", \"pinky_finger4\"\n",
    "    ]\n",
    "    \n",
    "    # Keypoint colors from onehand10k dataset (BGR format for OpenCV)\n",
    "    keypoint_colors = [\n",
    "        (255, 255, 255),       # wrist - white\n",
    "        (0, 128, 255),         # thumb1 - orange\n",
    "        (0, 128, 255),         # thumb2 - orange\n",
    "        (0, 128, 255),         # thumb3 - orange\n",
    "        (0, 128, 255),         # thumb4 - orange\n",
    "        (255, 153, 255),       # forefinger1 - pink\n",
    "        (255, 153, 255),       # forefinger2 - pink\n",
    "        (255, 153, 255),       # forefinger3 - pink\n",
    "        (255, 153, 255),       # forefinger4 - pink\n",
    "        (255, 178, 102),       # middle_finger1 - light blue\n",
    "        (255, 178, 102),       # middle_finger2 - light blue\n",
    "        (255, 178, 102),       # middle_finger3 - light blue\n",
    "        (255, 178, 102),       # middle_finger4 - light blue\n",
    "        (51, 51, 255),         # ring_finger1 - red\n",
    "        (51, 51, 255),         # ring_finger2 - red\n",
    "        (51, 51, 255),         # ring_finger3 - red\n",
    "        (51, 51, 255),         # ring_finger4 - red\n",
    "        (0, 255, 0),           # pinky_finger1 - green\n",
    "        (0, 255, 0),           # pinky_finger2 - green\n",
    "        (0, 255, 0),           # pinky_finger3 - green\n",
    "        (0, 255, 0)            # pinky_finger4 - green\n",
    "    ]\n",
    "    \n",
    "    # Skeleton colors from onehand10k dataset (BGR format for OpenCV)\n",
    "    skeleton_colors = [\n",
    "        # Thumb connections\n",
    "        (0, 128, 255), (0, 128, 255), (0, 128, 255), (0, 128, 255),\n",
    "        # Index (forefinger) connections\n",
    "        (255, 153, 255), (255, 153, 255), (255, 153, 255), (255, 153, 255),\n",
    "        # Middle finger connections\n",
    "        (255, 178, 102), (255, 178, 102), (255, 178, 102), (255, 178, 102),\n",
    "        # Ring finger connections\n",
    "        (51, 51, 255), (51, 51, 255), (51, 51, 255), (51, 51, 255),\n",
    "        # Pinky connections\n",
    "        (0, 255, 0), (0, 255, 0), (0, 255, 0), (0, 255, 0)\n",
    "    ]\n",
    "    \n",
    "    # Default bbox color in BGR\n",
    "    bbox_color = (0, 255, 0)  # Green\n",
    "    \n",
    "    # Check if input is a single image or a list of images\n",
    "    is_single_image = not isinstance(images, list)\n",
    "    \n",
    "    # If single image, convert to list format to unify processing\n",
    "    if is_single_image:\n",
    "        images_list = [images]\n",
    "        # If pose_results is provided and is a dict, convert to list\n",
    "        if pose_results is not None and isinstance(pose_results, dict):\n",
    "            pose_results_list = [pose_results]\n",
    "        else:\n",
    "            pose_results_list = None\n",
    "    else:\n",
    "        # For multiple images\n",
    "        images_list = images\n",
    "        # If pose_results is provided, use it\n",
    "        pose_results_list = pose_results if isinstance(pose_results, list) else None\n",
    "    \n",
    "    # Process each image\n",
    "    result_images = []\n",
    "    \n",
    "    for i, image in enumerate(images_list):\n",
    "        # Make a copy of the image to avoid modifying the original\n",
    "        vis_image = image.copy()\n",
    "        \n",
    "        # Convert RGB to BGR for OpenCV\n",
    "        if vis_image.shape[2] == 3 and backend == 'opencv':\n",
    "            vis_image = cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Skip processing if no pose results\n",
    "        if pose_results_list is None or i >= len(pose_results_list) or pose_results_list[i] is None:\n",
    "            # Just add the original image to the result and continue\n",
    "            if vis_image.shape[2] == 3 and backend == 'opencv':\n",
    "                vis_image = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)\n",
    "            result_images.append(vis_image)\n",
    "            continue\n",
    "        \n",
    "        # Get pose results for this image\n",
    "        result = pose_results_list[i]\n",
    "        \n",
    "        # Extract keypoints, bboxes and scores\n",
    "        keypoints_array = result.get('keypoints', None)\n",
    "        keypoint_scores = result.get('scores', None)\n",
    "        current_bboxes = result.get('boxes', None)\n",
    "        current_scores = result.get('box_scores', None)\n",
    "        \n",
    "        # Skip if no keypoints or bboxes\n",
    "        if (keypoints_array is None or len(keypoints_array) == 0) and (current_bboxes is None or len(current_bboxes) == 0):\n",
    "            if vis_image.shape[2] == 3 and backend == 'opencv':\n",
    "                vis_image = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)\n",
    "            result_images.append(vis_image)\n",
    "            continue\n",
    "        \n",
    "        # Draw bounding boxes if available and requested\n",
    "        if show_bbox and current_bboxes is not None and len(current_bboxes) > 0:\n",
    "            for idx, box in enumerate(current_bboxes):\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "                \n",
    "                # Draw the bounding box\n",
    "                cv2.rectangle(vis_image, (x1, y1), (x2, y2), bbox_color, bbox_thickness)\n",
    "                \n",
    "                # Add confidence score if requested\n",
    "                if show_confidence and current_scores is not None and idx < len(current_scores):\n",
    "                    score = current_scores[idx]\n",
    "                    conf_text = f\"Conf: {score:.2f}\"\n",
    "                    text_size = cv2.getTextSize(conf_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                    cv2.rectangle(vis_image, \n",
    "                                (x1, y1 - text_size[1] - 5), \n",
    "                                (x1 + text_size[0], y1), \n",
    "                                bbox_color, -1)\n",
    "                    cv2.putText(vis_image, conf_text, (x1, y1 - 5), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "        \n",
    "        # Draw keypoints and skeleton if available and requested\n",
    "        if (show_keypoints or show_skeleton) and keypoints_array is not None and len(keypoints_array) > 0:\n",
    "            for hand_idx, hand_keypoints in enumerate(keypoints_array):\n",
    "                # Skip hands with no valid keypoints\n",
    "                if len(hand_keypoints) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get scores for this hand\n",
    "                hand_scores = keypoint_scores[hand_idx] if keypoint_scores is not None else np.ones(hand_keypoints.shape[0])\n",
    "                \n",
    "                # Create list of valid keypoints\n",
    "                valid_keypoints = []\n",
    "                for j, (kp, score) in enumerate(zip(hand_keypoints, hand_scores)):\n",
    "                    if score >= keypoint_threshold:\n",
    "                        x, y = int(kp[0]), int(kp[1])\n",
    "                        valid_keypoints.append((j, x, y, score))\n",
    "                \n",
    "                # Draw skeleton if requested\n",
    "                if show_skeleton:\n",
    "                    # Create a dictionary for quick access to keypoint coordinates\n",
    "                    kp_dict = {idx: (x, y) for idx, x, y, _ in valid_keypoints}\n",
    "                    \n",
    "                    # Draw connections\n",
    "                    for j, connection in enumerate(skeleton_connections):\n",
    "                        idx1, idx2 = connection\n",
    "                        if idx1 in kp_dict and idx2 in kp_dict:\n",
    "                            pt1 = kp_dict[idx1]\n",
    "                            pt2 = kp_dict[idx2]\n",
    "                            # Use the color specific to this connection\n",
    "                            color = skeleton_colors[j] if j < len(skeleton_colors) else (255, 255, 255)\n",
    "                            cv2.line(vis_image, pt1, pt2, color, skeleton_thickness)\n",
    "                \n",
    "                # Draw keypoints if requested\n",
    "                if show_keypoints:\n",
    "                    for idx, x, y, conf in valid_keypoints:\n",
    "                        # Use the color specific to this keypoint\n",
    "                        color = keypoint_colors[idx] if idx < len(keypoint_colors) else (255, 255, 255)\n",
    "                        \n",
    "                        # Draw keypoint circle with filled center\n",
    "                        cv2.circle(vis_image, (x, y), keypoint_radius, color, -1)\n",
    "                        \n",
    "                        # Add a small border to make keypoints more visible\n",
    "                        cv2.circle(vis_image, (x, y), keypoint_radius, (0, 0, 0), 1)\n",
    "                        \n",
    "                        # Add keypoint labels if requested\n",
    "                        if show_keypoint_labels and idx < len(keypoint_names):\n",
    "                            label = f\"{keypoint_names[idx]}\"\n",
    "                            if show_confidence:\n",
    "                                label += f\" ({conf:.2f})\"\n",
    "                            \n",
    "                            # Place text near the keypoint\n",
    "                            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)[0]\n",
    "                            \n",
    "                            # Create a dark background for text\n",
    "                            cv2.rectangle(vis_image, \n",
    "                                        (x + 5, y - text_size[1] - 5), \n",
    "                                        (x + 5 + text_size[0], y), \n",
    "                                        (0, 0, 0), -1)\n",
    "                            \n",
    "                            # Add text label\n",
    "                            cv2.putText(vis_image, label, (x + 5, y - 5), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                \n",
    "                # Determine handedness if requested\n",
    "                if show_handedness and len(valid_keypoints) > 0:\n",
    "                    # Simplified handedness estimation using keypoint positions\n",
    "                    wrist_idx = 0\n",
    "                    thumb_tip_idx = 4\n",
    "                    index_tip_idx = 8\n",
    "                    \n",
    "                    # Create a dictionary of keypoint positions\n",
    "                    kp_positions = {idx: (x, y) for idx, x, y, _ in valid_keypoints}\n",
    "                    \n",
    "                    hand_type = \"Unknown\"\n",
    "                    \n",
    "                    # Check if we have the necessary keypoints\n",
    "                    if wrist_idx in kp_positions and thumb_tip_idx in kp_positions and index_tip_idx in kp_positions:\n",
    "                        wrist = kp_positions[wrist_idx]\n",
    "                        thumb_tip = kp_positions[thumb_tip_idx]\n",
    "                        index_tip = kp_positions[index_tip_idx]\n",
    "                        \n",
    "                        # Calculate vectors from wrist to fingertips\n",
    "                        wrist_to_thumb = (thumb_tip[0] - wrist[0], thumb_tip[1] - wrist[1])\n",
    "                        wrist_to_index = (index_tip[0] - wrist[0], index_tip[1] - wrist[1])\n",
    "                        \n",
    "                        # Cross product to determine if thumb is to the left or right of index finger\n",
    "                        cross_product = wrist_to_thumb[0] * wrist_to_index[1] - wrist_to_thumb[1] * wrist_to_index[0]\n",
    "                        \n",
    "                        # If cross product is positive, thumb is to the right of index finger (likely left hand)\n",
    "                        # If negative, thumb is to the left of index finger (likely right hand)\n",
    "                        if cross_product > 0:\n",
    "                            hand_type = \"Left\"\n",
    "                        else:\n",
    "                            hand_type = \"Right\"\n",
    "                    \n",
    "                    # Add hand type text\n",
    "                    hand_text = f\"{hand_type} Hand\"\n",
    "                    \n",
    "                    if current_bboxes is not None and hand_idx < len(current_bboxes):\n",
    "                        x1, y1, x2, y2 = current_bboxes[hand_idx].astype(int)\n",
    "                        \n",
    "                        text_size = cv2.getTextSize(hand_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                        \n",
    "                        # Place text above the bounding box\n",
    "                        text_y = y1 - text_size[1] - 15 if show_confidence else y1 - text_size[1] - 5\n",
    "                        text_x = x1\n",
    "                        \n",
    "                        cv2.rectangle(vis_image, \n",
    "                                    (text_x, text_y - text_size[1]), \n",
    "                                    (text_x + text_size[0], text_y + 5), \n",
    "                                    (0, 0, 255), -1)  # Blue background\n",
    "                        cv2.putText(vis_image, hand_text, (text_x, text_y), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    else:\n",
    "                        # If no bounding box, place text at the wrist position\n",
    "                        if wrist_idx in kp_positions:\n",
    "                            wrist_x, wrist_y = kp_positions[wrist_idx]\n",
    "                            text_size = cv2.getTextSize(hand_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "                            cv2.rectangle(vis_image, \n",
    "                                        (wrist_x, wrist_y - text_size[1] - 15), \n",
    "                                        (wrist_x + text_size[0], wrist_y - 10), \n",
    "                                        (0, 0, 255), -1)  # Blue background\n",
    "                            cv2.putText(vis_image, hand_text, (wrist_x, wrist_y - 15), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        # Convert back to RGB if needed\n",
    "        if vis_image.shape[2] == 3 and backend == 'opencv':\n",
    "            vis_image = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Add the processed image to the result list\n",
    "        result_images.append(vis_image)\n",
    "    \n",
    "    # Return a single image if input was a single image, otherwise return the list\n",
    "    if is_single_image:\n",
    "        return result_images[0]\n",
    "    else:\n",
    "        return result_images\n",
    "\n",
    "recording = processed_data.get(\"recording_0\")\n",
    "rgb_images = recording[\"color_images\"]\n",
    "\n",
    "cropped_frames, _ = crop_hands_sequence(\n",
    "    rgb_images,\n",
    "    output_size=(300, 300),\n",
    "    margin_percent=0.5,\n",
    ")\n",
    "\n",
    "boxes_list = []\n",
    "scores_list = []\n",
    "crop_info_list = []\n",
    "\n",
    "for frame in cropped_frames:\n",
    "    boxes, scores, crop_info = detect_hands(\n",
    "        frame,\n",
    "        return_coordinates='original',\n",
    "        crop_size=(300, 300),\n",
    "    )\n",
    "    boxes_list.append(boxes)\n",
    "    scores_list.append(scores)\n",
    "    crop_info_list.append(crop_info)\n",
    "\n",
    "pose_results = pose(\n",
    "    images=cropped_frames,\n",
    "    boxes=boxes_list,\n",
    "    scores=scores_list\n",
    ")\n",
    "\n",
    "visualized_frames = visualize_hand_landmarks(\n",
    "    images=cropped_frames,\n",
    "    pose_results=pose_results,\n",
    "    keypoint_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd3830",
   "metadata": {},
   "source": [
    "#### Create Keypoint Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9afe9ce",
   "metadata": {},
   "source": [
    "##### **Class Defenition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a30a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "class HandPoseLSTMDatasetCreator:\n",
    "    \"\"\"\n",
    "    Create LSTM dataset from windowed hand video data\n",
    "    \n",
    "    Class mapping:\n",
    "    - OH: Open Hand (0)\n",
    "    - IH: Intrinsic Plus (1) \n",
    "    - SF: Straight Fist (2)\n",
    "    - HH: Hook Hand (3)\n",
    "    - HC: Hand Close (4)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root: str, window_size: int = 16, num_keypoints: int = 21,\n",
    "                 flatten_keypoints: bool = False):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.window_size = window_size\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.flatten_keypoints = flatten_keypoints\n",
    "        \n",
    "        # Class mapping\n",
    "        self.class_mapping = {\n",
    "            'OH': 0,  # Open Hand\n",
    "            'IH': 1,  # Intrinsic Plus\n",
    "            'SF': 2,  # Straight Fist\n",
    "            'HH': 3,  # Hook Hand\n",
    "            'HC': 4   # Hand Close\n",
    "        }\n",
    "        \n",
    "        self.class_names = {v: k for k, v in self.class_mapping.items()}\n",
    "        \n",
    "        # Data containers\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        self.metadata = []\n",
    "        self.failed_samples = []\n",
    "        \n",
    "    def load_window_data(self, file_path: str) -> np.ndarray:\n",
    "        \"\"\"Load windowed data from .npy file\"\"\"\n",
    "        try:\n",
    "            data = np.load(file_path)\n",
    "            print(f\"Loaded {file_path}: shape {data.shape}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_single_window(self, rgb_frames: np.ndarray, class_name: str, \n",
    "                            file_name: str) -> Tuple[Optional[np.ndarray], Dict]:\n",
    "        \"\"\"\n",
    "        Process a single window of 16 frames\n",
    "        \n",
    "        Args:\n",
    "            rgb_frames: numpy array of shape (16, H, W, 3)\n",
    "            class_name: class abbreviation (OH, IH, SF, HH, HC)\n",
    "            file_name: original file name for metadata\n",
    "            \n",
    "        Returns:\n",
    "            pose_sequence: numpy array of shape (16, 21, 2) or None if failed\n",
    "            metadata: dictionary with processing information\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            'file_name': file_name,\n",
    "            'class_name': class_name,\n",
    "            'class_id': self.class_mapping[class_name],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'num_frames': len(rgb_frames),\n",
    "            'processing_status': 'started'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Detect hands for all frames\n",
    "            boxes_list = []\n",
    "            scores_list = []\n",
    "            crop_info_list = []\n",
    "            \n",
    "            for i, frame in enumerate(rgb_frames):\n",
    "                boxes, scores, crop_info = detect_hands(\n",
    "                    frame,\n",
    "                    return_coordinates='original',\n",
    "                    crop_size=(300, 300),\n",
    "                )\n",
    "                boxes_list.append(boxes)\n",
    "                scores_list.append(scores)\n",
    "                crop_info_list.append(crop_info)\n",
    "            \n",
    "            # Convert numpy array to list for pose function\n",
    "            rgb_frames_list = [frame for frame in rgb_frames]\n",
    "            \n",
    "            # Get pose results for all frames\n",
    "            pose_results = pose(\n",
    "                images=rgb_frames_list,\n",
    "                boxes=boxes_list,\n",
    "                scores=scores_list\n",
    "            )\n",
    "            \n",
    "            # Extract pose coordinates\n",
    "            pose_sequence = []\n",
    "            valid_frames = 0\n",
    "            \n",
    "            for frame_idx, result in enumerate(pose_results):\n",
    "                # Initialize with zeros\n",
    "                frame_keypoints = np.zeros((self.num_keypoints, 2))\n",
    "                \n",
    "                if result is not None:\n",
    "                    # Handle the specific dictionary format from pose()\n",
    "                    if isinstance(result, dict) and 'keypoints' in result:\n",
    "                        keypoints = result['keypoints']\n",
    "                        if isinstance(keypoints, np.ndarray) and keypoints.size > 0:\n",
    "                            if keypoints.shape == (1, self.num_keypoints, 2):\n",
    "                                frame_keypoints = keypoints[0].copy()\n",
    "                                valid_frames += 1\n",
    "                            elif keypoints.shape == (self.num_keypoints, 2):\n",
    "                                frame_keypoints = keypoints.copy()\n",
    "                                valid_frames += 1\n",
    "                            else:\n",
    "                                print(f\"Frame {frame_idx}: Unexpected keypoints shape: {keypoints.shape}\")\n",
    "                    elif isinstance(result, list) and len(result) > 0:\n",
    "                        # If result is a list of detections\n",
    "                        first_detection = result[0]\n",
    "                        if isinstance(first_detection, dict) and 'keypoints' in first_detection:\n",
    "                            keypoints = first_detection['keypoints']\n",
    "                            if isinstance(keypoints, np.ndarray) and keypoints.size > 0:\n",
    "                                if keypoints.shape == (1, self.num_keypoints, 2):\n",
    "                                    frame_keypoints = keypoints[0].copy()\n",
    "                                    valid_frames += 1\n",
    "                                elif keypoints.shape == (self.num_keypoints, 2):\n",
    "                                    frame_keypoints = keypoints.copy()\n",
    "                                    valid_frames += 1\n",
    "                \n",
    "                pose_sequence.append(frame_keypoints)\n",
    "            \n",
    "            pose_sequence = np.array(pose_sequence)\n",
    "            \n",
    "            # Update metadata\n",
    "            metadata['valid_frames'] = valid_frames\n",
    "            metadata['detection_rate'] = valid_frames / len(rgb_frames)\n",
    "            metadata['boxes_detected'] = [len(b) if b is not None else 0 for b in boxes_list]\n",
    "            \n",
    "            # Check if we have enough valid detections\n",
    "            min_required_frames = int(self.window_size * 0.3)  # Lowered to 30% for debugging\n",
    "            if valid_frames < min_required_frames:\n",
    "                metadata['processing_status'] = 'insufficient_detections'\n",
    "                print(f\"Insufficient detections for {file_name}: {valid_frames}/{self.window_size} frames (need at least {min_required_frames})\")\n",
    "                return None, metadata\n",
    "            \n",
    "            metadata['processing_status'] = 'success'\n",
    "            return pose_sequence, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            metadata['processing_status'] = 'failed'\n",
    "            metadata['error'] = str(e)\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, metadata\n",
    "    \n",
    "    def normalize_sequence(self, pose_sequence: np.ndarray, method: str = 'wrist') -> np.ndarray:\n",
    "        \"\"\"Normalize pose sequence\"\"\"\n",
    "        normalized = pose_sequence.copy()\n",
    "        \n",
    "        if method == 'wrist':\n",
    "            # Normalize relative to wrist position\n",
    "            for i in range(len(pose_sequence)):\n",
    "                # Check if frame has valid detections (not all zeros)\n",
    "                if np.any(pose_sequence[i] != 0):\n",
    "                    wrist_pos = pose_sequence[i, 0, :]\n",
    "                    normalized[i] = pose_sequence[i] - wrist_pos\n",
    "                    \n",
    "        elif method == 'bbox':\n",
    "            # Normalize to bounding box\n",
    "            for i in range(len(pose_sequence)):\n",
    "                # Check if frame has valid detections\n",
    "                if np.any(pose_sequence[i] != 0):\n",
    "                    # Get non-zero points only\n",
    "                    valid_mask = ~np.all(pose_sequence[i] == 0, axis=1)\n",
    "                    if np.any(valid_mask):\n",
    "                        valid_points = pose_sequence[i][valid_mask]\n",
    "                        min_vals = valid_points.min(axis=0)\n",
    "                        max_vals = valid_points.max(axis=0)\n",
    "                        range_vals = max_vals - min_vals\n",
    "                        range_vals[range_vals == 0] = 1  # Avoid division by zero\n",
    "                        \n",
    "                        # Normalize only valid points\n",
    "                        for j in range(len(pose_sequence[i])):\n",
    "                            if valid_mask[j]:\n",
    "                                normalized[i, j] = (pose_sequence[i, j] - min_vals) / range_vals\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def process_dataset(self, normalize: str = 'wrist', skip_failed: bool = True):\n",
    "        \"\"\"Process entire dataset\"\"\"\n",
    "        rgb_root = self.data_root / 'rgb'\n",
    "        \n",
    "        if not rgb_root.exists():\n",
    "            raise ValueError(f\"RGB data directory not found: {rgb_root}\")\n",
    "        \n",
    "        # Process each class\n",
    "        for class_name in self.class_mapping.keys():\n",
    "            class_dir = rgb_root / class_name\n",
    "            \n",
    "            if not class_dir.exists():\n",
    "                print(f\"Warning: Class directory not found: {class_dir}\")\n",
    "                continue\n",
    "            \n",
    "            # Get all .npy files in class directory\n",
    "            npy_files = list(class_dir.glob(\"*.npy\"))\n",
    "            print(f\"\\nProcessing class {class_name}: {len(npy_files)} files\")\n",
    "            \n",
    "            for npy_file in tqdm(npy_files, desc=f\"Processing {class_name}\"):\n",
    "                # Load RGB frames\n",
    "                rgb_frames = self.load_window_data(str(npy_file))\n",
    "                \n",
    "                if rgb_frames is None:\n",
    "                    continue\n",
    "                \n",
    "                # Process window\n",
    "                pose_sequence, metadata = self.process_single_window(\n",
    "                    rgb_frames, \n",
    "                    class_name, \n",
    "                    npy_file.name\n",
    "                )\n",
    "                \n",
    "                if pose_sequence is not None:\n",
    "                    # Normalize if requested\n",
    "                    if normalize:\n",
    "                        pose_sequence = self.normalize_sequence(pose_sequence, normalize)\n",
    "                    \n",
    "                    # Flatten keypoints if requested  # ADD THIS BLOCK\n",
    "                    if self.flatten_keypoints:\n",
    "                        # Reshape from (16, 21, 2) to (16, 42)\n",
    "                        pose_sequence = pose_sequence.reshape(self.window_size, -1)\n",
    "                    \n",
    "                    # Add to dataset\n",
    "                    self.sequences.append(pose_sequence)\n",
    "                    self.labels.append(self.class_mapping[class_name])\n",
    "                    self.metadata.append(metadata)\n",
    "                else:\n",
    "                    self.failed_samples.append(metadata)\n",
    "        \n",
    "        print(f\"\\nProcessing complete:\")\n",
    "        print(f\"Total samples: {len(self.sequences)}\")\n",
    "        print(f\"Failed samples: {len(self.failed_samples)}\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        if self.labels:\n",
    "            unique, counts = np.unique(self.labels, return_counts=True)\n",
    "            print(\"\\nClass distribution:\")\n",
    "            for cls, count in zip(unique, counts):\n",
    "                print(f\"  {self.class_names[cls]}: {count} samples\")\n",
    "    \n",
    "    def prepare_lstm_format(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Convert to LSTM input format\"\"\"\n",
    "        if not self.sequences:\n",
    "            raise ValueError(\"No sequences to prepare. Run process_dataset first.\")\n",
    "        \n",
    "        # Stack sequences\n",
    "        if self.flatten_keypoints:\n",
    "            # Already flattened during processing\n",
    "            X = np.stack(self.sequences)  # (num_samples, 16, 42)\n",
    "        else:\n",
    "            # Need to flatten now\n",
    "            X_3d = np.stack(self.sequences)  # (num_samples, 16, 21, 2)\n",
    "            num_samples = X_3d.shape[0]\n",
    "            X = X_3d.reshape(num_samples, self.window_size, -1)  # (num_samples, 16, 42)\n",
    "        \n",
    "        y = np.array(self.labels)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def save_by_folder_structure(self, output_base_path: str, save_individual_files: bool = True):\n",
    "        \"\"\"\n",
    "        Save processed landmarks maintaining the original folder structure\n",
    "        \n",
    "        Args:\n",
    "            output_base_path: Base path for saving (hand_landmark folder will be created here)\n",
    "            save_individual_files: If True, saves each sample as individual .npy file\n",
    "                                 If False, saves all samples per class in one file\n",
    "        \"\"\"\n",
    "        # Create hand_landmark directory structure\n",
    "        output_base = Path(output_base_path).parent if Path(output_base_path).suffix else Path(output_base_path)\n",
    "        landmark_root = output_base\n",
    "        \n",
    "        # Organize sequences by class\n",
    "        class_sequences = {class_name: [] for class_name in self.class_mapping.keys()}\n",
    "        class_metadata = {class_name: [] for class_name in self.class_mapping.keys()}\n",
    "        \n",
    "        # Group sequences by class\n",
    "        for seq, label, meta in zip(self.sequences, self.labels, self.metadata):\n",
    "            class_name = self.class_names[label]\n",
    "            class_sequences[class_name].append(seq)\n",
    "            class_metadata[class_name].append(meta)\n",
    "        \n",
    "        # Save by class\n",
    "        for class_name, sequences in class_sequences.items():\n",
    "            if not sequences:\n",
    "                continue\n",
    "                \n",
    "            class_dir = landmark_root / class_name\n",
    "            class_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if save_individual_files:\n",
    "                # Save each sample individually with original filename\n",
    "                for seq, meta in zip(sequences, class_metadata[class_name]):\n",
    "                    original_name = meta['file_name']\n",
    "                    # Change extension from .npy to _landmarks.npy\n",
    "                    landmark_name = original_name.replace('.npy', '_landmarks.npy')\n",
    "                    save_path = class_dir / landmark_name\n",
    "                    \n",
    "                    # Save landmark data\n",
    "                    np.save(save_path, seq)\n",
    "                    \n",
    "                    # Save metadata as json\n",
    "                    meta_path = save_path.with_suffix('.json')\n",
    "                    with open(meta_path, 'w') as f:\n",
    "                        json.dump(meta, f, indent=2)\n",
    "                        \n",
    "                print(f\"Saved {len(sequences)} individual files for class {class_name}\")\n",
    "            else:\n",
    "                # Save all samples of this class in one file\n",
    "                class_data = {\n",
    "                    'sequences': np.array(sequences),  # (n_samples, 16, 21, 2)\n",
    "                    'metadata': class_metadata[class_name],\n",
    "                    'class_name': class_name,\n",
    "                    'class_id': self.class_mapping[class_name]\n",
    "                }\n",
    "                \n",
    "                save_path = class_dir / f'{class_name}_all_landmarks.npz'\n",
    "                np.savez_compressed(save_path, **class_data)\n",
    "                print(f\"Saved {len(sequences)} samples for class {class_name} in {save_path}\")\n",
    "        \n",
    "        # Save summary file\n",
    "        summary = {\n",
    "            'total_samples': len(self.sequences),\n",
    "            'failed_samples': len(self.failed_samples),\n",
    "            'class_distribution': {cls: len(seqs) for cls, seqs in class_sequences.items()},\n",
    "            'window_size': self.window_size,\n",
    "            'num_keypoints': self.num_keypoints,\n",
    "            'processing_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        summary_path = landmark_root / 'dataset_summary.json'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nDataset saved to: {landmark_root}\")\n",
    "        print(f\"Total processed samples: {len(self.sequences)}\")\n",
    "        \n",
    "    def save_dataset(self, output_path: str, save_format: str = 'npz', \n",
    "                 maintain_structure: bool = False, save_individual: bool = True):\n",
    "        \"\"\"\n",
    "        Save processed dataset\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path for saving the dataset\n",
    "            save_format: 'npz' or 'separate' (only used when maintain_structure=False)\n",
    "            maintain_structure: If True, saves in folder structure; if False, saves as single file\n",
    "            save_individual: If True and maintain_structure=True, saves each sample separately\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if we have any data to save\n",
    "        if not self.sequences:\n",
    "            print(\"\\n!!! WARNING: No sequences were successfully processed !!!\")\n",
    "            print(f\"Total failed samples: {len(self.failed_samples)}\")\n",
    "            if self.failed_samples:\n",
    "                print(\"\\nDetailed failure analysis:\")\n",
    "                failure_reasons = {}\n",
    "                for failed in self.failed_samples:\n",
    "                    reason = failed.get('processing_status', 'unknown')\n",
    "                    failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\n",
    "                \n",
    "                for reason, count in failure_reasons.items():\n",
    "                    print(f\"  - {reason}: {count} samples\")\n",
    "                \n",
    "                print(\"\\nFirst 5 failed samples:\")\n",
    "                for failed in self.failed_samples[:5]:\n",
    "                    print(f\"\\n  File: {failed['file_name']}\")\n",
    "                    print(f\"  Status: {failed['processing_status']}\")\n",
    "                    if 'error' in failed:\n",
    "                        print(f\"  Error: {failed['error']}\")\n",
    "                    if 'detection_rate' in failed:\n",
    "                        print(f\"  Detection rate: {failed['detection_rate']:.2%}\")\n",
    "                    if 'valid_frames' in failed:\n",
    "                        print(f\"  Valid frames: {failed['valid_frames']}/{self.window_size}\")\n",
    "            \n",
    "            print(\"\\nNo dataset file was created due to lack of valid samples.\")\n",
    "            return\n",
    "        \n",
    "        # If maintain_structure is True, use folder structure saving\n",
    "        if maintain_structure:\n",
    "            self.save_by_folder_structure(output_base_path=output_path, save_individual_files=save_individual)\n",
    "            return\n",
    "        \n",
    "        # Otherwise, use the original saving method (single file)\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Prepare data\n",
    "        X, y = self.prepare_lstm_format()\n",
    "        \n",
    "        if save_format == 'npz':\n",
    "            # Save as compressed numpy archive\n",
    "            save_dict = {\n",
    "                'X': X,  # LSTM format\n",
    "                'y': y,  # Labels\n",
    "                'sequences_3d': np.stack(self.sequences),  # Original 3D format\n",
    "                'class_mapping': self.class_mapping,\n",
    "                'class_names': self.class_names,\n",
    "                'metadata': self.metadata,\n",
    "                'failed_samples': self.failed_samples,\n",
    "                'window_size': self.window_size,\n",
    "                'num_keypoints': self.num_keypoints,\n",
    "            }\n",
    "            \n",
    "            np.savez_compressed(str(output_path), **save_dict)\n",
    "            print(f\"\\nDataset saved to: {output_path}\")\n",
    "            \n",
    "        elif save_format == 'separate':\n",
    "            # Save as separate files\n",
    "            base_path = output_path.with_suffix('')\n",
    "            np.save(f\"{base_path}_X.npy\", X)\n",
    "            np.save(f\"{base_path}_y.npy\", y)\n",
    "            np.save(f\"{base_path}_sequences_3d.npy\", np.stack(self.sequences))\n",
    "            \n",
    "            # Save metadata as JSON\n",
    "            meta_dict = {\n",
    "                'class_mapping': self.class_mapping,\n",
    "                'class_names': self.class_names,\n",
    "                'window_size': self.window_size,\n",
    "                'num_keypoints': self.num_keypoints,\n",
    "                'total_samples': len(X),\n",
    "                'failed_samples': len(self.failed_samples),\n",
    "                'metadata': self.metadata,\n",
    "                'failed_samples_info': self.failed_samples\n",
    "            }\n",
    "            \n",
    "            with open(f\"{base_path}_metadata.json\", 'w') as f:\n",
    "                json.dump(meta_dict, f, indent=2)\n",
    "            \n",
    "            print(f\"\\nDataset saved to: {base_path}_*.npy/json\")\n",
    "        \n",
    "        print(f\"Dataset shape - X: {X.shape}, y: {y.shape}\")\n",
    "        \n",
    "    def split_dataset(self, train_ratio: float = 0.8, random_seed: int = 42):\n",
    "        \"\"\"Split dataset into train and validation sets\"\"\"\n",
    "        X, y = self.prepare_lstm_format()\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        n_samples = len(y)\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        \n",
    "        train_size = int(n_samples * train_ratio)\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "        \n",
    "        return {\n",
    "            'X_train': X[train_indices],\n",
    "            'X_val': X[val_indices],\n",
    "            'y_train': y[train_indices],\n",
    "            'y_val': y[val_indices],\n",
    "            'train_indices': train_indices,\n",
    "            'val_indices': val_indices\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da382874",
   "metadata": {},
   "source": [
    "##### **Execute Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c13bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset processing...\n",
      "\n",
      "Processing class OH: 12 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250402151124_w000.npy: shape (16, 300, 300, 3)\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-hand5_pt-aic-coco_210e-256x256-74fb594_20230320.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:   8%|â–Š         | 1/12 [00:08<01:34,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250402151124_w010.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  17%|â–ˆâ–‹        | 2/12 [00:11<00:50,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250402151609_w000.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:13<00:34,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250402151609_w009.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:25,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250402152331_w000.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:19,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250402152331_w001.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:20<00:15,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250506145704_w000.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:24<00:16,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250506145704_w001.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:29<00:15,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250506152951_w000.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:34<00:12,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250506152951_w001.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:39<00:08,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250506162630_w000.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:43<00:04,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\OH\\Record_20250506162630_w001.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OH: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:49<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class IH: 12 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250402151124_w001.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:   8%|â–Š         | 1/12 [00:03<00:35,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250402151124_w002.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  17%|â–ˆâ–‹        | 2/12 [00:06<00:31,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250402151609_w001.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:09<00:27,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250402151609_w002.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:12<00:24,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250402152331_w003.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:15<00:20,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250402152331_w004.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:18<00:17,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250506145704_w011.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:21<00:15,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250506145704_w012.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:24<00:12,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250506152951_w005.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:27<00:09,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250506152951_w006.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:31<00:06,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250506162630_w006.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:34<00:03,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\IH\\Record_20250506162630_w007.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IH: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:36<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class SF: 12 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250402151124_w003.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:   8%|â–Š         | 1/12 [00:03<00:33,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250402151124_w004.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  17%|â–ˆâ–‹        | 2/12 [00:06<00:33,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250402151609_w003.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:31,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250402151609_w004.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:13<00:27,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250402152331_w005.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:24,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250402152331_w006.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:20<00:20,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250506145704_w016.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:23<00:16,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250506145704_w017.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:26<00:12,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250506145704_w018.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:29<00:09,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250506152951_w010.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:32<00:06,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250506152951_w011.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:34<00:02,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\SF\\Record_20250506162630_w008.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:37<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class HH: 12 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250402151124_w007.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:   8%|â–Š         | 1/12 [00:02<00:26,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250402151124_w008.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  17%|â–ˆâ–‹        | 2/12 [00:05<00:25,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250402151609_w006.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:07<00:22,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250402151609_w007.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:10<00:20,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250402152331_w008.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:12<00:17,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250402152331_w009.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:14<00:14,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250506145704_w027.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:16<00:11,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250506145704_w028.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:19<00:09,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250506152951_w018.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:21<00:06,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250506152951_w019.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:23<00:04,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250506162630_w013.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:25<00:02,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HH\\Record_20250506162630_w014.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HH: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:28<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class HC: 12 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250402151124_w005.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:   8%|â–Š         | 1/12 [00:02<00:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250402151124_w006.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  17%|â–ˆâ–‹        | 2/12 [00:04<00:23,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250402151609_w005.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:07<00:22,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250402152331_w007.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:09<00:20,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506145704_w022.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:12<00:17,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506145704_w023.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:14<00:14,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506145704_w024.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:17<00:12,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506145704_w025.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:19<00:09,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506152951_w014.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:21<00:06,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506152951_w015.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:23<00:04,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506162630_w009.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:25<00:02,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\video_hand_focused_data\\rgb\\HC\\Record_20250506162630_w010.npy: shape (16, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HC: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:27<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Total samples: 60\n",
      "Failed samples: 0\n",
      "\n",
      "Class distribution:\n",
      "  OH: 12 samples\n",
      "  IH: 12 samples\n",
      "  SF: 12 samples\n",
      "  HH: 12 samples\n",
      "  HC: 12 samples\n",
      "Saved 12 individual files for class OH\n",
      "Saved 12 individual files for class IH\n",
      "Saved 12 individual files for class SF\n",
      "Saved 12 individual files for class HH\n",
      "Saved 12 individual files for class HC\n",
      "\n",
      "Dataset saved to: data\\video_hand_focused_data\\hand_landmark_flatten\n",
      "Total processed samples: 60\n"
     ]
    }
   ],
   "source": [
    "data_root = r\"data/video_hand_focused_data\"\n",
    "output_path = r\"data/video_hand_focused_data/hand_landmark_flatten\"\n",
    "\n",
    "# Create dataset processor\n",
    "dataset_creator = HandPoseLSTMDatasetCreator(\n",
    "    data_root=data_root,\n",
    "    window_size=16,\n",
    "    num_keypoints=21, \n",
    "    flatten_keypoints=True\n",
    ")\n",
    "\n",
    "# Process all data\n",
    "print(\"Starting dataset processing...\")\n",
    "dataset_creator.process_dataset(normalize='wrist', skip_failed=True)\n",
    "\n",
    "# Save with folder structure - individual files for each sample\n",
    "dataset_creator.save_dataset(\n",
    "    output_path=output_path,  \n",
    "    maintain_structure=True,\n",
    "    save_individual=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae50f0a",
   "metadata": {},
   "source": [
    "## 3. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2eaec6",
   "metadata": {},
   "source": [
    "### 3.1. Create Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62d72e",
   "metadata": {},
   "source": [
    "#### Using Non-Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "\n",
    "# 1. Create lists of file paths and labels\n",
    "image_rgb_dir = r'./data/video/rgb'\n",
    "filepaths, labels = [], []\n",
    "for class_name in os.listdir(image_rgb_dir):\n",
    "    class_dir = os.path.join(image_rgb_dir, class_name)\n",
    "    for npy_path in glob.glob(os.path.join(class_dir, '*.npy')):\n",
    "        filepaths.append(npy_path)\n",
    "        labels.append(class_name)\n",
    "\n",
    "# 2. Create dataset with file paths\n",
    "unique_labels = sorted(set(labels))\n",
    "pr_ds = Dataset.from_dict({\n",
    "    \"image_path\": filepaths,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# 3. Encode the label column\n",
    "pr_ds = pr_ds.class_encode_column(\"label\")\n",
    "\n",
    "# 4. Create a function to load images when needed\n",
    "def load_image(example):\n",
    "    example[\"image\"] = np.load(example[\"image_path\"])\n",
    "    return example\n",
    "\n",
    "# 5. Split the dataset\n",
    "split1 = pr_ds.train_test_split(test_size=0.25, shuffle=True, seed=42, stratify_by_column=\"label\")\n",
    "train_ds = split1[\"train\"]\n",
    "val_ds = split1[\"test\"]\n",
    "\n",
    "# 6. Build label2id / id2label\n",
    "label2id = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "id2label = {idx: lab for lab, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0f9e2",
   "metadata": {},
   "source": [
    "#### Using Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e35d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "\n",
    "# 1) Base directories\n",
    "base_dir     = os.path.join(\n",
    "    r\"D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\DATA\\video_hand_focused_data\",\n",
    "    \"split_rgb\"\n",
    ")\n",
    "train_dirs   = [os.path.join(base_dir, \"train_aug\")]\n",
    "val_dir      = os.path.join(base_dir, \"val\")\n",
    "\n",
    "# 2) Collect train file paths & labels\n",
    "train_paths, train_labels = [], []\n",
    "for d in train_dirs:\n",
    "    for class_name in os.listdir(d):\n",
    "        class_dir = os.path.join(d, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for npy_path in glob.glob(os.path.join(class_dir, \"*.npy\")):\n",
    "            train_paths.append(npy_path)\n",
    "            train_labels.append(class_name)\n",
    "\n",
    "# 3) Collect val file paths & labels\n",
    "val_paths, val_labels = [], []\n",
    "for class_name in os.listdir(val_dir):\n",
    "    class_dir = os.path.join(val_dir, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    for npy_path in glob.glob(os.path.join(class_dir, \"*.npy\")):\n",
    "        val_paths.append(npy_path)\n",
    "        val_labels.append(class_name)\n",
    "\n",
    "# 4) Define the ordered class list instead of using sorted set\n",
    "ordered_labels = [\"OH\", \"IH\", \"SF\", \"HC\", \"HH\"]\n",
    "\n",
    "# 5) Create custom class label feature with specified order\n",
    "class_feature = ClassLabel(names=ordered_labels)\n",
    "\n",
    "# 6) Create train dataset with custom class feature\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"image_path\": train_paths,\n",
    "    \"label\": train_labels\n",
    "}, features=Features({\n",
    "    \"image_path\": Value(\"string\"),\n",
    "    \"label\": class_feature\n",
    "}))\n",
    "\n",
    "# 7) Create validation dataset with custom class feature\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"image_path\": val_paths,\n",
    "    \"label\": val_labels\n",
    "}, features=Features({\n",
    "    \"image_path\": Value(\"string\"),\n",
    "    \"label\": class_feature\n",
    "}))\n",
    "\n",
    "# 8) Create a function to load images when needed\n",
    "def load_image(example):\n",
    "    example[\"image\"] = np.load(example[\"image_path\"])\n",
    "    return example\n",
    "\n",
    "# 9) Build label2id / id2label with the specified order\n",
    "label2id = {label: idx for idx, label in enumerate(ordered_labels)}\n",
    "id2label = {idx: label for idx, label in enumerate(ordered_labels)}\n",
    "\n",
    "# 10) Verify the label mappings\n",
    "print(f\"label2id: {label2id}\")\n",
    "print(f\"id2label: {id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f7986",
   "metadata": {},
   "source": [
    "#### Ensure Correct Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Function to visualize sample images for each class\n",
    "def visualize_class_samples(dataset, id2label):\n",
    "    plt.figure(figsize=(18, 10))  # Wider figure to accommodate all samples\n",
    "    \n",
    "    for class_id in range(len(id2label)):\n",
    "        # Get examples for this class\n",
    "        class_examples = [i for i, label in enumerate(dataset[\"label\"]) if label == class_id]\n",
    "        \n",
    "        if not class_examples:\n",
    "            continue\n",
    "            \n",
    "        # Select a random example\n",
    "        sample_idx = random.choice(class_examples)\n",
    "        \n",
    "        # Load the sample image\n",
    "        sample_path = dataset[\"image_path\"][sample_idx]\n",
    "        sample = np.load(sample_path)\n",
    "        \n",
    "        # For 4D arrays (video data), take the middle frame\n",
    "        if len(sample.shape) == 4:\n",
    "            middle_frame = sample.shape[0] // 2\n",
    "            sample = sample[middle_frame]\n",
    "        \n",
    "        # Plot the sample\n",
    "        plt.subplot(1, len(id2label), class_id + 1)\n",
    "        \n",
    "        # Handle different image formats (RGB, grayscale, etc.)\n",
    "        if len(sample.shape) == 3 and sample.shape[2] == 3:\n",
    "            plt.imshow(sample)\n",
    "        elif len(sample.shape) == 2 or (len(sample.shape) == 3 and sample.shape[2] == 1):\n",
    "            plt.imshow(sample, cmap='gray')\n",
    "        \n",
    "        plt.title(f\"{id2label[class_id]} (ID: {class_id})\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)  # Add more space between subplots\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample from each class\n",
    "print(\"\\nSample Images from Each Class:\")\n",
    "visualize_class_samples(train_ds, id2label)\n",
    "\n",
    "# Print explicit mapping for verification\n",
    "print(\"\\nClass Mapping Verification:\")\n",
    "for i in range(len(id2label)):\n",
    "    class_name = id2label[i]\n",
    "    print(f\"ID {i}: {class_name}\")\n",
    "    \n",
    "    # Count samples for this class in training set\n",
    "    train_count = sum(1 for label in train_ds[\"label\"] if label == i)\n",
    "    print(f\"  - Training samples: {train_count}\")\n",
    "    \n",
    "    # Count samples for this class in validation set\n",
    "    val_count = sum(1 for label in val_ds[\"label\"] if label == i)\n",
    "    print(f\"  - Validation samples: {val_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf7ea2",
   "metadata": {},
   "source": [
    "### 3.2. Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e2b85",
   "metadata": {},
   "source": [
    "#### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1daf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEForVideoClassification, AutoImageProcessor\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "model_ckpt = 'MCG-NJU/videomae-base'\n",
    "sw_processor = AutoImageProcessor.from_pretrained(model_ckpt, use_fast=True)\n",
    "\n",
    "# Use the length of ordered_labels instead of unique_labels\n",
    "sw_model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=len(ordered_labels),  # Changed from unique_labels to ordered_labels\n",
    "    id2label=id2label,\n",
    "    label2id=label2id, \n",
    ")\n",
    "\n",
    "print(sw_model.config)\n",
    "print(summary(sw_model,\n",
    "        input_size=(1, 16, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        row_settings=[\"depth\"],\n",
    "        device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2203d2a",
   "metadata": {},
   "source": [
    "#### Set Up Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import torchvision.transforms.functional as tvf\n",
    "import einops\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalize a (T, C, H, W) tensor by per-channel mean/std,\n",
    "    treating T as the batch dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 mean: Tuple[float, float, float],\n",
    "                 std:  Tuple[float, float, float],\n",
    "                 inplace: bool = False):\n",
    "        super().__init__()\n",
    "        self.mean    = mean\n",
    "        self.std     = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (T, C, H, W); tvf.normalize expects (N, C, H, W)\n",
    "        return tvf.normalize(x, self.mean, self.std, self.inplace)\n",
    "    \n",
    "mean = sw_processor.image_mean\n",
    "std = sw_processor.image_std\n",
    "num_frames_to_samples = sw_model.config.num_frames\n",
    "height = sw_processor.size.get(\"shortest_edge\", sw_processor.size.get(\"height\"))\n",
    "width = height\n",
    "resize_to = (height, width)\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip\n",
    "\n",
    "transformation_pipeline = Compose([\n",
    "    Lambda(lambda x: x / 255.0),                          # now x is float [0,1]\n",
    "    Normalize(mean, std),                                 # perâ€‘channel norm\n",
    "    Lambda(lambda x: F.interpolate(                             \n",
    "        x, size=resize_to, mode=\"bilinear\", align_corners=False \n",
    "    )),                                                          # resize to (H,W) = resize_to\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c2e45",
   "metadata": {},
   "source": [
    "#### Apply Transformation Pipeline to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(batch):\n",
    "    pixel_values = []\n",
    "    for path in batch[\"image_path\"]:\n",
    "        # Load the numpy array from file - shape (T, H, W, C) = (16, 800, 1280, 3)\n",
    "        arr = np.load(path)\n",
    "        \n",
    "        # Convert numpy array to tensor\n",
    "        vid = torch.as_tensor(arr, dtype=torch.float32)\n",
    "        \n",
    "        # Rearrange from (T, H, W, C) to (T, C, H, W) using einops\n",
    "        vid = einops.rearrange(vid, 't h w c -> t c h w')\n",
    "        \n",
    "        # Apply transformations\n",
    "        vid_t = transformation_pipeline(vid)\n",
    "        pixel_values.append(vid_t)\n",
    "    \n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"] = batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "def preprocess_val(batch):\n",
    "    pixel_values = []\n",
    "    for path in batch[\"image_path\"]:\n",
    "        # Load the numpy array from file - shape (T, H, W, C) = (16, 800, 1280, 3)\n",
    "        arr = np.load(path)\n",
    "        \n",
    "        # Convert numpy array to tensor\n",
    "        vid = torch.as_tensor(arr, dtype=torch.float32)\n",
    "        \n",
    "        # Rearrange from (T, H, W, C) to (T, C, H, W) using einops\n",
    "        vid = einops.rearrange(vid, 't h w c -> t c h w')\n",
    "        \n",
    "        # Apply transformations\n",
    "        vid_t = transformation_pipeline(vid)\n",
    "        pixel_values.append(vid_t)\n",
    "    \n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"] = batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing to training dataset\n",
    "train_ds = train_ds.map(\n",
    "    preprocess_train, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=[\"image_path\", \"label\"]\n",
    ")\n",
    "train_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "\n",
    "# Apply preprocessing to validation dataset\n",
    "val_ds = val_ds.map(\n",
    "    preprocess_val, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=[\"image_path\", \"label\"]\n",
    ")\n",
    "val_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d7099",
   "metadata": {},
   "source": [
    "## 4. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb00de1",
   "metadata": {},
   "source": [
    "### 4.1. Setup `TrainingArguments`, `Trainer`, and `evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback, TrainingArguments\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-without-aug\"\n",
    "num_train_epochs = 50\n",
    "EXPERIMENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "SAVE_DIR = f\"experiments/video/{EXPERIMENT_DATE}/{new_model_name}\"\n",
    "batch_size = 8\n",
    "\n",
    "args_pr = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    remove_unused_columns=False, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "confusion = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # scalar metrics as beforeâ€¦\n",
    "    acc   = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec  = precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "    rec   = recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1sc  = f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "    # get confusion matrix and turn it into a nested Python list\n",
    "    cm = confusion.compute(predictions=preds, references=labels)[\"confusion_matrix\"]\n",
    "    cm_list = cm.tolist()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":          acc,\n",
    "        \"precision\":         prec,\n",
    "        \"recall\":            rec,\n",
    "        \"f1\":                f1sc,\n",
    "        \"confusion_matrix\":  cm_list,    # now JSONâ€‘serializable\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # these will accumulate _all_ batches in the current epoch\n",
    "        self.epoch_losses     = []\n",
    "        self.epoch_preds      = []\n",
    "        self.epoch_labels     = []\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides Trainer.compute_loss to store batchâ€level loss & preds/labels.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\", None)\n",
    "        outputs = model(**inputs)\n",
    "        loss   = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if labels is not None:\n",
    "            # 1) store the loss\n",
    "            self.epoch_losses.append(loss.item())\n",
    "            # 2) store predictions + labels as 1D arrays\n",
    "            preds = logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labs  = labels.detach().cpu().numpy()\n",
    "            self.epoch_preds .extend(preds.tolist())\n",
    "            self.epoch_labels.extend(labs.tolist())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self, trainer):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "\n",
    "        # lists to hold epochâ€byâ€epoch values\n",
    "        self.train_losses     = []\n",
    "        self.train_accuracies = []\n",
    "        self.eval_losses      = []\n",
    "        self.eval_accuracies  = []\n",
    "        self.eval_confusion_matrices = [] \n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Compute average training loss & accuracy for the epoch\n",
    "        t = self.trainer\n",
    "        avg_loss = float(np.mean(t.epoch_losses))\n",
    "        acc      = np.mean(\n",
    "            np.array(t.epoch_preds) == np.array(t.epoch_labels)\n",
    "        )\n",
    "\n",
    "        # Store & clear for next epoch\n",
    "        self.train_losses    .append(avg_loss)\n",
    "        self.train_accuracies.append(acc)\n",
    "        t.epoch_losses .clear()\n",
    "        t.epoch_preds  .clear()\n",
    "        t.epoch_labels .clear()\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # metrics come prefixed with \"eval_\"\n",
    "        self.eval_losses   .append(metrics[\"eval_loss\"])\n",
    "        self.eval_accuracies.append(metrics[\"eval_accuracy\"])\n",
    "        self.eval_confusion_matrices.append(metrics[\"eval_confusion_matrix\"])\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=copy.deepcopy(sw_model),                 \n",
    "    args=args_pr,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=sw_processor,          \n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "\n",
    "metrics_cb = MetricsCallback(trainer)\n",
    "trainer.add_callback(metrics_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226da2b0",
   "metadata": {},
   "source": [
    "### 4.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46644249",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcea2db",
   "metadata": {},
   "source": [
    "### 4.3. Training, Validation History Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_val_ds = {\n",
    "    \"train_loss\":     metrics_cb.train_losses,\n",
    "    \"train_accuracy\": metrics_cb.train_accuracies,\n",
    "    \"eval_loss\":      metrics_cb.eval_losses,\n",
    "    \"eval_accuracy\":  metrics_cb.eval_accuracies,\n",
    "    \"eval_confusion_matrix\": metrics_cb.eval_confusion_matrices,\n",
    "}\n",
    "history_train_val_ds_path = os.path.join(SAVE_DIR, \"history_trainval.pkl\")\n",
    "with open(history_train_val_ds_path, \"wb\") as f:\n",
    "    pickle.dump(history_val_ds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e97a0",
   "metadata": {},
   "source": [
    "### 4.4. Plotting Training, Val Losses and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_val_losses(save_dir: str) -> None:\n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    import glob\n",
    "    import os\n",
    "\n",
    "    # Set global font properties\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 15\n",
    "\n",
    "    # Search for history pickle files\n",
    "    history_files = glob.glob(os.path.join(save_dir, \"*.pkl\"))\n",
    "\n",
    "    for history_path in history_files:\n",
    "        filename = os.path.basename(history_path)\n",
    "\n",
    "        if \"trainval\" not in filename:\n",
    "            continue  # skip non-trainval files\n",
    "\n",
    "        print(f\"Visualizing: {filename}\")\n",
    "        with open(history_path, \"rb\") as f:\n",
    "            hist = pickle.load(f)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(15, 15))\n",
    "\n",
    "        # Plot loss\n",
    "        axs[0].plot(hist[\"train_loss\"], label=\"Train Loss\", color='red')\n",
    "        axs[0].plot(hist[\"eval_loss\"], label=\"Validation Loss\", color='blue')\n",
    "        axs[0].set_title('Posture Recognition Training Loss', fontweight='bold')\n",
    "        axs[0].set_ylabel('Loss')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot accuracy\n",
    "        axs[1].plot(hist[\"train_accuracy\"], label=\"Train Accuracy\", color='orange')\n",
    "        axs[1].plot(hist[\"eval_accuracy\"], label=\"Validation Accuracy\", color='green')\n",
    "        axs[1].set_title('Posture Recognition Training Accuracy', fontweight='bold')\n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_ylabel('Accuracy')\n",
    "        axs[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def visualize_confusion_matrix(save_dir: str, suffix: str, mode: str = \"trainval\") -> None:\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    import glob\n",
    "    import os\n",
    "    # Look for the correct .pkl file based on mode\n",
    "    history_files = glob.glob(os.path.join(save_dir, \"*.pkl\"))\n",
    "    \n",
    "    # If only one history file exists, use it regardless of name\n",
    "    if len(history_files) == 1:\n",
    "        matched_file = history_files[0]\n",
    "    else:\n",
    "        matched_file = None\n",
    "        for file in history_files:\n",
    "            if mode.lower() in os.path.basename(file).lower():\n",
    "                matched_file = file\n",
    "                break\n",
    "                \n",
    "    if not matched_file:\n",
    "        print(f\"[ERROR] No history file found in '{save_dir}' for mode '{mode}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"[INFO] Loading confusion matrix from: {matched_file}\")\n",
    "    with open(matched_file, \"rb\") as f:\n",
    "        hist = pickle.load(f)\n",
    "    \n",
    "    # Decide whether to use the last epoch or a single matrix\n",
    "    if \"eval_confusion_matrix\" in hist:\n",
    "        if isinstance(hist[\"eval_confusion_matrix\"], list):\n",
    "            cm_data = hist[\"eval_confusion_matrix\"][-1]\n",
    "            title = f'Confusion Matrix (Last Epoch - Train/Val) - {suffix}'\n",
    "        else:\n",
    "            cm_data = hist[\"eval_confusion_matrix\"]\n",
    "            title = f\"Confusion Matrix ({mode.capitalize()})\"\n",
    "    else:\n",
    "        print(f\"[ERROR] No confusion matrix data found in history file\")\n",
    "        return\n",
    "    \n",
    "    cm = np.array(cm_data)\n",
    "    class_names = [\n",
    "        \"Hand Open\",\n",
    "        \"Intrinsic Plus\",\n",
    "        \"Straight Fist\",  # Fixed missing comma here\n",
    "        \"Hook Hand\",\n",
    "        \"Hand Close\",\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    ax.set_title(title, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure before showing it\n",
    "    output_path = os.path.join(save_dir, f\"confusion_matrix_{mode}.png\")\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"[INFO] Confusion matrix saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_train_val_losses(r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\experiments\\video\\20250516\\20250516-aug-3')\n",
    "visualize_confusion_matrix(r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\experiments\\video\\20250515\\videomae-base-finetuned-without-aug-hand-focused', mode=\"trainval\", suffix='Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2110e",
   "metadata": {},
   "source": [
    "## 5. Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac8755",
   "metadata": {},
   "source": [
    "### 5.1. Video Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from transformers import VideoMAEForVideoClassification, AutoImageProcessor\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "import torchvision.transforms.functional as tvf\n",
    "from typing import Tuple, List\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalize a (T, C, H, W) tensor by per-channel mean/std,\n",
    "    treating T as the batch dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 mean: Tuple[float, float, float],\n",
    "                 std:  Tuple[float, float, float],\n",
    "                 inplace: bool = False):\n",
    "        super().__init__()\n",
    "        self.mean    = mean\n",
    "        self.std     = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (T, C, H, W); tvf.normalize expects (N, C, H, W)\n",
    "        return tvf.normalize(x, self.mean, self.std, self.inplace)\n",
    "\n",
    "def sliding_window_sample(images, window_size=16, stride=8):\n",
    "    \"\"\"\n",
    "    Create sliding windows from a sequence of frames.\n",
    "    \n",
    "    Args:\n",
    "        images: List of frames or numpy array of shape [T, H, W, C]\n",
    "        window_size: Number of frames in each window\n",
    "        stride: Step size between windows\n",
    "        \n",
    "    Returns:\n",
    "        List of numpy arrays, each with shape [window_size, H, W, C]\n",
    "    \"\"\"\n",
    "    total_frames = len(images)\n",
    "    windows = []\n",
    "    \n",
    "    # Handle case where there aren't enough frames for a single window\n",
    "    if total_frames < window_size:\n",
    "        # Duplicate the last frame to reach window_size\n",
    "        padding_needed = window_size - total_frames\n",
    "        padded_images = list(images) + [images[-1]] * padding_needed\n",
    "        windows.append(np.stack(padded_images))\n",
    "        return windows\n",
    "    \n",
    "    # Create windows with stride\n",
    "    for start_idx in range(0, total_frames - window_size + 1, stride):\n",
    "        end_idx = start_idx + window_size\n",
    "        # Make a copy to ensure contiguous memory\n",
    "        window = np.ascontiguousarray(np.stack(images[start_idx:end_idx]))\n",
    "        windows.append(window)\n",
    "    \n",
    "    # If the last window doesn't align with the stride, add one more window\n",
    "    # that takes the last window_size frames\n",
    "    if (total_frames - window_size) % stride != 0:\n",
    "        last_start = total_frames - window_size\n",
    "        if last_start > (start_idx + stride):  # Only if it's different enough from the last added window\n",
    "            window = np.ascontiguousarray(np.stack(images[last_start:]))\n",
    "            windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "def load_model(model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Load the fine-tuned VideoMAE model and processor.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the fine-tuned model directory\n",
    "        device: Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded model\n",
    "        processor: Image processor\n",
    "        id2label: Dictionary mapping from ID to class label\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    \n",
    "    # Load the processor from the base model\n",
    "    processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\", use_fast=True)\n",
    "    \n",
    "    # Load the model config to get id2label mapping\n",
    "    model = VideoMAEForVideoClassification.from_pretrained(model_path)\n",
    "    id2label = model.config.id2label\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, processor, id2label\n",
    "\n",
    "def prepare_transforms(processor, model):\n",
    "    \"\"\"\n",
    "    Build transform pipeline for video processing.\n",
    "    \n",
    "    Args:\n",
    "        processor: Image processor\n",
    "        model: VideoMAE model\n",
    "        \n",
    "    Returns:\n",
    "        transform: Composition of transforms\n",
    "    \"\"\"\n",
    "    mean, std = processor.image_mean, processor.image_std\n",
    "    num_frames = model.config.num_frames\n",
    "    size = processor.size.get(\"shortest_edge\", processor.size.get(\"height\"))\n",
    "    resize_to = (size, size)\n",
    "    \n",
    "    transform = Compose([\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        Normalize(mean, std),\n",
    "        Lambda(lambda x: F.interpolate(\n",
    "            x, size=resize_to, mode=\"bilinear\", align_corners=False\n",
    "        )),\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def process_window(window, transform, device):\n",
    "    \"\"\"\n",
    "    Process a window of frames for model input.\n",
    "    \n",
    "    Args:\n",
    "        window: Numpy array of shape [T, H, W, C]\n",
    "        transform: Transform pipeline\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Tensor ready for model input\n",
    "    \"\"\"\n",
    "    # Force a copy with positive strides\n",
    "    window = window.copy()\n",
    "    \n",
    "    # Convert BGR to RGB if needed (OpenCV loads as BGR)\n",
    "    if window.shape[-1] == 3:  # If channels-last format\n",
    "        window = window[..., ::-1].copy()  # BGR to RGB with explicit copy\n",
    "    \n",
    "    # Convert to [T, C, H, W] format (channels first)\n",
    "    if window.shape[-1] == 3:  # If channels-last format\n",
    "        window = np.transpose(window, (0, 3, 1, 2)).copy()  # Another explicit copy\n",
    "    \n",
    "    # Convert to tensor - try with numpy array interface\n",
    "    try:\n",
    "        window_tensor = torch.from_numpy(window).float()\n",
    "    except:\n",
    "        # If that fails, try with a list conversion (less efficient but more robust)\n",
    "        window_tensor = torch.tensor(window.tolist(), dtype=torch.float32)\n",
    "    \n",
    "    # Apply transforms\n",
    "    window_tensor = transform(window_tensor)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    window_tensor = window_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    return window_tensor\n",
    "\n",
    "def predict_windows(windows, model, transform, device):\n",
    "    \"\"\"\n",
    "    Run prediction on a list of windows.\n",
    "    \n",
    "    Args:\n",
    "        windows: List of numpy arrays, each with shape [T, H, W, C]\n",
    "        model: VideoMAE model\n",
    "        transform: Transform pipeline\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        List of (class_id, confidence) tuples for each window\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for window in windows:\n",
    "            # Process window\n",
    "            inputs = process_window(window, transform, device)\n",
    "            \n",
    "            # Run inference\n",
    "            outputs = model(pixel_values=inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            pred_class = torch.argmax(logits, dim=-1).item()\n",
    "            confidence = probs[0, pred_class].item()\n",
    "            \n",
    "            results.append((pred_class, confidence))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def predict_video(video_path, model_path, output_path=None, window_size=16, stride=8):\n",
    "    \"\"\"\n",
    "    Predict activity in a video using the fine-tuned VideoMAE model.\n",
    "    Draws predictions on the video frames and saves output video.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        model_path: Path to the fine-tuned model directory\n",
    "        output_path: Path to save the output video (None for display only)\n",
    "        window_size: Number of frames in each window\n",
    "        stride: Step size between windows\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, processor, id2label = load_model(model_path, device)\n",
    "    transform = prepare_transforms(processor, model)\n",
    "    \n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Setup video writer if output path is specified\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use appropriate codec\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Read all frames\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Video loaded: {len(frames)} frames\")\n",
    "    \n",
    "    # Create windows\n",
    "    windows = sliding_window_sample(frames, window_size, stride)\n",
    "    print(f\"Created {len(windows)} windows\")\n",
    "    \n",
    "    # Run predictions\n",
    "    results = predict_windows(windows, model, transform, device)\n",
    "    \n",
    "    # Process results and create output video\n",
    "    window_duration = window_size / fps\n",
    "    predictions = []\n",
    "    \n",
    "    # Map window predictions to frames\n",
    "    frame_predictions = [None] * len(frames)\n",
    "    for i, (class_id, confidence) in enumerate(results):\n",
    "        start_frame = i * stride\n",
    "        end_frame = min(start_frame + window_size, len(frames))\n",
    "        \n",
    "        class_name = id2label[class_id]\n",
    "        timestamp = start_frame / fps\n",
    "        \n",
    "        prediction = {\n",
    "            'timestamp': timestamp,\n",
    "            'class_id': class_id,\n",
    "            'class_name': class_name,\n",
    "            'confidence': confidence,\n",
    "            'start_frame': start_frame,\n",
    "            'end_frame': end_frame\n",
    "        }\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        # Assign this prediction to all frames in the window\n",
    "        for frame_idx in range(start_frame, end_frame):\n",
    "            if frame_idx < len(frame_predictions):\n",
    "                frame_predictions[frame_idx] = prediction\n",
    "    \n",
    "    # Add prediction overlay to frames and write/display video\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Add black rectangle on top\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (0, 0), (width, 60), (0, 0, 0), -1)\n",
    "        \n",
    "        # Add text\n",
    "        if frame_predictions[i]:\n",
    "            pred = frame_predictions[i]\n",
    "            text = f\"{pred['class_name']} ({pred['confidence']:.2f})\"\n",
    "            cv2.putText(overlay, text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(overlay, \"Processing...\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        # Write frame\n",
    "        if output_path:\n",
    "            out.write(overlay)\n",
    "    \n",
    "    # Release resources\n",
    "    if output_path:\n",
    "        out.release()\n",
    "        print(f\"Output video saved to {output_path}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'id2label': id2label,\n",
    "        'num_frames': len(frames),\n",
    "        'fps': fps\n",
    "    }\n",
    "\n",
    "def predict_from_array(video_array, model_path, window_size=16, stride=8):\n",
    "    \"\"\"\n",
    "    Predict activity from a numpy array of video frames.\n",
    "    \n",
    "    Args:\n",
    "        video_array: Numpy array of shape [T, H, W, C]\n",
    "        model_path: Path to the fine-tuned model directory\n",
    "        window_size: Number of frames in each window\n",
    "        stride: Step size between windows\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results and processed frames with overlays\n",
    "    \"\"\"\n",
    "    # Ensure contiguous array to avoid stride issues\n",
    "    video_array = np.ascontiguousarray(video_array)\n",
    "    \n",
    "    # Load the model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, processor, id2label = load_model(model_path, device)\n",
    "    transform = prepare_transforms(processor, model)\n",
    "    \n",
    "    # Get video dimensions\n",
    "    total_frames = len(video_array)\n",
    "    height, width = video_array[0].shape[:2]\n",
    "    \n",
    "    # Create windows\n",
    "    windows = sliding_window_sample(video_array, window_size, stride)\n",
    "    print(f\"Created {len(windows)} windows from {total_frames} frames\")\n",
    "    \n",
    "    # Run predictions\n",
    "    results = predict_windows(windows, model, transform, device)\n",
    "    \n",
    "    # Process results\n",
    "    predictions = []\n",
    "    frame_predictions = [None] * total_frames\n",
    "    \n",
    "    for i, (class_id, confidence) in enumerate(results):\n",
    "        start_frame = i * stride\n",
    "        end_frame = min(start_frame + window_size, total_frames)\n",
    "        \n",
    "        class_name = id2label[class_id]\n",
    "        \n",
    "        prediction = {\n",
    "            'window_idx': i,\n",
    "            'class_id': class_id,\n",
    "            'class_name': class_name,\n",
    "            'confidence': confidence,\n",
    "            'start_frame': start_frame,\n",
    "            'end_frame': end_frame\n",
    "        }\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "        # Assign this prediction to all frames in the window\n",
    "        for frame_idx in range(start_frame, end_frame):\n",
    "            if frame_idx < len(frame_predictions):\n",
    "                frame_predictions[frame_idx] = prediction\n",
    "    \n",
    "    # Add prediction overlay to frames\n",
    "    processed_frames = []\n",
    "    for i, frame in enumerate(video_array):\n",
    "        # Make a copy of the frame\n",
    "        overlay = frame.copy()\n",
    "        \n",
    "        # Add black rectangle on top\n",
    "        cv2.rectangle(overlay, (0, 0), (width, 60), (0, 0, 0), -1)\n",
    "        \n",
    "        # Add text\n",
    "        if i < len(frame_predictions) and frame_predictions[i]:\n",
    "            pred = frame_predictions[i]\n",
    "            text = f\"{pred['class_name']} ({pred['confidence']:.2f})\"\n",
    "            cv2.putText(overlay, text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(overlay, \"No prediction\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        processed_frames.append(overlay)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'id2label': id2label,\n",
    "        'processed_frames': processed_frames\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Demo function to run prediction on a video file\"\"\"\n",
    "    try:\n",
    "        # Load the numpy array first\n",
    "        print(\"Loading numpy array...\")\n",
    "        video_array = np.load('./cropped_hands_sequence.npy')\n",
    "        print(f\"Loaded array with shape: {video_array.shape}\")\n",
    "        \n",
    "        # Ensure the array is C-contiguous\n",
    "        if not video_array.flags.c_contiguous:\n",
    "            print(\"Converting to C-contiguous array...\")\n",
    "            video_array = np.ascontiguousarray(video_array)\n",
    "        \n",
    "        result = predict_from_array(\n",
    "            video_array, \n",
    "            r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\experiments\\video\\20250516\\20250516-aug-6\\checkpoint-240',\n",
    "            window_size=16,\n",
    "            stride=16\n",
    "        )\n",
    "\n",
    "        vis = []\n",
    "        \n",
    "        # Save the processed frames if needed\n",
    "        if 'processed_frames' in result:\n",
    "            output_dir = './cropped_hands_sequence_output'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Save each processed frame\n",
    "            for i, frame in enumerate(result['processed_frames']):\n",
    "                cv2.imwrite(f\"{output_dir}/frame_{i:04d}.png\", frame)\n",
    "                vis.append(frame)\n",
    "            \n",
    "            print(f\"Saved {len(result['processed_frames'])} processed frames to {output_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(result['predictions'])} activity segments\")\n",
    "        for i, pred in enumerate(result['predictions']):\n",
    "            print(f\"Segment {i+1}: {pred['class_name']} (confidence: {pred['confidence']:.2f})\")\n",
    "\n",
    "        return vis\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vis = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
