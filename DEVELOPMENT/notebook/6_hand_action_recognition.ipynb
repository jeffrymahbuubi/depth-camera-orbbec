{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad688dd",
   "metadata": {},
   "source": [
    "# Tendon Gliding Hand Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54cd334",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "- The goal is to perform Tendon Gliding Hand Action Recognition by classifying hand postures into five distinct classes using real-time video. Additionally, the task involves calculating the accuracy of hand skeleton occurrences compared to the ground truth.\n",
    "- The classes are as follows: **Hand Open**, **Intrinsic Plan**, **Straight Fist**, **Hand Close**, and **Hook Hand**.  \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src='../../IMAGES/Classes.jpg' width=\"500px\" />\n",
    "</p>\n",
    "\n",
    "- The base workflow is as follows: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Import RAW Data]\n",
    "    A --> B[Separate into 5 different classes]\n",
    "    B --> C[Generate 2D keypoints of X and Y coordinates]\n",
    "    C --> D[Train LSTM model]\n",
    "    D --> E[Evaluate Performance]\n",
    "    E --> F[Metrics: Accuracy, Specificity, Sensitivity, F1-Score, Confusion Matrix]\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc7005",
   "metadata": {},
   "source": [
    "## 0. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ac3d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import rerun as rr\n",
    "import rerun.blueprint as rrb\n",
    "import seedir as sd\n",
    "import os\n",
    "import albumentations as A\n",
    "from pyorbbecsdk import *\n",
    "import cv2\n",
    "from utils import frame_to_bgr_image\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"NO_ALBUMENTATIONS_UPDATE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4de32",
   "metadata": {},
   "source": [
    "## 1. Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e3b579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bag_files(root_dir='data'):\n",
    "    \"\"\"\n",
    "    List all .bag files in the directory structure, excluding those in 'open-hand' folders.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory to start the search from.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of paths to .bag files.\n",
    "    \"\"\"\n",
    "    bag_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        # Skip 'open-hand' directories\n",
    "        if 'open-fist' in os.path.basename(root):\n",
    "            continue\n",
    "            \n",
    "        # Add .bag files to the list\n",
    "        for file in files:\n",
    "            if file.endswith('.bag'):\n",
    "                bag_files.append(os.path.join(root, file))\n",
    "                \n",
    "    return bag_files\n",
    "\n",
    "def playback_state_callback(state):\n",
    "    \"\"\"Callback function to handle playback state transitions.\"\"\"\n",
    "    global playback_finished\n",
    "    if state == OBMediaState.OB_MEDIA_BEGIN:\n",
    "        print(\"Bag player begin\")\n",
    "    elif state == OBMediaState.OB_MEDIA_END:\n",
    "        print(\"Bag player end\")\n",
    "        playback_finished = True  # Signal that playback has finished\n",
    "    elif state == OBMediaState.OB_MEDIA_PAUSED:\n",
    "        print(\"Bag player paused\")\n",
    "\n",
    "def process_frames(bag_file):\n",
    "    \"\"\"\n",
    "    Process the .bag file and return lists of processed images.\n",
    "    \n",
    "    Returns:\n",
    "        depth_image_list: List of raw depth data (converted to float and scaled).\n",
    "        color_image_list: List of processed color images.\n",
    "        overlaid_image_list: List of images with overlay (color blended with depth colormap).\n",
    "    \"\"\"\n",
    "    global playback_finished\n",
    "    playback_finished = False  # Reset flag\n",
    "\n",
    "    pipeline = Pipeline(bag_file)\n",
    "    playback = pipeline.get_playback()\n",
    "    playback.set_playback_state_callback(playback_state_callback)\n",
    "\n",
    "    # Start the pipeline\n",
    "    pipeline.start()\n",
    "\n",
    "    depth_image_list = []\n",
    "    color_image_list = []\n",
    "\n",
    "    while not playback_finished:\n",
    "        frames = pipeline.wait_for_frames(100)\n",
    "        if frames is None:\n",
    "            if playback_finished:\n",
    "                print(\"All frames have been processed and converted successfully.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Retrieve frames once per iteration\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        if depth_frame is not None:\n",
    "            width = depth_frame.get_width()\n",
    "            height = depth_frame.get_height()\n",
    "            scale = depth_frame.get_depth_scale()\n",
    "\n",
    "            # Process raw depth data\n",
    "            depth_data = np.frombuffer(depth_frame.get_data(), dtype=np.uint16)\n",
    "            depth_data = depth_data.reshape((height, width))\n",
    "            depth_data = depth_data.astype(np.float32) * scale\n",
    "            depth_image_list.append(depth_data)\n",
    "\n",
    "            # Normalize and invert to obtain desired mapping (farthest = red, closest = blue)\n",
    "            depth_norm = cv2.normalize(depth_data, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "            inverted_depth = 255 - depth_norm\n",
    "            depth_image = cv2.applyColorMap(inverted_depth, cv2.COLORMAP_JET)\n",
    "        else:\n",
    "            depth_image = None\n",
    "\n",
    "        if color_frame is not None:\n",
    "            width = color_frame.get_width()\n",
    "            height = color_frame.get_height()\n",
    "\n",
    "            color_data = frame_to_bgr_image(color_frame)\n",
    "            color_image = cv2.resize(color_data, (width, height))\n",
    "            # Convert to BGR if necessary; adjust if frame_to_bgr_image already outputs BGR\n",
    "            color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "            color_image_list.append(color_image)\n",
    "        else:\n",
    "            color_image = None\n",
    "\n",
    "    return depth_image_list, color_image_list\n",
    "\n",
    "def rerun_visualization(*image_lists):\n",
    "    \"\"\"\n",
    "    Create a rerun visualization with multiple image lists displayed in a grid.\n",
    "    \n",
    "    Args:\n",
    "        *image_lists: Variable number of image lists to display\n",
    "    \"\"\"\n",
    "    stream = rr.new_recording(\"spawn\", spawn=True)\n",
    "    \n",
    "    # Dynamically create spatial views for each image list\n",
    "    spatial_views = []\n",
    "    for i in range(len(image_lists)):\n",
    "        spatial_views.append(rrb.Spatial2DView(origin=f'/color_image_{i}'))\n",
    "    \n",
    "    # Calculate a reasonable number of columns for the grid\n",
    "    # You can adjust this logic based on your preference\n",
    "    num_columns = min(3, len(image_lists))  # Max 3 columns\n",
    "    \n",
    "    # Setup the blueprint with dynamic grid configuration\n",
    "    blueprint = rrb.Blueprint(\n",
    "        rrb.Grid(*spatial_views, grid_columns=num_columns),\n",
    "        collapse_panels=True\n",
    "    )\n",
    "    \n",
    "    # Calculate the maximum length across all image lists\n",
    "    max_length = max(len(image_list) for image_list in image_lists)\n",
    "    \n",
    "    # Log all images with proper time sequencing\n",
    "    for idx in range(max_length):\n",
    "        stream.set_time_sequence(\"frame\", idx)\n",
    "        \n",
    "        # Log each image list at the current index if available\n",
    "        for list_idx, image_list in enumerate(image_lists):\n",
    "            if idx < len(image_list):\n",
    "                stream.log(f\"color_image_{list_idx}\", rr.Image(image_list[idx]))\n",
    "    \n",
    "    stream.send_blueprint(blueprint)\n",
    "\n",
    "def sliding_window_sample(images, window_size=16, stride=8):\n",
    "    total_frames = len(images)\n",
    "    windows = []\n",
    "    \n",
    "    for start_idx in range(0, total_frames - window_size + 1, stride):\n",
    "        end_idx = start_idx + window_size\n",
    "        window = np.stack(images[start_idx:end_idx])\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "def visualize_windows(windows):\n",
    "    \"\"\"\n",
    "    Visualize multiple windows of image sequences in Rerun.\n",
    "    \n",
    "    Args:\n",
    "        windows: List of image windows, where each window is a sequence of frames\n",
    "        max_windows_to_display: Maximum number of windows to display in the grid\n",
    "    \"\"\"\n",
    "    stream = rr.new_recording(\"spawn\", spawn=True)\n",
    "    \n",
    "    # Limit the number of windows to display to avoid overcrowding\n",
    "    num_windows = len(windows)\n",
    "    \n",
    "    # Create spatial views for each window\n",
    "    spatial_views = []\n",
    "    for i in range(num_windows):\n",
    "        spatial_views.append(rrb.Spatial2DView(origin=f'/window_{i}'))\n",
    "    \n",
    "    # Calculate grid layout (max 3 columns)\n",
    "    num_columns = min(3, num_windows)\n",
    "    \n",
    "    # Setup the blueprint with dynamic grid configuration\n",
    "    blueprint = rrb.Blueprint(\n",
    "        rrb.Grid(*spatial_views, grid_columns=num_columns),\n",
    "        collapse_panels=True\n",
    "    )\n",
    "    \n",
    "    # Log the frames of each window\n",
    "    window_size = windows[0].shape[0]  # Get the size of each window\n",
    "    \n",
    "    # Log each frame in each window\n",
    "    for frame_idx in range(window_size):\n",
    "        stream.set_time_sequence(\"frame\", frame_idx)\n",
    "        \n",
    "        # Log the current frame from each window\n",
    "        for window_idx in range(num_windows):\n",
    "            if window_idx < len(windows):\n",
    "                stream.log(f\"window_{window_idx}\", \n",
    "                          rr.Image(windows[window_idx][frame_idx]))\n",
    "    \n",
    "    # Display info about the windows\n",
    "    print(f\"Visualizing {num_windows} windows out of {len(windows)} total\")\n",
    "    print(f\"Each window contains {window_size} frames\")\n",
    "    print(f\"Window shape: {windows[0].shape}\")\n",
    "    \n",
    "    stream.send_blueprint(blueprint)\n",
    "\n",
    "def save_windowed_data(recordings, bag_files, window_size=16, stride=8):\n",
    "    \"\"\"\n",
    "    Save windowed data for RGB and depth images for each recording to its respective folder.\n",
    "    \n",
    "    Args:\n",
    "        recordings: Dictionary containing recordings with color_images and depth_images\n",
    "        bag_files: List of bag file paths sorted to match recording indices\n",
    "        window_size: Size of each window\n",
    "        stride: Stride between consecutive windows\n",
    "    \"\"\"\n",
    "    print(f\"Saving windowed data for {len(recordings)} recordings...\")\n",
    "    \n",
    "    for recording_idx, bag_file_path in enumerate(bag_files):\n",
    "        recording_key = f\"recording_{recording_idx}\"\n",
    "        \n",
    "        # Check if the recording exists\n",
    "        if recording_key in recordings:\n",
    "            recording_data = recordings[recording_key]\n",
    "            # Create base output directory for this recording\n",
    "            bag_path = Path(bag_file_path)\n",
    "            base_output_dir = bag_path.parent / \"windowed_data\"\n",
    "            \n",
    "            # Process each image type (RGB and depth)\n",
    "            image_types = {\n",
    "                \"color_images\": \"rgb\",\n",
    "                \"depth_images\": \"depth\"\n",
    "            }\n",
    "            \n",
    "            for image_key, folder_name in image_types.items():\n",
    "                if image_key in recording_data and recording_data[image_key] is not None:\n",
    "                    images = recording_data[image_key]\n",
    "                    \n",
    "                    # Create windows for this image type\n",
    "                    windows = sliding_window_sample(images, window_size=window_size, stride=stride)\n",
    "                    \n",
    "                    # Create output directory for this image type\n",
    "                    output_dir = base_output_dir / folder_name\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Create a base filename from the bag file\n",
    "                    base_filename = bag_path.stem\n",
    "                    \n",
    "                    # Save each window as a separate .npy file\n",
    "                    for window_idx, window in enumerate(windows):\n",
    "                        # Create filename: RecordXXX_window_NNN.npy\n",
    "                        window_filename = f\"{base_filename}_window_{window_idx:03d}.npy\"\n",
    "                        output_path = output_dir / window_filename\n",
    "                        \n",
    "                        # Save the window data\n",
    "                        np.save(output_path, window)\n",
    "                    \n",
    "                    print(f\"Saved {len(windows)} {folder_name} windows for {recording_key} to {output_dir}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {image_key} for {recording_key}: Data not available\")\n",
    "        else:\n",
    "            print(f\"Skipping recording {recording_idx}: Recording not found\")\n",
    "    \n",
    "    print(\"Windowed data saving complete!\")\n",
    "\n",
    "# Function to create all the recordings dictionary from processed_data\n",
    "def prepare_recordings_dict(processed_data):\n",
    "    recordings = {}\n",
    "    for i in range(6):  # Assuming there are 6 recordings (0-5)\n",
    "        recording_key = f\"recording_{i}\"\n",
    "        if recording_key in processed_data:\n",
    "            recordings[recording_key] = processed_data[recording_key]\n",
    "    return recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66277b49",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "- In total there are six different recordings of tendon gliding task as follows: \n",
    "    - From üìÅ `20250402`\n",
    "        - `Record_20250402151124.bag`\n",
    "        - `Record_20250402151609.bag`\n",
    "        - `Record_20250402152331.bag`\n",
    "    - From üìÅ `20250506`\n",
    "        - `Record_20250506145704.bag`\n",
    "        - `Record_20250506152951.bag`\n",
    "        - `Record_20250506162630.bag`\n",
    "- Each data consist of RGB frames and Depth frames\n",
    "- Workflow: \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data Extraction] --> B[Data Visualization]\n",
    "    B --> C[Windowing Process]\n",
    "    C --> D[Save Windowed Data]\n",
    "    D --> E[Data Class Generation]\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af7b46",
   "metadata": {},
   "source": [
    "**Data Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c173b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bag file: data\\20250402\\Record_20250402151124.bag\n",
      "Bag player begin\n",
      "Bag player end\n",
      "Processing bag file: data\\20250402\\Record_20250402151609.bag\n",
      "Bag player begin\n",
      "Bag player end\n",
      "Processing bag file: data\\20250402\\Record_20250402152331.bag\n",
      "Bag player begin\n",
      "Bag player end\n",
      "Processing bag file: data\\20250506\\T2\\tendon-gliding\\Record_20250506145704.bag\n",
      "Bag player begin\n",
      "Bag player end\n",
      "Processing bag file: data\\20250506\\T3\\tendon-gliding\\Record_20250506152951.bag\n",
      "Bag player begin\n",
      "Bag player end\n",
      "Processing bag file: data\\20250506\\T4\\tendon-gliding\\Record_20250506162630.bag\n",
      "Bag player begin\n",
      "Bag player end\n"
     ]
    }
   ],
   "source": [
    "bag_files = list_bag_files()\n",
    "data_20250402 = bag_files[:3]      # First three items\n",
    "data_20250506 = bag_files[-3:]     # Last three items\n",
    "processed_data = {}\n",
    "\n",
    "# Process the 20250402 files\n",
    "for idx, bag_file in enumerate(data_20250402):\n",
    "    print(f\"Processing bag file: {bag_file}\")\n",
    "    # Process the frames in the bag file\n",
    "    depth_images, color_images = process_frames(bag_file)\n",
    "    # Store the lists in the dictionary with dataset identifier\n",
    "    processed_data[f\"recording_{idx}\"] = {\n",
    "        \"dataset\": \"20250402\",\n",
    "        \"depth_images\": depth_images,\n",
    "        \"color_images\": color_images\n",
    "    }\n",
    "\n",
    "# Process the 20250506 files\n",
    "for idx, bag_file in enumerate(data_20250506):\n",
    "    print(f\"Processing bag file: {bag_file}\")\n",
    "    # Process the frames in the bag file\n",
    "    depth_images, color_images = process_frames(bag_file)\n",
    "    # Store the lists in the dictionary with dataset identifier\n",
    "    processed_data[f\"recording_{idx+3}\"] = {  # Add offset to avoid key collision\n",
    "        \"dataset\": \"20250506\",\n",
    "        \"depth_images\": depth_images,\n",
    "        \"color_images\": color_images\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5712de1",
   "metadata": {},
   "source": [
    "**Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2e0dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_0 = processed_data.get(\"recording_0\")\n",
    "recording_1 = processed_data.get(\"recording_1\")\n",
    "recording_2 = processed_data.get(\"recording_2\")\n",
    "recording_3 = processed_data.get(\"recording_3\")\n",
    "recording_4 = processed_data.get(\"recording_4\")\n",
    "recording_5 = processed_data.get(\"recording_5\")\n",
    "\n",
    "image_lists = []\n",
    "for recording_idx in range(6):  # Assuming you have 6 recordings\n",
    "    recording_key = f\"recording_{recording_idx}\"\n",
    "    if recording_key in processed_data and \"color_images\" in processed_data[recording_key]:\n",
    "        image_lists.append(processed_data[recording_key][\"color_images\"])\n",
    "\n",
    "# Visualize all valid image lists together\n",
    "if image_lists:\n",
    "    rerun_visualization(*image_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a96ace",
   "metadata": {},
   "source": [
    "**Windowing Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6683dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 12 windows out of 12 total\n",
      "Each window contains 16 frames\n",
      "Window shape: (16, 800, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get the color images from recording_0\n",
    "color_images = recording_0[\"color_images\"]\n",
    "\n",
    "# Get multiple windows of 16 frames each\n",
    "rgb_windows = sliding_window_sample(color_images, window_size=16, stride=8)\n",
    "\n",
    "# Visualize the windows\n",
    "visualize_windows(rgb_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c54758",
   "metadata": {},
   "source": [
    "**Save Windowed Data into üìÅ `rgb` and üìÅ `depth`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1749f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving windowed data for 6 recordings...\n",
      "Saved 12 rgb windows for recording_0 to data\\20250402\\windowed_data\\rgb\n",
      "Saved 12 depth windows for recording_0 to data\\20250402\\windowed_data\\depth\n",
      "Saved 10 rgb windows for recording_1 to data\\20250402\\windowed_data\\rgb\n",
      "Saved 10 depth windows for recording_1 to data\\20250402\\windowed_data\\depth\n",
      "Saved 13 rgb windows for recording_2 to data\\20250402\\windowed_data\\rgb\n",
      "Saved 13 depth windows for recording_2 to data\\20250402\\windowed_data\\depth\n",
      "Saved 38 rgb windows for recording_3 to data\\20250506\\T2\\tendon-gliding\\windowed_data\\rgb\n",
      "Saved 38 depth windows for recording_3 to data\\20250506\\T2\\tendon-gliding\\windowed_data\\depth\n",
      "Saved 29 rgb windows for recording_4 to data\\20250506\\T3\\tendon-gliding\\windowed_data\\rgb\n",
      "Saved 29 depth windows for recording_4 to data\\20250506\\T3\\tendon-gliding\\windowed_data\\depth\n",
      "Saved 22 rgb windows for recording_5 to data\\20250506\\T4\\tendon-gliding\\windowed_data\\rgb\n",
      "Saved 22 depth windows for recording_5 to data\\20250506\\T4\\tendon-gliding\\windowed_data\\depth\n",
      "Windowed data saving complete!\n"
     ]
    }
   ],
   "source": [
    "recordings = prepare_recordings_dict(processed_data)\n",
    "save_windowed_data(recordings, bag_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f3535",
   "metadata": {},
   "source": [
    "**Data Class Generation**\n",
    "\n",
    "There are 5 class to be pick up from the windowed data: `Hand Open (HO)`, `Intrinsic Hand (IH)`, `Straight Fist (SF)`, `Hand Close (HC)`, and `Hook Hand (HH)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17af1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied Open Hand (OH) sample from Record_20250402151124, window 0 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250402151124, window 10 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250402151124, window 1 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250402151124, window 2 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250402151124, window 3 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250402151124, window 4 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250402151124, window 5 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250402151124, window 6 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250402151124, window 7 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250402151124, window 8 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250402151609, window 0 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250402151609, window 9 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250402151609, window 1 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250402151609, window 2 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250402151609, window 3 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250402151609, window 4 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250402151609, window 5 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250402151609, window 6 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250402151609, window 7 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250402152331, window 0 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250402152331, window 1 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250402152331, window 3 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250402152331, window 4 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250402152331, window 5 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250402152331, window 6 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250402152331, window 7 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250402152331, window 8 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250402152331, window 9 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250506145704, window 0 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250506145704, window 1 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250506145704, window 11 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250506145704, window 12 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250506145704, window 16 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250506145704, window 17 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250506145704, window 22 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250506145704, window 23 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250506145704, window 27 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250506145704, window 28 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250506152951, window 0 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250506152951, window 1 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250506152951, window 5 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250506152951, window 6 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250506152951, window 10 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250506152951, window 11 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250506152951, window 14 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250506152951, window 15 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250506152951, window 18 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250506152951, window 19 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250506162630, window 0 (both rgb and depth)\n",
      "Copied Open Hand (OH) sample from Record_20250506162630, window 1 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250506162630, window 6 (both rgb and depth)\n",
      "Copied Intrinsic Hand (IH) sample from Record_20250506162630, window 7 (both rgb and depth)\n",
      "Copied Straight Fist (SF) sample from Record_20250506162630, window 8 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250506162630, window 9 (both rgb and depth)\n",
      "Copied Hand Close (HC) sample from Record_20250506162630, window 10 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250506162630, window 13 (both rgb and depth)\n",
      "Copied Hook Hand (HH) sample from Record_20250506162630, window 14 (both rgb and depth)\n",
      "Class Open Hand (OH): 12/12 samples collected\n",
      "Class Intrinsic Hand (IH): 12/12 samples collected\n",
      "Class Straight Fist (SF): 11/12 samples collected\n",
      "Need 1 more samples for SF\n",
      "Added extra Straight Fist (SF) sample from Record_20250506145704, window 18 (both rgb and depth)\n",
      "Class Hand Close (HC): 10/12 samples collected\n",
      "Need 2 more samples for HC\n",
      "Added extra Hand Close (HC) sample from Record_20250506145704, window 24 (both rgb and depth)\n",
      "Added extra Hand Close (HC) sample from Record_20250506145704, window 25 (both rgb and depth)\n",
      "Class Hook Hand (HH): 12/12 samples collected\n",
      "Final count for Open Hand (OH):\n",
      "  - RGB: 12 samples\n",
      "  - Depth: 12 samples\n",
      "Final count for Intrinsic Hand (IH):\n",
      "  - RGB: 12 samples\n",
      "  - Depth: 12 samples\n",
      "Final count for Straight Fist (SF):\n",
      "  - RGB: 12 samples\n",
      "  - Depth: 12 samples\n",
      "Final count for Hand Close (HC):\n",
      "  - RGB: 12 samples\n",
      "  - Depth: 12 samples\n",
      "Final count for Hook Hand (HH):\n",
      "  - RGB: 12 samples\n",
      "  - Depth: 12 samples\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(recordings, bag_files, class_info_dict, output_dir=\"video\", max_samples_per_recording=2):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset with RGB and depth samples for each class from each recording.\n",
    "    \n",
    "    Args:\n",
    "        recordings: Dictionary containing recordings with processed data\n",
    "        bag_files: List of bag file paths\n",
    "        class_info_dict: Dictionary mapping recording names to class window indices\n",
    "        output_dir: Output directory name\n",
    "        max_samples_per_recording: Maximum number of samples to include per class per recording\n",
    "    \"\"\"\n",
    "    # Define class acronyms and their full names\n",
    "    class_acronyms = [\"OH\", \"IH\", \"SF\", \"HC\", \"HH\"]\n",
    "    class_full_names = {\n",
    "        \"OH\": \"Open Hand\",\n",
    "        \"IH\": \"Intrinsic Hand\",\n",
    "        \"SF\": \"Straight Fist\",\n",
    "        \"HC\": \"Hand Close\",\n",
    "        \"HH\": \"Hook Hand\"\n",
    "    }\n",
    "    \n",
    "    # Define modalities\n",
    "    modalities = [\"rgb\", \"depth\"]\n",
    "    \n",
    "    # Create main output directory\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Create class and modality folders\n",
    "    for acronym in class_acronyms:\n",
    "        for modality in modalities:\n",
    "            class_modality_folder = output_path / modality / acronym\n",
    "            os.makedirs(class_modality_folder, exist_ok=True)\n",
    "    \n",
    "    # Track which samples we've collected for each class\n",
    "    collected_samples = defaultdict(list)\n",
    "    \n",
    "    # Process each recording\n",
    "    for recording_idx, bag_file_path in enumerate(bag_files):\n",
    "        bag_path = Path(bag_file_path)\n",
    "        recording_name = bag_path.stem\n",
    "        recording_key = f\"recording_{recording_idx}\"\n",
    "        \n",
    "        # Skip if recording doesn't exist in our data\n",
    "        if recording_key not in recordings:\n",
    "            print(f\"Warning: {recording_key} not found in recordings, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Skip if recording doesn't have class information\n",
    "        if recording_name not in class_info_dict:\n",
    "            print(f\"Warning: No class information for {recording_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get class information for this recording\n",
    "        recording_class_info = class_info_dict[recording_name]\n",
    "        \n",
    "        # Process each class in this recording\n",
    "        for acronym in class_acronyms:\n",
    "            # Get available window indices for this class\n",
    "            window_indices = recording_class_info.get(acronym, [])\n",
    "            \n",
    "            # Limit to the maximum number of samples per recording\n",
    "            selected_indices = window_indices[:max_samples_per_recording]\n",
    "            \n",
    "            # For each selected window index\n",
    "            for window_idx in selected_indices:\n",
    "                # Process each modality (rgb and depth)\n",
    "                for modality in modalities:\n",
    "                    # Get windowed data directory for this recording and modality\n",
    "                    windowed_data_dir = bag_path.parent / \"windowed_data\" / modality\n",
    "                    \n",
    "                    # Build the source filename\n",
    "                    window_filename = f\"{recording_name}_window_{window_idx:03d}.npy\"\n",
    "                    source_path = windowed_data_dir / window_filename\n",
    "                    \n",
    "                    # Skip if source file doesn't exist\n",
    "                    if not source_path.exists():\n",
    "                        print(f\"Warning: {modality} file {source_path} not found, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Create destination filename\n",
    "                    dest_filename = f\"{recording_name}_w{window_idx:03d}.npy\"\n",
    "                    dest_path = output_path / modality / acronym / dest_filename\n",
    "                    \n",
    "                    # Copy the file\n",
    "                    shutil.copy2(source_path, dest_path)\n",
    "                \n",
    "                # Track which samples we've collected (only track once per sample, not per modality)\n",
    "                collected_samples[acronym].append((recording_name, window_idx))\n",
    "                \n",
    "                print(f\"Copied {class_full_names[acronym]} ({acronym}) sample from {recording_name}, window {window_idx} (both rgb and depth)\")\n",
    "    \n",
    "    # Check if we have enough samples for each class (12 samples per class)\n",
    "    target_count = len(bag_files) * max_samples_per_recording\n",
    "    for acronym in class_acronyms:\n",
    "        samples_count = len(collected_samples[acronym])\n",
    "        \n",
    "        print(f\"Class {class_full_names[acronym]} ({acronym}): {samples_count}/{target_count} samples collected\")\n",
    "        \n",
    "        # If we don't have enough samples, try to get more from recordings with extra samples\n",
    "        if samples_count < target_count:\n",
    "            additional_needed = target_count - samples_count\n",
    "            print(f\"Need {additional_needed} more samples for {acronym}\")\n",
    "            \n",
    "            # Find all available samples for this class across all recordings\n",
    "            all_available = []\n",
    "            for recording_idx, bag_file_path in enumerate(bag_files):\n",
    "                bag_path = Path(bag_file_path)\n",
    "                recording_name = bag_path.stem\n",
    "                \n",
    "                if recording_name in class_info_dict and acronym in class_info_dict[recording_name]:\n",
    "                    # Get indices that haven't been used yet\n",
    "                    used_indices = [idx for name, idx in collected_samples[acronym] if name == recording_name]\n",
    "                    available_indices = [idx for idx in class_info_dict[recording_name][acronym] if idx not in used_indices]\n",
    "                    \n",
    "                    for idx in available_indices:\n",
    "                        all_available.append((recording_name, idx))\n",
    "            \n",
    "            # Use extra samples to fill in\n",
    "            for i, (recording_name, window_idx) in enumerate(all_available):\n",
    "                if i >= additional_needed:\n",
    "                    break\n",
    "                    \n",
    "                # Find the bag file path for this recording\n",
    "                recording_bag_path = None\n",
    "                for bag_file_path in bag_files:\n",
    "                    if Path(bag_file_path).stem == recording_name:\n",
    "                        recording_bag_path = Path(bag_file_path)\n",
    "                        break\n",
    "                \n",
    "                if recording_bag_path is None:\n",
    "                    continue\n",
    "                \n",
    "                # Process each modality for this additional sample\n",
    "                for modality in modalities:\n",
    "                    # Get the source path\n",
    "                    windowed_data_dir = recording_bag_path.parent / \"windowed_data\" / modality\n",
    "                    window_filename = f\"{recording_name}_window_{window_idx:03d}.npy\"\n",
    "                    source_path = windowed_data_dir / window_filename\n",
    "                    \n",
    "                    # Skip if source file doesn't exist\n",
    "                    if not source_path.exists():\n",
    "                        print(f\"Warning: Additional {modality} file {source_path} not found, skipping...\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Create destination filename\n",
    "                    dest_filename = f\"{recording_name}_w{window_idx:03d}.npy\"\n",
    "                    dest_path = output_path / modality / acronym / dest_filename\n",
    "                    \n",
    "                    # Copy the file\n",
    "                    shutil.copy2(source_path, dest_path)\n",
    "                \n",
    "                # Track which samples we've collected (only once per sample)\n",
    "                collected_samples[acronym].append((recording_name, window_idx))\n",
    "                \n",
    "                print(f\"Added extra {class_full_names[acronym]} ({acronym}) sample from {recording_name}, window {window_idx} (both rgb and depth)\")\n",
    "    \n",
    "    # Final count and validation\n",
    "    for acronym in class_acronyms:\n",
    "        rgb_count = len(list((output_path / \"rgb\" / acronym).glob(\"*.npy\")))\n",
    "        depth_count = len(list((output_path / \"depth\" / acronym).glob(\"*.npy\")))\n",
    "        \n",
    "        print(f\"Final count for {class_full_names[acronym]} ({acronym}):\")\n",
    "        print(f\"  - RGB: {rgb_count} samples\")\n",
    "        print(f\"  - Depth: {depth_count} samples\")\n",
    "        \n",
    "        # Check if counts match\n",
    "        if rgb_count != depth_count:\n",
    "            print(f\"  - WARNING: RGB and depth counts don't match for {acronym}!\")\n",
    "\n",
    "# Use the provided dictionary directly\n",
    "info_dict = {\n",
    "    \"Record_20250402151124\": {\n",
    "        \"OH\": [0, 10, 11],\n",
    "        \"IH\": [1, 2],\n",
    "        \"SF\": [3, 4],\n",
    "        \"HC\": [5, 6],\n",
    "        \"HH\": [7, 8]\n",
    "    },\n",
    "    \"Record_20250402151609\": {\n",
    "        \"OH\": [0, 9],\n",
    "        \"IH\": [1, 2],\n",
    "        \"SF\": [3, 4],\n",
    "        \"HC\": [5],\n",
    "        \"HH\": [6, 7]\n",
    "    },\n",
    "    \"Record_20250402152331\": {\n",
    "        \"OH\": [0, 1, 2, 12],\n",
    "        \"IH\": [3, 4],\n",
    "        \"SF\": [5, 6],\n",
    "        \"HC\": [7],\n",
    "        \"HH\": [8, 9]\n",
    "    },\n",
    "    \"Record_20250506145704\": {\n",
    "        \"OH\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        \"IH\": [11, 12, 13, 14, 15],\n",
    "        \"SF\": [16, 17, 18, 19, 20, 21],\n",
    "        \"HC\": [22, 23, 24, 25, 26],\n",
    "        \"HH\": [27, 28, 29, 30, 31]\n",
    "    },\n",
    "    \"Record_20250506152951\": {\n",
    "        \"OH\": [0, 1, 2, 3, 4],\n",
    "        \"IH\": [5, 6, 7, 8],\n",
    "        \"SF\": [10, 11, 12, 13],\n",
    "        \"HC\": [14, 15, 16, 17],\n",
    "        \"HH\": [18, 19, 20, 21]\n",
    "    },\n",
    "    \"Record_20250506162630\": {\n",
    "        \"OH\": [0, 1, 2, 3, 4, 5],\n",
    "        \"IH\": [6, 7],\n",
    "        \"SF\": [8],\n",
    "        \"HC\": [9, 10, 11, 12],\n",
    "        \"HH\": [13, 14]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the dataset\n",
    "create_dataset(processed_data, bag_files, info_dict, output_dir=r\"./data/video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae50f0a",
   "metadata": {},
   "source": [
    "## 3. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2eaec6",
   "metadata": {},
   "source": [
    "**Create Dataset Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133b620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f81f9efa442431c8fcee6a049596043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "\n",
    "# 1. Create lists of file paths and labels\n",
    "image_rgb_dir = r'./data/video/rgb'\n",
    "filepaths, labels = [], []\n",
    "for class_name in os.listdir(image_rgb_dir):\n",
    "    class_dir = os.path.join(image_rgb_dir, class_name)\n",
    "    for npy_path in glob.glob(os.path.join(class_dir, '*.npy')):\n",
    "        filepaths.append(npy_path)\n",
    "        labels.append(class_name)\n",
    "\n",
    "# 2. Create dataset with file paths\n",
    "unique_labels = sorted(set(labels))\n",
    "pr_ds = Dataset.from_dict({\n",
    "    \"image_path\": filepaths,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# 3. Encode the label column\n",
    "pr_ds = pr_ds.class_encode_column(\"label\")\n",
    "\n",
    "# 4. Create a function to load images when needed\n",
    "def load_image(example):\n",
    "    example[\"image\"] = np.load(example[\"image_path\"])\n",
    "    return example\n",
    "\n",
    "# 5. Split the dataset\n",
    "split1 = pr_ds.train_test_split(test_size=0.25, shuffle=True, seed=42, stratify_by_column=\"label\")\n",
    "train_ds = split1[\"train\"]\n",
    "val_ds = split1[\"test\"]\n",
    "\n",
    "# 6. Build label2id / id2label\n",
    "label2id = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "id2label = {idx: lab for lab, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf7ea2",
   "metadata": {},
   "source": [
    "**Model Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1daf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoMAEConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"VideoMAEForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"decoder_hidden_size\": 384,\n",
      "  \"decoder_intermediate_size\": 1536,\n",
      "  \"decoder_num_attention_heads\": 6,\n",
      "  \"decoder_num_hidden_layers\": 4,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"HC\",\n",
      "    \"1\": \"HH\",\n",
      "    \"2\": \"IH\",\n",
      "    \"3\": \"OH\",\n",
      "    \"4\": \"SF\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"HC\": 0,\n",
      "    \"HH\": 1,\n",
      "    \"IH\": 2,\n",
      "    \"OH\": 3,\n",
      "    \"SF\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"videomae\",\n",
      "  \"norm_pix_loss\": true,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_frames\": 16,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"tubelet_size\": 2,\n",
      "  \"use_mean_pooling\": false\n",
      "}\n",
      "\n",
      "=======================================================================================================================================\n",
      "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #\n",
      "=======================================================================================================================================\n",
      "VideoMAEForVideoClassification                               [1, 16, 3, 224, 224]      [1, 5]                    --\n",
      "‚îú‚îÄVideoMAEModel: 1-1                                         [1, 16, 3, 224, 224]      [1, 1568, 768]            --\n",
      "‚îÇ    ‚îî‚îÄVideoMAEEmbeddings: 2-1                               [1, 16, 3, 224, 224]      [1, 1568, 768]            --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄVideoMAEPatchEmbeddings: 3-1                     [1, 16, 3, 224, 224]      [1, 1568, 768]            1,180,416\n",
      "‚îÇ    ‚îî‚îÄVideoMAEEncoder: 2-2                                  [1, 1568, 768]            [1, 1568, 768]            --\n",
      "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-2                                  --                        --                        85,045,248\n",
      "‚îÇ    ‚îî‚îÄLayerNorm: 2-3                                        [1, 1568, 768]            [1, 1568, 768]            1,536\n",
      "‚îú‚îÄLinear: 1-2                                                [1, 768]                  [1, 5]                    3,845\n",
      "=======================================================================================================================================\n",
      "Total params: 86,231,045\n",
      "Trainable params: 86,231,045\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.91\n",
      "=======================================================================================================================================\n",
      "Input size (MB): 9.63\n",
      "Forward/backward pass size (MB): 944.11\n",
      "Params size (MB): 259.92\n",
      "Estimated Total Size (MB): 1213.66\n",
      "=======================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEForVideoClassification, AutoImageProcessor\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "model_ckpt = 'MCG-NJU/videomae-base'\n",
    "sw_processor = AutoImageProcessor.from_pretrained(model_ckpt, use_fast=True)\n",
    "sw_model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id, \n",
    ")\n",
    "\n",
    "print(sw_model.config)\n",
    "\n",
    "print(summary(sw_model,\n",
    "        input_size=(1, 16, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        row_settings=[\"depth\"],\n",
    "        device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2203d2a",
   "metadata": {},
   "source": [
    "**Set Up Transformation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb6e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import torchvision.transforms.functional as tvf\n",
    "import einops\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalize a (T, C, H, W) tensor by per-channel mean/std,\n",
    "    treating T as the batch dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 mean: Tuple[float, float, float],\n",
    "                 std:  Tuple[float, float, float],\n",
    "                 inplace: bool = False):\n",
    "        super().__init__()\n",
    "        self.mean    = mean\n",
    "        self.std     = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (T, C, H, W); tvf.normalize expects (N, C, H, W)\n",
    "        return tvf.normalize(x, self.mean, self.std, self.inplace)\n",
    "    \n",
    "mean = sw_processor.image_mean\n",
    "std = sw_processor.image_std\n",
    "num_frames_to_samples = sw_model.config.num_frames\n",
    "height = sw_processor.size.get(\"shortest_edge\", sw_processor.size.get(\"height\"))\n",
    "width = height\n",
    "resize_to = (height, width)\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip\n",
    "\n",
    "transformation_pipeline = Compose([\n",
    "    Lambda(lambda x: x / 255.0),                          # now x is float [0,1]\n",
    "    Normalize(mean, std),                                 # per‚Äëchannel norm\n",
    "    Lambda(lambda x: F.interpolate(                             \n",
    "        x, size=resize_to, mode=\"bilinear\", align_corners=False \n",
    "    )),                                                          # resize to (H,W) = resize_to\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c2e45",
   "metadata": {},
   "source": [
    "**Apply Transformation Pipeline to Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fe7f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6653ed617e4905a6109c16a8231cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fcd95be8154e1899804d9c2b0540e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_train(batch):\n",
    "    pixel_values = []\n",
    "    for path in batch[\"image_path\"]:\n",
    "        # Load the numpy array from file - shape (T, H, W, C) = (16, 800, 1280, 3)\n",
    "        arr = np.load(path)\n",
    "        \n",
    "        # Convert numpy array to tensor\n",
    "        vid = torch.as_tensor(arr, dtype=torch.float32)\n",
    "        \n",
    "        # Rearrange from (T, H, W, C) to (T, C, H, W) using einops\n",
    "        vid = einops.rearrange(vid, 't h w c -> t c h w')\n",
    "        \n",
    "        # Apply transformations\n",
    "        vid_t = transformation_pipeline(vid)\n",
    "        pixel_values.append(vid_t)\n",
    "    \n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"] = batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "def preprocess_val(batch):\n",
    "    pixel_values = []\n",
    "    for path in batch[\"image_path\"]:\n",
    "        # Load the numpy array from file - shape (T, H, W, C) = (16, 800, 1280, 3)\n",
    "        arr = np.load(path)\n",
    "        \n",
    "        # Convert numpy array to tensor\n",
    "        vid = torch.as_tensor(arr, dtype=torch.float32)\n",
    "        \n",
    "        # Rearrange from (T, H, W, C) to (T, C, H, W) using einops\n",
    "        vid = einops.rearrange(vid, 't h w c -> t c h w')\n",
    "        \n",
    "        # Apply transformations\n",
    "        vid_t = transformation_pipeline(vid)\n",
    "        pixel_values.append(vid_t)\n",
    "    \n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"] = batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing to training dataset\n",
    "train_ds = train_ds.map(\n",
    "    preprocess_train, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=[\"image_path\", \"label\"]\n",
    ")\n",
    "train_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "\n",
    "# Apply preprocessing to validation dataset\n",
    "val_ds = val_ds.map(\n",
    "    preprocess_val, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=[\"image_path\", \"label\"]\n",
    ")\n",
    "val_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d7099",
   "metadata": {},
   "source": [
    "## 4. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb00de1",
   "metadata": {},
   "source": [
    "**Setup `TrainingArguments`, `Trainer`, and `evaluate`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed02005d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa67c80492f4c978f0b75521d0ae49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaacda7a9cf437482e156c9decfbe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24ec6291d444b8380b52306b47154bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c558282c474a129fdca0e82d6e630f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fa358e62af422fba29ebca4a8961ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO X1E\\AppData\\Local\\Temp\\ipykernel_26412\\1492750814.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainerCallback, TrainingArguments\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-psw105-train-aug\"\n",
    "num_train_epochs = 50\n",
    "EXPERIMENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "SAVE_DIR = f\"experiments/video/{EXPERIMENT_DATE}/{new_model_name}\"\n",
    "batch_size = 8\n",
    "\n",
    "args_pr = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    remove_unused_columns=False, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "confusion = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # scalar metrics as before‚Ä¶\n",
    "    acc   = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec  = precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "    rec   = recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1sc  = f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "    # get confusion matrix and turn it into a nested Python list\n",
    "    cm = confusion.compute(predictions=preds, references=labels)[\"confusion_matrix\"]\n",
    "    cm_list = cm.tolist()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":          acc,\n",
    "        \"precision\":         prec,\n",
    "        \"recall\":            rec,\n",
    "        \"f1\":                f1sc,\n",
    "        \"confusion_matrix\":  cm_list,    # now JSON‚Äëserializable\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # these will accumulate _all_ batches in the current epoch\n",
    "        self.epoch_losses     = []\n",
    "        self.epoch_preds      = []\n",
    "        self.epoch_labels     = []\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides Trainer.compute_loss to store batch‚Äêlevel loss & preds/labels.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\", None)\n",
    "        outputs = model(**inputs)\n",
    "        loss   = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if labels is not None:\n",
    "            # 1) store the loss\n",
    "            self.epoch_losses.append(loss.item())\n",
    "            # 2) store predictions + labels as 1D arrays\n",
    "            preds = logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labs  = labels.detach().cpu().numpy()\n",
    "            self.epoch_preds .extend(preds.tolist())\n",
    "            self.epoch_labels.extend(labs.tolist())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self, trainer):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "\n",
    "        # lists to hold epoch‚Äêby‚Äêepoch values\n",
    "        self.train_losses     = []\n",
    "        self.train_accuracies = []\n",
    "        self.eval_losses      = []\n",
    "        self.eval_accuracies  = []\n",
    "        self.eval_confusion_matrices = [] \n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Compute average training loss & accuracy for the epoch\n",
    "        t = self.trainer\n",
    "        avg_loss = float(np.mean(t.epoch_losses))\n",
    "        acc      = np.mean(\n",
    "            np.array(t.epoch_preds) == np.array(t.epoch_labels)\n",
    "        )\n",
    "\n",
    "        # Store & clear for next epoch\n",
    "        self.train_losses    .append(avg_loss)\n",
    "        self.train_accuracies.append(acc)\n",
    "        t.epoch_losses .clear()\n",
    "        t.epoch_preds  .clear()\n",
    "        t.epoch_labels .clear()\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # metrics come prefixed with \"eval_\"\n",
    "        self.eval_losses   .append(metrics[\"eval_loss\"])\n",
    "        self.eval_accuracies.append(metrics[\"eval_accuracy\"])\n",
    "        self.eval_confusion_matrices.append(metrics[\"eval_confusion_matrix\"])\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=copy.deepcopy(sw_model),                 \n",
    "    args=args_pr,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=sw_processor,          \n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "\n",
    "metrics_cb = MetricsCallback(trainer)\n",
    "trainer.add_callback(metrics_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226da2b0",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46644249",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcea2db",
   "metadata": {},
   "source": [
    "**Training, Validation History Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_val_ds = {\n",
    "    \"train_loss\":     metrics_cb.train_losses,\n",
    "    \"train_accuracy\": metrics_cb.train_accuracies,\n",
    "    \"eval_loss\":      metrics_cb.eval_losses,\n",
    "    \"eval_accuracy\":  metrics_cb.eval_accuracies,\n",
    "    \"eval_confusion_matrix\": metrics_cb.eval_confusion_matrices,\n",
    "}\n",
    "history_train_val_ds_path = os.path.join(SAVE_DIR, \"history_trainval.pkl\")\n",
    "with open(history_train_val_ds_path, \"wb\") as f:\n",
    "    pickle.dump(history_val_ds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fb4a9",
   "metadata": {},
   "source": [
    "Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
