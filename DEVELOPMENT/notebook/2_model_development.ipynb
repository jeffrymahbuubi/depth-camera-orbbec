{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posture Recognition and Finger Sliding Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [Batch and Epoch training metrics for transformers Trainer](https://stackoverflow.com/questions/78311534/batch-and-epoch-training-metrics-for-transformers-trainer)\n",
    " - [Huggingface Trainer(): K-Fold Cross Validation](https://stackoverflow.com/questions/75510487/huggingface-trainer-k-fold-cross-validation)\n",
    " - [Metrics for Training Set in Trainer](https://discuss.huggingface.co/t/metrics-for-training-set-in-trainer/2461)\n",
    " - [Huggingface: Datasets](https://huggingface.co/docs/datasets/v2.12.0/en/installation)\n",
    " - [Hugginface: Transformers - Trainer API](https://huggingface.co/docs/transformers/trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import einops\n",
    "import os\n",
    "import datasets\n",
    "import seedir as sd\n",
    "import glob\n",
    "import albumentations as A\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posture Recognition (Image Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Image\n",
    "\n",
    "# 1) Load filepaths and labels (you already have this)\n",
    "image_rgb_dir = r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\DATA\\20250402\\image\\rgb'\n",
    "\n",
    "filepaths, labels = [], []\n",
    "for class_name in os.listdir(image_rgb_dir):\n",
    "    class_dir = os.path.join(image_rgb_dir, class_name)\n",
    "    for npy_path in glob.glob(os.path.join(class_dir, '*.npy')):\n",
    "        filepaths.append(npy_path)\n",
    "        labels.append(class_name)\n",
    "\n",
    "# 2) Create Hugging‑Face Dataset\n",
    "arrays = [np.load(p) for p in filepaths]\n",
    "pr_ds = Dataset.from_dict({\n",
    "    \"image\": arrays,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "pr_ds = pr_ds.class_encode_column(\"label\")\n",
    "\n",
    "# 1) initial 80/20 split\n",
    "split1 = pr_ds.train_test_split(test_size=0.2, shuffle=True, seed=42, stratify_by_column=\"label\")\n",
    "train_ds = split1[\"train\"]        # 80%\n",
    "temp_ds  = split1[\"test\"]         # 20%\n",
    "\n",
    "# 2) split that 20% into half validation, half test → 10/10\n",
    "split2 = temp_ds.train_test_split(test_size=0.5, shuffle=True, seed=42, stratify_by_column=\"label\")\n",
    "val_ds  = split2[\"train\"]         # 10%\n",
    "test_ds = split2[\"test\"]          # 10%\n",
    "\n",
    "# 4) Build label2id / id2label\n",
    "unique_labels = sorted(set(labels))\n",
    "label2id = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "id2label = {idx: lab for lab, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finger Sliding Workspace (Video Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Original Data**\n",
    "\n",
    "- Not yet split into `train` and `val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_rgb_dir = r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\DATA\\20250402\\video\\rgb'\n",
    "\n",
    "filepaths, labels = [], []\n",
    "for class_name in os.listdir(video_rgb_dir):\n",
    "    class_dir = os.path.join(video_rgb_dir, class_name)\n",
    "    for npy_path in glob.glob(os.path.join(class_dir, '*.npy')):\n",
    "        filepaths.append(npy_path)\n",
    "        labels.append(class_name)\n",
    "\n",
    "arrays = [np.load(p) for p in filepaths]\n",
    "sw_ds = Dataset.from_dict({\n",
    "    \"video\": arrays,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "sw_ds = sw_ds.class_encode_column(\"label\")\n",
    "\n",
    "split1 = sw_ds.train_test_split(test_size=0.33, shuffle=True, seed=42, stratify_by_column=\"label\")\n",
    "train_ds = split1[\"train\"]       \n",
    "val_ds  = split1[\"test\"]\n",
    "\n",
    "unique_labels = sorted(set(labels))\n",
    "label2id = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "id2label = {idx: lab for lab, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Augmented Data**\n",
    "\n",
    "- Already split into `train` and `val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1) Base directories\n",
    "base_dir     = os.path.join(\n",
    "    r\"D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\DATA\\20250402\\video\",\n",
    "    \"split_rgb\"\n",
    ")\n",
    "train_dirs   = [os.path.join(base_dir, \"train_aug\")]\n",
    "val_dir      = os.path.join(base_dir, \"val\")\n",
    "\n",
    "# 2) Collect train file paths & labels\n",
    "train_paths, train_labels = [], []\n",
    "for d in train_dirs:\n",
    "    for class_name in os.listdir(d):\n",
    "        class_dir = os.path.join(d, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for npy_path in glob.glob(os.path.join(class_dir, \"*.npy\")):\n",
    "            train_paths.append(npy_path)\n",
    "            train_labels.append(class_name)\n",
    "\n",
    "# 3) Collect val file paths & labels\n",
    "val_paths, val_labels = [], []\n",
    "for class_name in os.listdir(val_dir):\n",
    "    class_dir = os.path.join(val_dir, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    for npy_path in glob.glob(os.path.join(class_dir, \"*.npy\")):\n",
    "        val_paths.append(npy_path)\n",
    "        val_labels.append(class_name)\n",
    "\n",
    "# 4) Load numpy arrays into memory\n",
    "train_arrays = [np.load(p) for p in train_paths]\n",
    "val_arrays   = [np.load(p) for p in val_paths]\n",
    "\n",
    "# 5) Build HuggingFace Datasets\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"video\": train_arrays,   # shape (T, C, H, W)\n",
    "    \"label\": train_labels\n",
    "})\n",
    "val_ds   = Dataset.from_dict({\n",
    "    \"video\": val_arrays,\n",
    "    \"label\": val_labels\n",
    "})\n",
    "\n",
    "# 6) Encode string labels into ClassLabel (int)\n",
    "train_ds = train_ds.class_encode_column(\"label\")\n",
    "val_ds   = val_ds.class_encode_column(\"label\")\n",
    "\n",
    "unique_labels = sorted(set(train_labels))\n",
    "label2id = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "id2label = {idx: lab for lab, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel, ViTForImageClassification\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "pr_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "pr_model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k',\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(pr_model.config)\n",
    "\n",
    "# Create dummy input (batch_size=1, channels=3, height=224, width=224)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Send model to the same device as input\n",
    "pr_model_eval = copy.deepcopy(pr_model)\n",
    "pr_model_eval.eval()\n",
    "\n",
    "# Print model summary\n",
    "summary(pr_model_eval, input_data=dummy_input, depth=3, col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEForVideoClassification, AutoImageProcessor\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "model_ckpt = 'MCG-NJU/videomae-base'\n",
    "sw_processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "sw_model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id, \n",
    ")\n",
    "\n",
    "print(sw_model.config)\n",
    "\n",
    "print(summary(sw_model,\n",
    "        input_size=(1, 16, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        row_settings=[\"depth\"],\n",
    "        device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up Transformation Pipeline with Albumentations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = pr_processor.image_mean\n",
    "std = pr_processor.image_std\n",
    "size = pr_processor.size\n",
    "h, w = pr_processor.size[\"height\"], pr_processor.size[\"width\"]\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(h, w),\n",
    "\n",
    "    # pick exactly one of these (p=1 ensures one op always applied)\n",
    "    A.OneOf([\n",
    "        A.NoOp(),\n",
    "\n",
    "        A.Affine(shear=25, p=1.0),        # ShearX/Y combined\n",
    "        A.Affine(translate_percent={\"x\":0.2,\"y\":0}, p=1.0),  # TranslateX\n",
    "        A.Affine(translate_percent={\"x\":0,\"y\":0.2}, p=1.0),  # TranslateY\n",
    "        A.Rotate(limit=45, p=1.0),\n",
    "\n",
    "        A.Sharpen(p=1.0),\n",
    "\n",
    "        A.Posterize(num_bits=4, p=1.0),\n",
    "        A.Solarize(threshold=128, p=1.0),\n",
    "        A.Equalize(p=1.0),\n",
    "    ], p=1.0),\n",
    "\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    A.ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(h, w),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    A.ToTensorV2()\n",
    "])\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_grid_transformed(ds, transform, title):\n",
    "    idxs = random.sample(range(len(ds)), k=min(9, len(ds)))\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for ax, idx in zip(axes.ravel(), idxs):\n",
    "        item = ds[idx]\n",
    "        img = np.array(item[\"image\"]).astype(np.uint8)\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        # Apply the Albumentations transform\n",
    "        transformed = transform(image=img)[\"image\"]\n",
    "\n",
    "        # If normalized, unnormalize for display\n",
    "        if transformed.max() <= 2 and transformed.min() >= -2:  # rough check\n",
    "            transformed = transformed.permute(1, 2, 0).numpy()\n",
    "            transformed = transformed * std + mean\n",
    "            transformed = np.clip(transformed, 0, 1)\n",
    "        else:\n",
    "            transformed = transformed.transpose(1, 2, 0)\n",
    "\n",
    "        ax.imshow(transformed)\n",
    "        ax.set_title(label)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "show_grid_transformed(train_ds, train_transform, \"Transformed Train Samples\")\n",
    "show_grid_transformed(val_ds, val_transform,  \"Transformed Val Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the Transformation Pipeline to Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(batch):\n",
    "    imgs = []\n",
    "    for img in batch[\"image\"]:\n",
    "        arr = np.array(img, copy=False)\n",
    "        if arr.dtype != np.uint8:\n",
    "            arr = arr.astype(np.uint8)\n",
    "        imgs.append(train_transform(image=arr)[\"image\"])\n",
    "\n",
    "    batch[\"pixel_values\"] = imgs\n",
    "    batch[\"labels\"]       = batch[\"label\"]  # already ints\n",
    "    return batch\n",
    "\n",
    "def preprocess_val(batch):\n",
    "    imgs = []\n",
    "    for img in batch[\"image\"]:\n",
    "        arr = np.array(img, copy=False)\n",
    "        if arr.dtype != np.uint8:\n",
    "            arr = arr.astype(np.uint8)\n",
    "        imgs.append(val_transform(image=arr)[\"image\"])\n",
    "\n",
    "    batch[\"pixel_values\"] = imgs\n",
    "    batch[\"labels\"]       = batch[\"label\"]  # already ints\n",
    "    return batch\n",
    "\n",
    "# map & reassign train_ds\n",
    "train_ds = train_ds.map(\n",
    "    preprocess_train,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=[\"image\", \"label\"]\n",
    ")\n",
    "train_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "\n",
    "# map & reassign val_ds\n",
    "val_ds = val_ds.map(\n",
    "    preprocess_val,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=[\"image\", \"label\"]\n",
    ")\n",
    "val_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "\n",
    "# map & reassign test_ds\n",
    "test_ds = test_ds.map(\n",
    "    preprocess_val,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=[\"image\", \"label\"]\n",
    ")\n",
    "test_ds.set_format(type=\"torch\", columns=[\"pixel_values\",\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up Transformation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import torchvision.transforms.functional as tvf\n",
    "import random\n",
    "\n",
    "class RandomShortSideScale(nn.Module):\n",
    "    \"\"\"\n",
    "    Randomly pick a short‐side target in [min_size, max_size] and scale accordingly.\n",
    "    Input: (T, C, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, min_size: int, max_size: int, interpolation: str = \"bilinear\"):\n",
    "        super().__init__()\n",
    "        self.min_size     = min_size\n",
    "        self.max_size     = max_size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (T, C, H, W)\n",
    "        t, c, h, w = x.shape\n",
    "        size = random.randint(self.min_size, self.max_size)\n",
    "        if w < h:\n",
    "            new_h = int(math.floor((h / w) * size))\n",
    "            new_w = size\n",
    "        else:\n",
    "            new_h = size\n",
    "            new_w = int(math.floor((w / h) * size))\n",
    "        # F.interpolate treats first dim (T) as batch:\n",
    "        return F.interpolate(x, size=(new_h, new_w), \n",
    "                             mode=self.interpolation, align_corners=False)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalize a (T, C, H, W) tensor by per-channel mean/std,\n",
    "    treating T as the batch dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 mean: Tuple[float, float, float],\n",
    "                 std:  Tuple[float, float, float],\n",
    "                 inplace: bool = False):\n",
    "        super().__init__()\n",
    "        self.mean    = mean\n",
    "        self.std     = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is (T, C, H, W); tvf.normalize expects (N, C, H, W)\n",
    "        return tvf.normalize(x, self.mean, self.std, self.inplace)\n",
    "\n",
    "class UniformTemporalSubsample(nn.Module):\n",
    "    \"\"\"\n",
    "    Uniformly subsample `num_samples` frames from a (T, C, H, W) tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples: int):\n",
    "        super().__init__()\n",
    "        if num_samples <= 0:\n",
    "            raise ValueError(f\"num_samples must be > 0, got {num_samples}\")\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (T, C, H, W)\n",
    "        if x.ndim != 4:\n",
    "            return x\n",
    "        t = x.shape[0]\n",
    "        if t == 0:\n",
    "            return x\n",
    "        idx = torch.linspace(0, t - 1, self.num_samples, \n",
    "                             device=x.device).round().long()\n",
    "        return x.index_select(dim=0, index=idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sw_processor.image_mean\n",
    "std = sw_processor.image_std\n",
    "num_frames_to_samples = sw_model.config.num_frames\n",
    "height = sw_processor.size.get(\"shortest_edge\", sw_processor.size.get(\"height\"))\n",
    "width = height\n",
    "resize_to = (height, width)\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, RandomCrop, RandomHorizontalFlip\n",
    "\n",
    "train_transform = Compose([\n",
    "    UniformTemporalSubsample(num_frames_to_samples),      # → (T,C,H,W)\n",
    "    # RandomShortSideScale(256,320),                        # spatial scale on raw pixels\n",
    "    # RandomCrop(resize_to),                                # fixed 224×224 crop\n",
    "    # RandomHorizontalFlip(0.5),                            # augment\n",
    "    Lambda(lambda x: x / 255.0),                          # now x is float [0,1]\n",
    "    Normalize(mean, std),                                 # per‑channel norm\n",
    "    Lambda(lambda x: F.interpolate(                             \n",
    "        x, size=resize_to, mode=\"bilinear\", align_corners=False \n",
    "    )),                                                          # resize to (H,W) = resize_to\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    UniformTemporalSubsample(num_frames_to_samples),            # → (T, C, H, W)\n",
    "    Lambda(lambda x: x / 255.0),                                # to [0,1]\n",
    "    Normalize(mean, std),                                       # per‑channel norm\n",
    "    Lambda(lambda x: F.interpolate(                             \n",
    "        x, size=resize_to, mode=\"bilinear\", align_corners=False \n",
    "    )),                                                          # resize to (H,W) = resize_to\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizaing Transformation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "\n",
    "def unnormalize_img(img: np.ndarray) -> np.ndarray:\n",
    "    # img is (H, W, C) in [0,1]\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor: torch.Tensor, filename=\"sample.gif\") -> str:\n",
    "    \"\"\"\n",
    "    video_tensor: (T, C, H, W)\n",
    "    \"\"\"\n",
    "    T, C, H, W = video_tensor.shape\n",
    "    frames = []\n",
    "    for t in range(T):\n",
    "        frame = video_tensor[t]                     # (C, H, W)\n",
    "        arr = frame.permute(1, 2, 0).cpu().numpy()  # (H, W, C)\n",
    "        arr = unnormalize_img(arr)\n",
    "        frames.append(arr)\n",
    "    imageio.mimsave(filename, frames, duration=0.25)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor: torch.Tensor, gif_name=\"sample.gif\") -> Image:\n",
    "    \"\"\"\n",
    "    video_tensor: (T, C, H, W)\n",
    "    \"\"\"\n",
    "    gif_path = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_path)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Example usage on val_ds:\n",
    "\n",
    "idx     = 0\n",
    "example = train_ds[idx]\n",
    "\n",
    "# extract & squeeze:\n",
    "arr = np.array(example[\"video\"], dtype=np.float32)\n",
    "if arr.ndim == 5 and arr.shape[0] == 1:\n",
    "    arr = arr.squeeze(0)         # now (T, C, H, W)\n",
    "\n",
    "vid   = torch.from_numpy(arr).float()      # (T, C, H, W)\n",
    "vid_t = train_transform(vid)                 # still (T, C, H, W)\n",
    "display_gif(vid_t, gif_name=\"val_sample.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the Transformation Pipeline to Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def preprocess_train(batch):\n",
    "    pixel_values = []\n",
    "    for arr in batch[\"video\"]:\n",
    "        # arr: numpy (T,C,H,W)\n",
    "        vid  = torch.as_tensor(arr, dtype=torch.float32) \n",
    "        vid_t = train_transform(vid)               # now a (T,C,H,W) Tensor\n",
    "        pixel_values.append(vid_t)\n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"]       =  batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "def preprocess_val(batch):\n",
    "    pixel_values = []\n",
    "    for arr in batch[\"video\"]:\n",
    "        vid   = torch.as_tensor(arr, dtype=torch.float32) \n",
    "        vid_t = val_transform(vid)\n",
    "        pixel_values.append(vid_t)\n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"]       = batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(preprocess_train, batched=True, batch_size=8,\n",
    "                        remove_columns=[\"video\",\"label\"])\n",
    "train_ds.set_format(type=\"torch\", columns=[\"pixel_values\",\"labels\"])\n",
    "\n",
    "val_ds = val_ds.map(preprocess_val, batched=True, batch_size=8,\n",
    "                    remove_columns=[\"video\",\"label\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"pixel_values\",\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup `TrainingArguments`, `Trainer`, and `evaluate`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback, TrainingArguments\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "model_ckpt = 'google/vit-base-patch16-224-in21k'\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-pr240-stratify\"\n",
    "num_train_epochs = 50\n",
    "EXPERIMENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "SAVE_DIR = f\"experiments/image/{EXPERIMENT_DATE}/{new_model_name}\"\n",
    "batch_size = 8\n",
    "\n",
    "args_pr = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    remove_unused_columns=False, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "confusion = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # scalar metrics as before…\n",
    "    acc   = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec  = precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "    rec   = recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1sc  = f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "    # get confusion matrix and turn it into a nested Python list\n",
    "    cm = confusion.compute(predictions=preds, references=labels)[\"confusion_matrix\"]\n",
    "    cm_list = cm.tolist()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":          acc,\n",
    "        \"precision\":         prec,\n",
    "        \"recall\":            rec,\n",
    "        \"f1\":                f1sc,\n",
    "        \"confusion_matrix\":  cm_list,    # now JSON‑serializable\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # these will accumulate _all_ batches in the current epoch\n",
    "        self.epoch_losses     = []\n",
    "        self.epoch_preds      = []\n",
    "        self.epoch_labels     = []\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides Trainer.compute_loss to store batch‐level loss & preds/labels.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\", None)\n",
    "        outputs = model(**inputs)\n",
    "        loss   = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if labels is not None:\n",
    "            # 1) store the loss\n",
    "            self.epoch_losses.append(loss.item())\n",
    "            # 2) store predictions + labels as 1D arrays\n",
    "            preds = logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labs  = labels.detach().cpu().numpy()\n",
    "            self.epoch_preds .extend(preds.tolist())\n",
    "            self.epoch_labels.extend(labs.tolist())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self, trainer):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "\n",
    "        # lists to hold epoch‐by‐epoch values\n",
    "        self.train_losses     = []\n",
    "        self.train_accuracies = []\n",
    "        self.eval_losses      = []\n",
    "        self.eval_accuracies  = []\n",
    "        self.eval_confusion_matrices = [] \n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Compute average training loss & accuracy for the epoch\n",
    "        t = self.trainer\n",
    "        avg_loss = float(np.mean(t.epoch_losses))\n",
    "        acc      = np.mean(\n",
    "            np.array(t.epoch_preds) == np.array(t.epoch_labels)\n",
    "        )\n",
    "\n",
    "        # Store & clear for next epoch\n",
    "        self.train_losses    .append(avg_loss)\n",
    "        self.train_accuracies.append(acc)\n",
    "        t.epoch_losses .clear()\n",
    "        t.epoch_preds  .clear()\n",
    "        t.epoch_labels .clear()\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # metrics come prefixed with \"eval_\"\n",
    "        self.eval_losses   .append(metrics[\"eval_loss\"])\n",
    "        self.eval_accuracies.append(metrics[\"eval_accuracy\"])\n",
    "        # record the confusion matrix\n",
    "        self.eval_confusion_matrices.append(metrics[\"eval_confusion_matrix\"])\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=copy.deepcopy(pr_model),                 \n",
    "    args=args_pr,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=pr_processor,          \n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "\n",
    "metrics_cb = MetricsCallback(trainer)\n",
    "trainer.add_callback(metrics_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing History Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_test_ds = trainer.evaluate(test_ds)\n",
    "history_test_ds = {\n",
    "    'eval_accuracy': history_test_ds['eval_accuracy'],\n",
    "    'eval_preicision': history_test_ds['eval_precision'],\n",
    "    'eval_recall': history_test_ds['eval_recall'],\n",
    "    'eval_f1': history_test_ds['eval_f1'],\n",
    "    'eval_confusion_matrix': history_test_ds['eval_confusion_matrix'],\n",
    "}\n",
    "history_test_ds_path = os.path.join(SAVE_DIR, \"history_testing.pkl\")\n",
    "with open(history_test_ds_path, \"wb\") as f:\n",
    "    pickle.dump(history_test_ds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training, Validation History Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_val_ds = {\n",
    "    \"train_loss\":     metrics_cb.train_losses,\n",
    "    \"train_accuracy\": metrics_cb.train_accuracies,\n",
    "    \"eval_loss\":      metrics_cb.eval_losses,\n",
    "    \"eval_accuracy\":  metrics_cb.eval_accuracies,\n",
    "    \"eval_confusion_matrix\": metrics_cb.eval_confusion_matrices,\n",
    "}\n",
    "history_train_val_ds_path = os.path.join(SAVE_DIR, \"history_trainval.pkl\")\n",
    "with open(history_train_val_ds_path, \"wb\") as f:\n",
    "    pickle.dump(history_val_ds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_val_losses(save_dir: str) -> None:\n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    import glob\n",
    "    import os\n",
    "\n",
    "    # Set global font properties\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 15\n",
    "\n",
    "    # Search for history pickle files\n",
    "    history_files = glob.glob(os.path.join(save_dir, \"*.pkl\"))\n",
    "\n",
    "    for history_path in history_files:\n",
    "        filename = os.path.basename(history_path)\n",
    "\n",
    "        if \"trainval\" not in filename:\n",
    "            continue  # skip non-trainval files\n",
    "\n",
    "        print(f\"Visualizing: {filename}\")\n",
    "        with open(history_path, \"rb\") as f:\n",
    "            hist = pickle.load(f)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(15, 15))\n",
    "\n",
    "        # Plot loss\n",
    "        axs[0].plot(hist[\"train_loss\"], label=\"Train Loss\", color='red')\n",
    "        axs[0].plot(hist[\"eval_loss\"], label=\"Validation Loss\", color='blue')\n",
    "        axs[0].set_title('Posture Recognition Training Loss', fontweight='bold')\n",
    "        axs[0].set_ylabel('Loss')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot accuracy\n",
    "        axs[1].plot(hist[\"train_accuracy\"], label=\"Train Accuracy\", color='orange')\n",
    "        axs[1].plot(hist[\"eval_accuracy\"], label=\"Validation Accuracy\", color='green')\n",
    "        axs[1].set_title('Posture Recognition Training Accuracy', fontweight='bold')\n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_ylabel('Accuracy')\n",
    "        axs[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def visualize_confusion_matrix(save_dir: str, mode: str = \"trainval\") -> None:\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    import glob\n",
    "    import os\n",
    "\n",
    "    # Look for the correct .pkl file based on mode\n",
    "    history_files = glob.glob(os.path.join(save_dir, \"*.pkl\"))\n",
    "    matched_file = None\n",
    "\n",
    "    for file in history_files:\n",
    "        if mode.lower() in os.path.basename(file).lower():\n",
    "            matched_file = file\n",
    "            break\n",
    "\n",
    "    if not matched_file:\n",
    "        print(f\"[ERROR] No history file found in '{save_dir}' for mode '{mode}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Loading confusion matrix from: {matched_file}\")\n",
    "    with open(matched_file, \"rb\") as f:\n",
    "        hist = pickle.load(f)\n",
    "\n",
    "    # Decide whether to use the last epoch or a single matrix\n",
    "    if mode == \"trainval\":\n",
    "        cm_data = hist[\"eval_confusion_matrix\"][-1]\n",
    "        title = \"Confusion Matrix (Last Epoch - Train/Val)\"\n",
    "    else:\n",
    "        cm_data = hist[\"eval_confusion_matrix\"]\n",
    "        title = f\"Confusion Matrix ({mode.capitalize()})\"\n",
    "\n",
    "    cm = np.array(cm_data)\n",
    "    class_names = [\n",
    "        \"Hand Open\",\n",
    "        \"Hand Close\",\n",
    "        \"Hook Hand\",\n",
    "        \"Intrinsic Plus\",\n",
    "        \"Straight Fist\"\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    ax.set_title(title, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_train_val_losses(SAVE_DIR)\n",
    "visualize_confusion_matrix(SAVE_DIR, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup `TrainingArguments`, `Trainer`, and `evaluate`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback, TrainingArguments\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-psw105-train-aug\"\n",
    "num_train_epochs = 50\n",
    "EXPERIMENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "SAVE_DIR = f\"experiments/video/{EXPERIMENT_DATE}/{new_model_name}\"\n",
    "batch_size = 8\n",
    "\n",
    "args_pr = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    remove_unused_columns=False, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"best\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "confusion = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # scalar metrics as before…\n",
    "    acc   = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec  = precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "    rec   = recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1sc  = f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "    # get confusion matrix and turn it into a nested Python list\n",
    "    cm = confusion.compute(predictions=preds, references=labels)[\"confusion_matrix\"]\n",
    "    cm_list = cm.tolist()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":          acc,\n",
    "        \"precision\":         prec,\n",
    "        \"recall\":            rec,\n",
    "        \"f1\":                f1sc,\n",
    "        \"confusion_matrix\":  cm_list,    # now JSON‑serializable\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # these will accumulate _all_ batches in the current epoch\n",
    "        self.epoch_losses     = []\n",
    "        self.epoch_preds      = []\n",
    "        self.epoch_labels     = []\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Overrides Trainer.compute_loss to store batch‐level loss & preds/labels.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\", None)\n",
    "        outputs = model(**inputs)\n",
    "        loss   = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if labels is not None:\n",
    "            # 1) store the loss\n",
    "            self.epoch_losses.append(loss.item())\n",
    "            # 2) store predictions + labels as 1D arrays\n",
    "            preds = logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labs  = labels.detach().cpu().numpy()\n",
    "            self.epoch_preds .extend(preds.tolist())\n",
    "            self.epoch_labels.extend(labs.tolist())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self, trainer):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "\n",
    "        # lists to hold epoch‐by‐epoch values\n",
    "        self.train_losses     = []\n",
    "        self.train_accuracies = []\n",
    "        self.eval_losses      = []\n",
    "        self.eval_accuracies  = []\n",
    "        self.eval_confusion_matrices = [] \n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Compute average training loss & accuracy for the epoch\n",
    "        t = self.trainer\n",
    "        avg_loss = float(np.mean(t.epoch_losses))\n",
    "        acc      = np.mean(\n",
    "            np.array(t.epoch_preds) == np.array(t.epoch_labels)\n",
    "        )\n",
    "\n",
    "        # Store & clear for next epoch\n",
    "        self.train_losses    .append(avg_loss)\n",
    "        self.train_accuracies.append(acc)\n",
    "        t.epoch_losses .clear()\n",
    "        t.epoch_preds  .clear()\n",
    "        t.epoch_labels .clear()\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # metrics come prefixed with \"eval_\"\n",
    "        self.eval_losses   .append(metrics[\"eval_loss\"])\n",
    "        self.eval_accuracies.append(metrics[\"eval_accuracy\"])\n",
    "        self.eval_confusion_matrices.append(metrics[\"eval_confusion_matrix\"])\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=copy.deepcopy(sw_model),                 \n",
    "    args=args_pr,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=sw_processor,          \n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "\n",
    "metrics_cb = MetricsCallback(trainer)\n",
    "trainer.add_callback(metrics_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training, Validation History Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_val_ds = {\n",
    "    \"train_loss\":     metrics_cb.train_losses,\n",
    "    \"train_accuracy\": metrics_cb.train_accuracies,\n",
    "    \"eval_loss\":      metrics_cb.eval_losses,\n",
    "    \"eval_accuracy\":  metrics_cb.eval_accuracies,\n",
    "    \"eval_confusion_matrix\": metrics_cb.eval_confusion_matrices,\n",
    "}\n",
    "history_train_val_ds_path = os.path.join(SAVE_DIR, \"history_trainval.pkl\")\n",
    "with open(history_train_val_ds_path, \"wb\") as f:\n",
    "    pickle.dump(history_val_ds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_val_losses(save_dir: str) -> None:\n",
    "    import pickle\n",
    "    import matplotlib.pyplot as plt\n",
    "    import glob\n",
    "    import os\n",
    "\n",
    "    # Set global font properties\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 15\n",
    "\n",
    "    # Search for history pickle files\n",
    "    history_files = glob.glob(os.path.join(save_dir, \"*.pkl\"))\n",
    "\n",
    "    for history_path in history_files:\n",
    "        filename = os.path.basename(history_path)\n",
    "\n",
    "        if \"trainval\" not in filename:\n",
    "            continue  # skip non-trainval files\n",
    "\n",
    "        print(f\"Visualizing: {filename}\")\n",
    "        with open(history_path, \"rb\") as f:\n",
    "            hist = pickle.load(f)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(15, 15))\n",
    "\n",
    "        # Plot loss\n",
    "        axs[0].plot(hist[\"train_loss\"], label=\"Train Loss\", color='red')\n",
    "        axs[0].plot(hist[\"eval_loss\"], label=\"Validation Loss\", color='blue')\n",
    "        axs[0].set_title('Posture Recognition Training Loss', fontweight='bold')\n",
    "        axs[0].set_ylabel('Loss')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot accuracy\n",
    "        axs[1].plot(hist[\"train_accuracy\"], label=\"Train Accuracy\", color='orange')\n",
    "        axs[1].plot(hist[\"eval_accuracy\"], label=\"Validation Accuracy\", color='green')\n",
    "        axs[1].set_title('Posture Recognition Training Accuracy', fontweight='bold')\n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_ylabel('Accuracy')\n",
    "        axs[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def visualize_confusion_matrix(save_dir: str, mode: str = \"trainval\") -> None:\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    import glob\n",
    "    import os\n",
    "\n",
    "    # Look for the correct .pkl file based on mode\n",
    "    history_files = glob.glob(os.path.join(save_dir, \"*.pkl\"))\n",
    "    matched_file = None\n",
    "\n",
    "    for file in history_files:\n",
    "        if mode.lower() in os.path.basename(file).lower():\n",
    "            matched_file = file\n",
    "            break\n",
    "\n",
    "    if not matched_file:\n",
    "        print(f\"[ERROR] No history file found in '{save_dir}' for mode '{mode}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Loading confusion matrix from: {matched_file}\")\n",
    "    with open(matched_file, \"rb\") as f:\n",
    "        hist = pickle.load(f)\n",
    "\n",
    "    # Decide whether to use the last epoch or a single matrix\n",
    "    if mode == \"trainval\":\n",
    "        cm_data = hist[\"eval_confusion_matrix\"][-1]\n",
    "        title = \"Confusion Matrix (Last Epoch - Train/Val)\"\n",
    "    else:\n",
    "        cm_data = hist[\"eval_confusion_matrix\"]\n",
    "        title = f\"Confusion Matrix ({mode.capitalize()})\"\n",
    "\n",
    "    cm = np.array(cm_data)\n",
    "    class_names = [\n",
    "        \"Hand Open\",\n",
    "        \"Hand Close\",\n",
    "        \"Hook Hand\",\n",
    "        \"Intrinsic Plus\",\n",
    "        \"Straight Fist\"\n",
    "    ]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    ax.set_title(title, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_train_val_losses(SAVE_DIR)\n",
    "visualize_confusion_matrix(SAVE_DIR, mode=\"trainval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inferring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Load model and processor\n",
    "ckpt_dir = r'D:\\RESEARCH ASSISTANT\\6. Depth Camera\\CODE\\Orbbec Gemini 2XL\\REMOTE\\DEVELOPMENT\\notebook\\experiments\\image\\20250419\\vit-base-patch16-224-in21k-finetuned-pr240\\checkpoint-96'\n",
    "model = ViTForImageClassification.from_pretrained(ckpt_dir)\n",
    "processor = ViTImageProcessor.from_pretrained(ckpt_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Prepare mean/std for un-normalization\n",
    "mean = torch.tensor(processor.image_mean).view(1, 3, 1, 1).to(device)\n",
    "std = torch.tensor(processor.image_std).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "# Sample 9 examples from test_ds\n",
    "idxs = random.sample(range(len(test_ds)), k=min(9, len(test_ds)))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "for ax, idx in zip(axes.ravel(), idxs):\n",
    "    item = test_ds[idx]\n",
    "    pixel_values = item[\"pixel_values\"]  # Tensor [3, H, W]\n",
    "    label_idx = item[\"labels\"].item()      \n",
    "    true_label = id2label[label_idx]\n",
    "\n",
    "    # Run inference\n",
    "    input_tensor = pixel_values.unsqueeze(0).to(device)  # [1, 3, H, W]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=input_tensor)\n",
    "        probs = softmax(outputs.logits, dim=-1)\n",
    "        pred_idx    = probs.argmax(dim=-1).item()\n",
    "        pred_label  = id2label[pred_idx]\n",
    "        confidence = probs[0, pred_idx].item()\n",
    "\n",
    "    # Un-normalize for display\n",
    "    img_tensor = input_tensor * std + mean\n",
    "    img = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    ax.imshow(img)\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "    ax.set_title(f\"{pred_label} ({confidence:.2f})\", color=color)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
