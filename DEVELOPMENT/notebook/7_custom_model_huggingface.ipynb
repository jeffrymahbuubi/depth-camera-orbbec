{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374fc6ea",
   "metadata": {},
   "source": [
    "# Custom Model Huggingface Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc94bdb",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72f41c",
   "metadata": {},
   "source": [
    "- This notebook dedicated to explore how to define a custom model and train the custom model with Huggingface framework\n",
    "- Useful sources:\n",
    "    - [Huggingface Models](https://huggingface.co/docs/transformers/en/models)\n",
    "    - [Medium Blog - A Guide to Craft Your Own Custom Hugging Face Model](https://medium.com/@edandwe/a-guide-to-craft-your-own-custom-hugging-face-model-ba9cd555a646)\n",
    "- Fundamentally, there are several things to defined to start using custom model: \n",
    "    - `AutoModel` class is a convenient way to load an architecture without needing to know the exact model class name because there are many models available. It automatically selects the correct model class based on the configuration file.\n",
    "    - `AutoConfig`\n",
    "    - The base workflow is as follows: \n",
    "      \n",
    "       ```mermaid\n",
    "        flowchart LR\n",
    "            A[checkpoint or local folder] --> B[config file]\n",
    "            A --> D[model file]\n",
    "            B --> C[config class]\n",
    "            C --> E[model config]\n",
    "            C --> F[model class]\n",
    "            E --> G[model]\n",
    "            F --> G\n",
    "            G --> H[pretrained model]\n",
    "            D --> H\n",
    "       ```\n",
    "\n",
    "    - More detail in this video [Instantiate a Transformers model (PyTorch)](https://www.youtube.com/watch?v=AhChOFRegn4&t=101s)\n",
    "    - `PretrainedConfig` class contains all the necessary information to build a model, two things to follows:\n",
    "        - 1. A custom configuration must subclass PretrainedConfig. This ensures a custom model has all the functionality of a Transformersâ€™ model such as `from_pretrained()`, `save_pretrained()`, and `push_to_hub()`.\n",
    "        - 2. The PretrainedConfig `__init__` must accept any `kwargs` and they must be passed to the superclass `__init__`.\n",
    "        - > It is useful to check the validity of some of the parameters. In the example below, a check is implemented to ensure block_type and stem_type belong to one of the predefined values.\n",
    "        - > Add model_type to the configuration class to enable AutoClass support.\n",
    "    - `PretrainedModel` class, inheriting from PreTrainedModel and initializing the superclass with the configuration extends Transformersâ€™ functionalities such as saving and loading to the custom model/ \n",
    "        - > Add `config_class` to the model class to enable AutoClass support.\n",
    "        - NOTE: \n",
    "            - A model can return any output format. Returning a dictionary (like `ResnetModelForImageClassification`) with losses when labels are available makes the custom model compatible with [Trainer](https://huggingface.co/docs/transformers/v4.52.1/en/main_classes/trainer#transformers.Trainer).\n",
    "            - For other output formats, youâ€™ll need your own training loop or a different library for training.\n",
    "    - The `AutoClass` API is a shortcut for automatically loading the correct architecture for a given model. It is convenient to enable this for users loading your custom model. It is convenient to enable this for users loading your custom model.\n",
    "        - Make sure you have the `model_type` attribute (must be different from existing model types) in the configuration class and `config_class` attribute in the model class. Use the [register()](https://huggingface.co/docs/transformers/v4.52.1/en/model_doc/auto#transformers.AutoConfig.register) method to add the custom configuration and model to the [AutoClass](https://huggingface.co/docs/transformers/en/models#model-classes) API.\n",
    "        - > The first argument to `AutoConfig.register()` must match the `model_type` attribute in the custom configuration class, and the first argument to `AutoModel.register()` must match the `config_class` of the custom model class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db5eaa",
   "metadata": {},
   "source": [
    "## 0. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019a08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainerCallback, TrainingArguments\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d303dc",
   "metadata": {},
   "source": [
    "## 1. Custom Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107b0e7",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066d2a9",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447d103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class LSTMConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`LSTMModel`]. It is used to instantiate an\n",
    "    LSTM model according to the specified arguments, defining the model architecture. Instantiating a configuration\n",
    "    with the defaults will yield a similar configuration to that of a basic LSTM architecture.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "    Args:\n",
    "        input_size (`int`, *optional*, defaults to 34):\n",
    "            The dimension of the input features. For pose estimation, this would be the number of keypoints * 2 (x,y coordinates).\n",
    "        hidden_size (`int`, *optional*, defaults to 128):\n",
    "            The dimension of the hidden states.\n",
    "        num_layers (`int`, *optional*, defaults to 1):\n",
    "            Number of recurrent layers.\n",
    "        num_labels (`int`, *optional*, defaults to 6):\n",
    "            The number of labels for classification.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probability for all fully connected layers in the model.\n",
    "        bidirectional (`bool`, *optional*, defaults to `False`):\n",
    "            If `True`, becomes a bidirectional LSTM.\n",
    "        batch_first (`bool`, *optional*, defaults to `True`):\n",
    "            If `True`, then the input and output tensors are provided as (batch, seq, feature).\n",
    "        proj_size (`int`, *optional*, defaults to 0):\n",
    "            If > 0, will use LSTM with projections of corresponding size.\n",
    "        window_size (`int`, *optional*, defaults to 32):\n",
    "            The size of the sliding window for sequential data.\n",
    "        learning_rate (`float`, *optional*, defaults to 0.001):\n",
    "            The learning rate for training.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n",
    "            The epsilon used by the layer normalization layers.\n",
    "        use_layer_norm (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to use layer normalization after the LSTM.\n",
    "        use_projection (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to use a projection layer after LSTM.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import LSTMConfig, LSTMModel\n",
    "\n",
    "    >>> # Initializing a LSTM configuration\n",
    "    >>> configuration = LSTMConfig()\n",
    "\n",
    "    >>> # Initializing a model from the configuration\n",
    "    >>> model = LSTMModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"lstm\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=34,\n",
    "        hidden_size=128,\n",
    "        num_layers=1,\n",
    "        num_labels=5,\n",
    "        dropout=0.0,\n",
    "        bidirectional=False,\n",
    "        batch_first=True,\n",
    "        proj_size=0,\n",
    "        window_size=16,\n",
    "        learning_rate=0.001,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-5,\n",
    "        use_layer_norm=False,\n",
    "        use_projection=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_first = batch_first\n",
    "        self.proj_size = proj_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.use_projection = use_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b25e3a",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33174e7",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de416ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"LSTMConfig\"\n",
    "\n",
    "LSTM_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    # Add pretrained model identifiers here\n",
    "]\n",
    "\n",
    "\n",
    "class LSTMPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = LSTMConfig\n",
    "    base_model_prefix = \"lstm\"\n",
    "    supports_gradient_checkpointing = False\n",
    "    _no_split_modules = [\"LSTMLayer\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LSTM):\n",
    "            for name, param in module.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    \"\"\"LSTM layer with optional layer normalization and dropout.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config.input_size if hasattr(config, 'input_size') else config.hidden_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=config.batch_first,\n",
    "            dropout=config.dropout if config.num_layers > 1 else 0,\n",
    "            bidirectional=config.bidirectional,\n",
    "            proj_size=config.proj_size if config.proj_size > 0 else 0,\n",
    "        )\n",
    "        \n",
    "        self.use_layer_norm = config.use_layer_norm\n",
    "        if self.use_layer_norm:\n",
    "            norm_size = config.hidden_size * (2 if config.bidirectional else 1)\n",
    "            self.layer_norm = nn.LayerNorm(norm_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_tensor,\n",
    "        hidden_states=None,\n",
    "        cell_states=None,\n",
    "    ):\n",
    "        lstm_output, (hidden_states, cell_states) = self.lstm(\n",
    "            input_tensor, \n",
    "            (hidden_states, cell_states) if hidden_states is not None else None\n",
    "        )\n",
    "        \n",
    "        if self.use_layer_norm:\n",
    "            lstm_output = self.layer_norm(lstm_output)\n",
    "        \n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        \n",
    "        return lstm_output, (hidden_states, cell_states)\n",
    "\n",
    "\n",
    "LSTM_START_DOCSTRING = r\"\"\"\n",
    "    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n",
    "    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
    "    behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`LSTMConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "LSTM_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.FloatTensor` of shape `(batch_size, sequence_length, input_size)`):\n",
    "            Input sequence tensor.\n",
    "        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare LSTM Model transformer outputting raw hidden-states without any specific head on top.\",\n",
    "    LSTM_START_DOCSTRING,\n",
    ")\n",
    "class LSTMModel(LSTMPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.lstm_layer = LSTMLayer(config)\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LSTM_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import LSTMConfig, LSTMModel\n",
    "        >>> import torch\n",
    "\n",
    "        >>> # Initializing a LSTM configuration\n",
    "        >>> configuration = LSTMConfig()\n",
    "\n",
    "        >>> # Initializing a model from the configuration\n",
    "        >>> model = LSTMModel(configuration)\n",
    "\n",
    "        >>> # Random input tensor (batch_size=2, sequence_length=32, input_size=34)\n",
    "        >>> inputs = torch.randn(2, 32, 34)\n",
    "        >>> outputs = model(inputs)\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "        ```\"\"\"\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask\n",
    "            attention_mask = attention_mask.unsqueeze(-1).expand_as(input_ids)\n",
    "            input_ids = input_ids * attention_mask\n",
    "\n",
    "        # Pass through LSTM\n",
    "        sequence_output, (final_hidden, final_cell) = self.lstm_layer(input_ids)\n",
    "\n",
    "        # Get the last hidden state\n",
    "        if self.config.bidirectional:\n",
    "            # Concatenate forward and backward hidden states\n",
    "            last_hidden_state = torch.cat([final_hidden[-2], final_hidden[-1]], dim=-1)\n",
    "        else:\n",
    "            last_hidden_state = final_hidden[-1]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, last_hidden_state)\n",
    "\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "            hidden_states=(last_hidden_state,) if output_hidden_states else None,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"LSTM Model with a classification head on top (a linear layer on top of the hidden-states output).\"\"\",\n",
    "    LSTM_START_DOCSTRING,\n",
    ")\n",
    "class LSTMForSequenceClassification(LSTMPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.lstm = LSTMModel(config)\n",
    "        \n",
    "        # Classification head\n",
    "        classifier_input_size = config.hidden_size * (2 if config.bidirectional else 1)\n",
    "        \n",
    "        if config.use_projection:\n",
    "            self.pre_classifier = nn.Linear(classifier_input_size, config.hidden_size)\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "            self.dropout = nn.Dropout(config.dropout)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(classifier_input_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LSTM_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import LSTMConfig, LSTMForSequenceClassification\n",
    "        >>> import torch\n",
    "\n",
    "        >>> # Number of labels for classification\n",
    "        >>> num_labels = 6\n",
    "\n",
    "        >>> # Initializing a LSTM configuration\n",
    "        >>> configuration = LSTMConfig(num_labels=num_labels)\n",
    "\n",
    "        >>> # Initializing a model from the configuration\n",
    "        >>> model = LSTMForSequenceClassification(configuration)\n",
    "\n",
    "        >>> # Random input tensor (batch_size=2, sequence_length=32, input_size=34)\n",
    "        >>> inputs = torch.randn(2, 32, 34)\n",
    "        >>> labels = torch.tensor([1, 3])\n",
    "\n",
    "        >>> outputs = model(inputs, labels=labels)\n",
    "        >>> loss = outputs.loss\n",
    "        >>> logits = outputs.logits\n",
    "        ```\"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.lstm(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Get the last hidden state from LSTM\n",
    "        if return_dict:\n",
    "            # For sequence classification, we typically use the last hidden state\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "            if len(outputs.hidden_states) > 0:\n",
    "                pooled_output = outputs.hidden_states[0]  # Final hidden state\n",
    "            else:\n",
    "                # If hidden states not returned, use the last timestep\n",
    "                pooled_output = sequence_output[:, -1, :]\n",
    "        else:\n",
    "            sequence_output = outputs[0]\n",
    "            pooled_output = outputs[1]\n",
    "\n",
    "        # Apply projection if configured\n",
    "        if self.config.use_projection:\n",
    "            pooled_output = self.pre_classifier(pooled_output)\n",
    "            pooled_output = torch.relu(pooled_output)\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=None,  # LSTM doesn't have attention weights\n",
    "        )\n",
    "\n",
    "\n",
    "# Add to AutoModel registry\n",
    "def register_lstm_auto_model():\n",
    "    \"\"\"Register the LSTM model with AutoModel.\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoConfig, AutoModel, AutoModelForSequenceClassification\n",
    "        \n",
    "        AutoConfig.register(\"lstm\", LSTMConfig)\n",
    "        AutoModel.register(LSTMConfig, LSTMModel)\n",
    "        AutoModelForSequenceClassification.register(LSTMConfig, LSTMForSequenceClassification)\n",
    "    except ImportError:\n",
    "        logger.warning(\"Could not register LSTM model with AutoModel. Please ensure transformers is installed.\")\n",
    "\n",
    "\n",
    "# Register on module import\n",
    "register_lstm_auto_model()\n",
    "\n",
    "__all__ = [\n",
    "    \"LSTMConfig\",\n",
    "    \"LSTMPreTrainedModel\", \n",
    "    \"LSTMModel\",\n",
    "    \"LSTMForSequenceClassification\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59a3b2",
   "metadata": {},
   "source": [
    "## 2. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5c52d",
   "metadata": {},
   "source": [
    "### 2.1. Create Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2a294cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ hand_landmark_flatten/\n",
      "â”œâ”€ğŸ“„ dataset_summary.json\n",
      "â”œâ”€ğŸ“ HC/\n",
      "â”‚ â”œâ”€ğŸ“„ Record_20250402151124_w005_landmarks.json\n",
      "â”‚ â””â”€ğŸ“„ Record_20250402151124_w005_landmarks.npy\n",
      "â”œâ”€ğŸ“ HH/\n",
      "â”‚ â”œâ”€ğŸ“„ Record_20250402151124_w007_landmarks.json\n",
      "â”‚ â””â”€ğŸ“„ Record_20250402151124_w007_landmarks.npy\n",
      "â”œâ”€ğŸ“ IH/\n",
      "â”‚ â”œâ”€ğŸ“„ Record_20250402151124_w001_landmarks.json\n",
      "â”‚ â””â”€ğŸ“„ Record_20250402151124_w001_landmarks.npy\n",
      "â”œâ”€ğŸ“ OH/\n",
      "â”‚ â”œâ”€ğŸ“„ Record_20250402151124_w000_landmarks.json\n",
      "â”‚ â””â”€ğŸ“„ Record_20250402151124_w000_landmarks.npy\n",
      "â””â”€ğŸ“ SF/\n",
      "  â”œâ”€ğŸ“„ Record_20250402151124_w003_landmarks.json\n",
      "  â””â”€ğŸ“„ Record_20250402151124_w003_landmarks.npy\n",
      "(16, 42)\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "sd.seedir(r'./data/video_hand_focused_data/hand_landmark_flatten', style='emoji', itemlimit=(None, 2))\n",
    "print(np.load(r'./data/video_hand_focused_data/hand_landmark_flatten/HC/Record_20250402151124_w005_landmarks.npy').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b635ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0600fc5835d437ebb1b31e286254454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: 45 samples\n",
      "Validation set: 15 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19aecd51d2534cf6bddd002cbb740bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d702cc46a829439ea213ed1f1ec22418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "import glob\n",
    "\n",
    "landmark_dir = r'data/video_hand_focused_data/hand_landmark_flatten'\n",
    "val_ratio = 0.25\n",
    "\n",
    "# Create lists of file paths and labels\n",
    "filepaths, labels = [], []\n",
    "class_mapping = {\n",
    "    'OH': 0,  # Open Hand\n",
    "    'IH': 1,  # Intrinsic Plus\n",
    "    'SF': 2,  # Straight Fist\n",
    "    'HH': 3,  # Hook Hand\n",
    "    'HC': 4   # Hand Close\n",
    "}\n",
    "\n",
    "# Scan through each class directory\n",
    "for class_name in class_mapping.keys():\n",
    "    class_dir = os.path.join(landmark_dir, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f\"Warning: Class directory not found: {class_dir}\")\n",
    "        continue\n",
    "        \n",
    "    # Get all .npy files (excluding metadata jsons)\n",
    "    for npy_path in glob.glob(os.path.join(class_dir, '*_landmarks.npy')):\n",
    "        filepaths.append(npy_path)\n",
    "        labels.append(class_name)\n",
    "\n",
    "unique_labels = sorted(set(labels))\n",
    "\n",
    "# Create dataset with file paths\n",
    "dataset = Dataset.from_dict({\n",
    "    \"landmark_path\": filepaths,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "# Encode the label column\n",
    "dataset = dataset.class_encode_column(\"label\")\n",
    "\n",
    "# Create a function to load landmarks when needed\n",
    "def load_landmarks(example):\n",
    "    # Load the landmark data\n",
    "    landmarks = np.load(example[\"landmark_path\"])\n",
    "    # Flatten if needed (should already be (16, 42) if flatten_keypoints=True was used)\n",
    "    if landmarks.shape == (16, 21, 2):\n",
    "        landmarks = landmarks.reshape(16, -1)\n",
    "    example[\"input_ids\"] = landmarks.tolist()\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Split the dataset\n",
    "split = dataset.train_test_split(\n",
    "    test_size=val_ratio, \n",
    "    shuffle=True, \n",
    "    seed=42, \n",
    "    stratify_by_column=\"label\"\n",
    ")\n",
    "\n",
    "train_ds = split[\"train\"]\n",
    "val_ds = split[\"test\"]\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_ds)} samples\")\n",
    "print(f\"Validation set: {len(val_ds)} samples\")\n",
    "\n",
    "# After loading the landmarks, remove the metadata field before training\n",
    "train_ds = train_ds.map(load_landmarks, remove_columns=[\"landmark_path\"])\n",
    "val_ds = val_ds.map(load_landmarks, remove_columns=[\"landmark_path\"])\n",
    "\n",
    "# Rename 'label' to 'labels' for compatibility with Trainer\n",
    "train_ds = train_ds.rename_column(\"label\", \"labels\")\n",
    "val_ds = val_ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Build label mappings\n",
    "label2id = {lab: idx for idx, lab in enumerate(unique_labels)}\n",
    "id2label = {idx: lab for lab, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebd2dd",
   "metadata": {},
   "source": [
    "### 2.2. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af44fcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture Summary:\n",
      "=================================================================================================================================================\n",
      "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
      "=================================================================================================================================================\n",
      "LSTMForSequenceClassification (LSTMForSequenceClassification)     [32, 16, 42]         [32, 256]            --                   True\n",
      "â”œâ”€LSTMModel (lstm)                                                [32, 16, 42]         [32, 256]            --                   True\n",
      "â”‚    â””â”€LSTMLayer (lstm_layer)                                     [32, 16, 42]         [32, 16, 256]        --                   True\n",
      "â”‚    â”‚    â””â”€LSTM (lstm)                                           [32, 16, 42]         [32, 16, 256]        571,392              True\n",
      "â”‚    â”‚    â””â”€LayerNorm (layer_norm)                                [32, 16, 256]        [32, 16, 256]        512                  True\n",
      "â”‚    â”‚    â””â”€Dropout (dropout)                                     [32, 16, 256]        [32, 16, 256]        --                   --\n",
      "â”œâ”€Linear (pre_classifier)                                         [32, 256]            [32, 128]            32,896               True\n",
      "â”œâ”€Dropout (dropout)                                               [32, 128]            [32, 128]            --                   --\n",
      "â”œâ”€Linear (classifier)                                             [32, 128]            [32, 5]              645                  True\n",
      "=================================================================================================================================================\n",
      "Total params: 605,445\n",
      "Trainable params: 605,445\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 293.64\n",
      "=================================================================================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 2.13\n",
      "Params size (MB): 2.42\n",
      "Estimated Total Size (MB): 4.64\n",
      "=================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "INPUT_SIZE = 42\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "NUM_LABELS = 5\n",
    "DROPOUT = 0.2\n",
    "BIDIRECTIONAL = True\n",
    "BATCH_FIRST = True\n",
    "WINDOW_SIZE = 16\n",
    "USE_PROJECTION = True\n",
    "USE_LAYER_NORM = True\n",
    "OUTPUT_HIDDEN_STATES = True\n",
    "\n",
    "config = LSTMConfig(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=NUM_LABELS,\n",
    "    dropout=DROPOUT,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    batch_first=BATCH_FIRST,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    use_projection=USE_PROJECTION,\n",
    "    use_layer_norm=USE_LAYER_NORM,\n",
    "    output_hidden_states=OUTPUT_HIDDEN_STATES\n",
    ")\n",
    "\n",
    "model = LSTMForSequenceClassification(config)\n",
    "\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "\n",
    "input_shape = (32, 16, 42)  # batch_size=32, window_size=16, features=42\n",
    "model_summary = summary(\n",
    "    model,\n",
    "    input_size=input_shape,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94acf30",
   "metadata": {},
   "source": [
    "### 2.3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764c685",
   "metadata": {},
   "source": [
    "#### Setup `Trainer`, `Callback`, and `Evaluate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd088148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Setup paths\n",
    "experiment_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "model_name = f\"hand-pose-lstm-h{HIDDEN_SIZE}-l{NUM_LAYERS}\"\n",
    "output_dir = f\"experiments/hand_pose/{experiment_date}/{model_name}\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        remove_unused_columns=False,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"best\",\n",
    "        save_total_limit=3,\n",
    "        learning_rate=5e-4,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=100,\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    # Load metrics\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    precision = evaluate.load(\"precision\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    confusion = evaluate.load(\"confusion_matrix\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec = precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "    rec = recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1sc = f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion.compute(predictions=preds, references=labels)[\"confusion_matrix\"]\n",
    "    cm_list = cm.tolist()\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1sc,\n",
    "        \"confusion_matrix\": cm_list,\n",
    "    }  \n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"Custom trainer that tracks batch-level metrics\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.epoch_losses = []\n",
    "        self.epoch_preds = []\n",
    "        self.epoch_labels = []\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Override to store batch-level loss & predictions\"\"\"\n",
    "        labels = inputs.get(\"labels\", None)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Store the loss\n",
    "            self.epoch_losses.append(loss.item())\n",
    "            \n",
    "            # Store predictions + labels\n",
    "            preds = logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labs = labels.detach().cpu().numpy()\n",
    "            self.epoch_preds.extend(preds.tolist())\n",
    "            self.epoch_labels.extend(labs.tolist())\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    \"\"\"Callback to track and store training metrics\"\"\"\n",
    "    def __init__(self, trainer):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.eval_losses = []\n",
    "        self.eval_accuracies = []\n",
    "        self.eval_confusion_matrices = []\n",
    "        self.eval_f1_scores = []\n",
    "        self.eval_precisions = []\n",
    "        self.eval_recalls = []\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Compute and store training metrics at epoch end\"\"\"\n",
    "        t = self.trainer\n",
    "        if t.epoch_losses:\n",
    "            avg_loss = float(np.mean(t.epoch_losses))\n",
    "            acc = np.mean(np.array(t.epoch_preds) == np.array(t.epoch_labels))\n",
    "            \n",
    "            self.train_losses.append(avg_loss)\n",
    "            self.train_accuracies.append(acc)\n",
    "            \n",
    "            # Clear for next epoch\n",
    "            t.epoch_losses.clear()\n",
    "            t.epoch_preds.clear()\n",
    "            t.epoch_labels.clear()\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        \"\"\"Store validation metrics\"\"\"\n",
    "        self.eval_losses.append(metrics.get(\"eval_loss\", 0))\n",
    "        self.eval_accuracies.append(metrics.get(\"eval_accuracy\", 0))\n",
    "        self.eval_confusion_matrices.append(metrics.get(\"eval_confusion_matrix\", []))\n",
    "        self.eval_f1_scores.append(metrics.get(\"eval_f1\", 0))\n",
    "        self.eval_precisions.append(metrics.get(\"eval_precision\", 0))\n",
    "        self.eval_recalls.append(metrics.get(\"eval_recall\", 0))\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "# Add metrics callback\n",
    "metrics_cb = MetricsCallback(trainer)\n",
    "trainer.add_callback(metrics_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a8714",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90c260ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/200 00:00 < 00:07, 25.00 it/s, Epoch 1/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\transformers\\trainer.py:2661\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2661\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m   2663\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2667\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\transformers\\trainer.py:3096\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3094\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3096\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3097\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\transformers\\trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3045\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3048\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\transformers\\trainer.py:4154\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4151\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4153\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4155\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4157\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4164\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\transformers\\trainer.py:4443\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4441\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4442\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4443\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4445\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4446\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4447\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[48], line 41\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m     38\u001b[0m confusion \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfusion_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m---> 41\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[0;32m     44\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32md:\\AUNUUN JEFFRY MAHBUUBI\\PROJECT AND RESEARCH\\PROJECTS\\28. Depth Camera\\CODE\\DEPTH CAMERA\\Orbbec Gemini 2XL\\pyorbbec-sdk-experiments\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
